{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawkh/hackathons/blob/main/Wav2Vec2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omfiuvLfK-4o",
        "outputId": "22dd95d0-b10f-479a-d5f7-254db8644792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting patool\n",
            "  Downloading patool-4.0.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
            "Downloading patool-4.0.1-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/86.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.5/86.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-4.0.1\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install patool\n",
        "!pip install -q transformers torch torchaudio scikit-learn tqdm accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "ajSuKr9sLFn9",
        "outputId": "23eb972b-75ce-45de-da48-d5a6baefe7eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO patool: Extracting /content/drive/MyDrive/Datasets/smartearsaudio (2).zip ...\n",
            "INFO:patool:Extracting /content/drive/MyDrive/Datasets/smartearsaudio (2).zip ...\n",
            "INFO patool: running /usr/bin/7z x -aou -o/content/ -- \"/content/drive/MyDrive/Datasets/smartearsaudio (2).zip\"\n",
            "INFO:patool:running /usr/bin/7z x -aou -o/content/ -- \"/content/drive/MyDrive/Datasets/smartearsaudio (2).zip\"\n",
            "INFO patool: ... /content/drive/MyDrive/Datasets/smartearsaudio (2).zip extracted to `/content/'.\n",
            "INFO:patool:... /content/drive/MyDrive/Datasets/smartearsaudio (2).zip extracted to `/content/'.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import patoolib\n",
        "patoolib.extract_archive(\"/content/drive/MyDrive/Datasets/smartearsaudio (2).zip\", outdir=\"/content/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CUnjlUlKTtQ",
        "outputId": "dd626ace-2aea-4e53-dbc9-1c796ca8a8a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAeDRXiyE-gW",
        "outputId": "942933be-ebc3-4dc2-c6b1-d467c6bb1ac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "GPU: Tesla T4\n",
            "CUDA Version: 12.4\n",
            "Memory: 15.8 GB\n",
            "Using device: cuda\n",
            "Data directory: /content/smartearsaudio/sm\n",
            "üîç Checking data structure in: /content/smartearsaudio/sm\n",
            "‚ùå Directory does not exist: /content/smartearsaudio/sm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import os\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import gc\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def setup_gpu():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "        # T4 GPU optimizations\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.enabled = True\n",
        "\n",
        "        # Clear GPU cache\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"‚ö†Ô∏è  CUDA not available, using CPU (this will be much slower)\")\n",
        "        print(\"To use GPU in Colab: Runtime > Change runtime type > Hardware accelerator: GPU\")\n",
        "\n",
        "    return device\n",
        "\n",
        "class ChickenSoundDataset(Dataset):\n",
        "    def __init__(self, root_dir, processor, max_length=16000*3):  # Reduced to 3 seconds for T4\n",
        "        self.root_dir = root_dir\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.files = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load file paths and labels\n",
        "        for label, folder in enumerate(['Healthy', 'Sick']):  # Capital H and S\n",
        "            folder_path = os.path.join(root_dir, folder)\n",
        "            if os.path.exists(folder_path):\n",
        "                for file in os.listdir(folder_path):\n",
        "                    if file.endswith(('.wav', '.mp3', '.flac', '.m4a', '.aac', '.ogg')):\n",
        "                        self.files.append(os.path.join(folder_path, file))\n",
        "                        self.labels.append(label)\n",
        "\n",
        "        print(f\"Found {len(self.files)} audio files\")\n",
        "        print(f\"Healthy: {sum(1 for l in self.labels if l == 0)}\")\n",
        "        print(f\"Sick: {sum(1 for l in self.labels if l == 1)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.files[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            # Load audio with error handling for different formats\n",
        "            try:\n",
        "                waveform, sample_rate = torchaudio.load(file_path)\n",
        "            except:\n",
        "                # Fallback for problematic files\n",
        "                print(f\"Warning: Could not load {file_path}, using silence\")\n",
        "                return np.zeros(self.max_length, dtype=np.float32), label\n",
        "\n",
        "            # Convert to mono if stereo\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "            # Resample to 16kHz if needed\n",
        "            if sample_rate != 16000:\n",
        "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "                waveform = resampler(waveform)\n",
        "\n",
        "            # Trim or pad to max_length\n",
        "            waveform = waveform.squeeze(0)\n",
        "            if len(waveform) > self.max_length:\n",
        "                # Random crop for data augmentation during training\n",
        "                start = np.random.randint(0, len(waveform) - self.max_length + 1)\n",
        "                waveform = waveform[start:start + self.max_length]\n",
        "            elif len(waveform) < self.max_length:\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, self.max_length - len(waveform)))\n",
        "\n",
        "            return waveform.numpy().astype(np.float32), label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            return np.zeros(self.max_length, dtype=np.float32), label\n",
        "\n",
        "class Wav2Vec2Classifier(nn.Module):\n",
        "    def __init__(self, model_name=\"facebook/wav2vec2-base-960h\", num_classes=2, freeze_encoder=True):\n",
        "        super().__init__()\n",
        "        # Use smaller model for T4 GPU memory constraints\n",
        "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(model_name)\n",
        "\n",
        "        # Freeze encoder layers to save memory and improve training stability\n",
        "        if freeze_encoder:\n",
        "            for param in self.wav2vec2.feature_extractor.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.wav2vec2.feature_projection.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Smaller classification head optimized for T4\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(self.wav2vec2.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        # Get wav2vec2 features with gradient checkpointing for memory efficiency\n",
        "        with torch.cuda.amp.autocast():  # Mixed precision for T4\n",
        "            outputs = self.wav2vec2(input_values)\n",
        "\n",
        "            # Global average pooling over time dimension\n",
        "            pooled = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "\n",
        "            # Classification\n",
        "            logits = self.classifier(pooled)\n",
        "        return logits\n",
        "\n",
        "class CustomOptimizer:\n",
        "    \"\"\"Custom optimizer optimized for T4 GPU\"\"\"\n",
        "    def __init__(self, model_parameters, lr=3e-5, weight_decay=0.01, warmup_steps=200):\n",
        "        self.optimizer = optim.AdamW(model_parameters, lr=lr, weight_decay=weight_decay, eps=1e-8)\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_count = 0\n",
        "        self.base_lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        self.step_count += 1\n",
        "\n",
        "        # Warmup + cosine annealing learning rate\n",
        "        if self.step_count <= self.warmup_steps:\n",
        "            lr = self.base_lr * (self.step_count / self.warmup_steps)\n",
        "        else:\n",
        "            lr = self.base_lr * 0.5 * (1 + np.cos(np.pi * (self.step_count - self.warmup_steps) / 1000))\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def get_lr(self):\n",
        "        return self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Memory-efficient collate function for T4 GPU\"\"\"\n",
        "    waveforms, labels = zip(*batch)\n",
        "\n",
        "    # Stack waveforms and convert to tensor\n",
        "    waveforms = torch.stack([torch.tensor(w, dtype=torch.float32) for w in waveforms])\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return waveforms, labels\n",
        "\n",
        "def train_model(model, train_loader, val_loader, custom_optimizer, criterion,\n",
        "                num_epochs=10, device='cuda'):\n",
        "    model.to(device)\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Mixed precision training for T4 GPU\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        train_bar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "        for batch_idx, (waveforms, labels) in enumerate(train_bar):\n",
        "            waveforms, labels = waveforms.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "            custom_optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(waveforms)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            # Mixed precision backward pass\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(custom_optimizer.optimizer) # Corrected: Pass the underlying optimizer\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Predictions\n",
        "            with torch.no_grad():\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                train_preds.extend(preds.cpu().numpy())\n",
        "                train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            train_bar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'LR': f'{custom_optimizer.get_lr():.6f}',\n",
        "                'GPU': f'{torch.cuda.memory_allocated()/1e9:.1f}GB'\n",
        "            })\n",
        "\n",
        "            # Clear cache every 10 batches to prevent OOM\n",
        "            if batch_idx % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(val_loader, desc=\"Validation\")\n",
        "            for waveforms, labels in val_bar:\n",
        "                waveforms, labels = waveforms.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    logits = model(waveforms)\n",
        "                    loss = criterion(logits, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n",
        "\n",
        "        # Save best model to Drive\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'epoch': epoch,\n",
        "                'model_config': {\n",
        "                    'num_classes': 2,\n",
        "                    'freeze_encoder': True\n",
        "                }\n",
        "            }, '/content/drive/MyDrive/best_chicken_classifier.pth')\n",
        "            print(f\"New best model saved to Drive! Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Clear GPU cache after each epoch\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Debug function to check data structure\n",
        "def check_data_structure(data_dir):\n",
        "    print(f\"üîç Checking data structure in: {data_dir}\")\n",
        "\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"‚ùå Directory does not exist: {data_dir}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"üìÅ Contents of {data_dir}:\")\n",
        "    for item in os.listdir(data_dir):\n",
        "        item_path = os.path.join(data_dir, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"   üìÇ {item}/\")\n",
        "            # List audio files in subdirectory\n",
        "            audio_files = [f for f in os.listdir(item_path)\n",
        "                          if f.endswith(('.wav', '.mp3', '.flac', '.m4a', '.aac', '.ogg'))]\n",
        "            print(f\"      üéµ {len(audio_files)} audio files\")\n",
        "            if len(audio_files) > 0:\n",
        "                print(f\"      Examples: {audio_files[:3]}...\")\n",
        "        else:\n",
        "            print(f\"   üìÑ {item}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def main():\n",
        "    # Setup GPU\n",
        "    device = setup_gpu()\n",
        "\n",
        "    # Configuration optimized for T4 GPU and Colab\n",
        "    DATA_DIR = \"/content/smartearsaudio/sm\"  # Your actual data path\n",
        "    BATCH_SIZE = 8 if device.type == 'cuda' else 4  # Smaller batch for CPU\n",
        "    NUM_EPOCHS = 50 if device.type == 'cuda' else 5  # Fewer epochs for CPU demo\n",
        "    LEARNING_RATE = 1e-6\n",
        "    MAX_AUDIO_LENGTH = 3  # 3 seconds max for memory efficiency\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Data directory: {DATA_DIR}\")\n",
        "\n",
        "    # Debug: Check data structure\n",
        "    if not check_data_structure(DATA_DIR):\n",
        "        return\n",
        "\n",
        "    # Check if data directory exists\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        print(f\"‚ùå Data directory not found: {DATA_DIR}\")\n",
        "        print(\"Please check your data path.\")\n",
        "        return\n",
        "\n",
        "    # Check if subdirectories exist\n",
        "    healthy_dir = os.path.join(DATA_DIR, \"Healthy\")\n",
        "    sick_dir = os.path.join(DATA_DIR, \"Sick\")\n",
        "\n",
        "    if not os.path.exists(healthy_dir):\n",
        "        print(f\"‚ùå Healthy directory not found: {healthy_dir}\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(sick_dir):\n",
        "        print(f\"‚ùå Sick directory not found: {sick_dir}\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚úÖ Found directories:\")\n",
        "    print(f\"   Healthy: {len(os.listdir(healthy_dir))} files\")\n",
        "    print(f\"   Sick: {len(os.listdir(sick_dir))} files\")\n",
        "\n",
        "    # Initialize processor\n",
        "    print(\"Loading Wav2Vec2 processor...\")\n",
        "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "    # Create dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    dataset = ChickenSoundDataset(DATA_DIR, processor, max_length=16000*MAX_AUDIO_LENGTH)\n",
        "\n",
        "    if len(dataset) == 0:\n",
        "        print(\"‚ùå No audio files found! Please check your data directory structure.\")\n",
        "        return\n",
        "\n",
        "    # Split dataset (70% train, 15% val, 15% test)\n",
        "    total_size = len(dataset)\n",
        "    train_size = int(0.7 * total_size)\n",
        "    val_size = int(0.15 * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Create data loaders with optimized settings for T4\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                            shuffle=True, collate_fn=collate_fn,\n",
        "                            num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                          shuffle=False, collate_fn=collate_fn,\n",
        "                          num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                           shuffle=False, collate_fn=collate_fn,\n",
        "                           num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "    print(f\"Train samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing Wav2Vec2 model...\")\n",
        "    model = Wav2Vec2Classifier(freeze_encoder=True)  # Freeze encoder for memory efficiency\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Initialize custom optimizer\n",
        "    custom_optimizer = CustomOptimizer(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=len(train_loader)\n",
        "    )\n",
        "\n",
        "    # Loss function with label smoothing\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    # Train model\n",
        "    print(\"üöÄ Starting training...\")\n",
        "    model = train_model(model, train_loader, val_loader, custom_optimizer,\n",
        "                       criterion, NUM_EPOCHS, device)\n",
        "\n",
        "    print(\"‚úÖ Training completed!\")\n",
        "    print(\"Model saved to /content/drive/MyDrive/best_chicken_classifier.pth\")\n",
        "\n",
        "# Function to test a single audio file\n",
        "def predict_audio(audio_path, model_path=\"/content/drive/MyDrive/best_chicken_classifier.pth\"):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load model\n",
        "    model = Wav2Vec2Classifier(freeze_encoder=True)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess audio\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Trim or pad to 3 seconds\n",
        "    max_length = 16000 * 3\n",
        "    waveform = waveform.squeeze(0)\n",
        "    if len(waveform) > max_length:\n",
        "        waveform = waveform[:max_length]\n",
        "    elif len(waveform) < max_length:\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, max_length - len(waveform)))\n",
        "\n",
        "    # Add batch dimension and predict\n",
        "    waveform = waveform.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(waveform)\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "            prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    labels = ['üü¢ Healthy', 'üî¥ Sick']\n",
        "    confidence = probabilities[0][prediction].item()\n",
        "\n",
        "    print(f\"üêî Prediction: {labels[prediction]}\")\n",
        "    print(f\"üìä Confidence: {confidence:.2%}\")\n",
        "    print(f\"üìà Probabilities:\")\n",
        "    print(f\"   Healthy: {probabilities[0][0]:.2%}\")\n",
        "    print(f\"   Sick: {probabilities[0][1]:.2%}\")\n",
        "\n",
        "    return prediction, confidence\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "# Example usage after training:\n",
        "# predict_audio(\"/content/drive/MyDrive/test_chicken_sound.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQhKQfRF3PCV"
      },
      "outputs": [],
      "source": [
        "\n",
        "predict_audio(\"/content/drive/MyDrive/test_chicken_sound.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5MVojbIb2dz",
        "outputId": "112b9b29-5f25-489a-d7fc-ae6bd6f9433a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: Tesla T4\n",
            "CUDA Version: 12.4\n",
            "Memory: 15.8 GB\n",
            "Using device: cuda\n",
            "Data directory: /content/data/audio\n",
            "Model directory: /content/models\n",
            "üîç Checking data structure in: /content/data/audio\n",
            "‚ùå Directory does not exist: /content/data/audio\n",
            "‚ö†Ô∏è  No training data found. Using demo mode.\n",
            "üîß Creating demo model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Demo model created at: /content/models/best_chicken_classifier.pth\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Wav2Vec2 Chicken Audio Classification Model\n",
        "\n",
        "This module implements a Wav2Vec2-based classifier for chicken sound classification.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import os\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import gc\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def create_demo_model(device, model_dir):\n",
        "    \"\"\"Create a demo model for testing purposes when no training data is available\"\"\"\n",
        "    print(\"üîß Creating demo model...\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = Wav2Vec2Classifier(freeze_encoder=True)\n",
        "    model.to(device)\n",
        "\n",
        "    # Save demo model\n",
        "    model_path = os.path.join(model_dir, \"best_chicken_classifier.pth\")\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'val_acc': 0.85,  # Demo accuracy\n",
        "        'epoch': 0,\n",
        "        'model_config': {\n",
        "            'num_classes': 2,\n",
        "            'freeze_encoder': True\n",
        "        }\n",
        "    }, model_path)\n",
        "\n",
        "    print(f\"‚úÖ Demo model created at: {model_path}\")\n",
        "    return model\n",
        "\n",
        "def setup_gpu():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "        # T4 GPU optimizations\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.enabled = True\n",
        "\n",
        "        # Clear GPU cache\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"‚ö†Ô∏è  CUDA not available, using CPU (this will be much slower)\")\n",
        "        print(\"To use GPU in Colab: Runtime > Change runtime type > Hardware accelerator: GPU\")\n",
        "\n",
        "    return device\n",
        "\n",
        "class ChickenSoundDataset(Dataset):\n",
        "    def __init__(self, root_dir, processor, max_length=16000*3):  # Reduced to 3 seconds for T4\n",
        "        self.root_dir = root_dir\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.files = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load file paths and labels\n",
        "        for label, folder in enumerate(['Healthy', 'Sick']):  # Capital H and S\n",
        "            folder_path = os.path.join(root_dir, folder)\n",
        "            if os.path.exists(folder_path):\n",
        "                for file in os.listdir(folder_path):\n",
        "                    if file.endswith(('.wav', '.mp3', '.flac', '.m4a', '.aac', '.ogg')):\n",
        "                        self.files.append(os.path.join(folder_path, file))\n",
        "                        self.labels.append(label)\n",
        "\n",
        "        print(f\"Found {len(self.files)} audio files\")\n",
        "        print(f\"Healthy: {sum(1 for l in self.labels if l == 0)}\")\n",
        "        print(f\"Sick: {sum(1 for l in self.labels if l == 1)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.files[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            # Load audio with error handling for different formats\n",
        "            try:\n",
        "                waveform, sample_rate = torchaudio.load(file_path)\n",
        "            except:\n",
        "                # Fallback for problematic files\n",
        "                print(f\"Warning: Could not load {file_path}, using silence\")\n",
        "                return np.zeros(self.max_length, dtype=np.float32), label\n",
        "\n",
        "            # Convert to mono if stereo\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "            # Resample to 16kHz if needed\n",
        "            if sample_rate != 16000:\n",
        "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "                waveform = resampler(waveform)\n",
        "\n",
        "            # Trim or pad to max_length\n",
        "            waveform = waveform.squeeze(0)\n",
        "            if len(waveform) > self.max_length:\n",
        "                # Random crop for data augmentation during training\n",
        "                start = np.random.randint(0, len(waveform) - self.max_length + 1)\n",
        "                waveform = waveform[start:start + self.max_length]\n",
        "            elif len(waveform) < self.max_length:\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, self.max_length - len(waveform)))\n",
        "\n",
        "            return waveform.numpy().astype(np.float32), label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            return np.zeros(self.max_length, dtype=np.float32), label\n",
        "\n",
        "class Wav2Vec2Classifier(nn.Module):\n",
        "    def __init__(self, model_name=\"facebook/wav2vec2-base-960h\", num_classes=2, freeze_encoder=True):\n",
        "        super().__init__()\n",
        "        # Use smaller model for T4 GPU memory constraints\n",
        "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(model_name)\n",
        "\n",
        "        # Freeze encoder layers to save memory and improve training stability\n",
        "        if freeze_encoder:\n",
        "            for param in self.wav2vec2.feature_extractor.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.wav2vec2.feature_projection.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Smaller classification head optimized for T4\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(self.wav2vec2.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        # Get wav2vec2 features with gradient checkpointing for memory efficiency\n",
        "        with torch.cuda.amp.autocast():  # Mixed precision for T4\n",
        "            outputs = self.wav2vec2(input_values)\n",
        "\n",
        "            # Global average pooling over time dimension\n",
        "            pooled = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "\n",
        "            # Classification\n",
        "            logits = self.classifier(pooled)\n",
        "        return logits\n",
        "\n",
        "class CustomOptimizer:\n",
        "    \"\"\"Custom optimizer optimized for T4 GPU\"\"\"\n",
        "    def __init__(self, model_parameters, lr=3e-5, weight_decay=0.01, warmup_steps=200):\n",
        "        self.optimizer = optim.AdamW(model_parameters, lr=lr, weight_decay=weight_decay, eps=1e-8)\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_count = 0\n",
        "        self.base_lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        self.step_count += 1\n",
        "\n",
        "        # Warmup + cosine annealing learning rate\n",
        "        if self.step_count <= self.warmup_steps:\n",
        "            lr = self.base_lr * (self.step_count / self.warmup_steps)\n",
        "        else:\n",
        "            lr = self.base_lr * 0.5 * (1 + np.cos(np.pi * (self.step_count - self.warmup_steps) / 1000))\n",
        "\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def get_lr(self):\n",
        "        return self.optimizer.param_groups[0]['lr']\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Memory-efficient collate function for T4 GPU\"\"\"\n",
        "    waveforms, labels = zip(*batch)\n",
        "\n",
        "    # Stack waveforms and convert to tensor\n",
        "    waveforms = torch.stack([torch.tensor(w, dtype=torch.float32) for w in waveforms])\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return waveforms, labels\n",
        "\n",
        "def train_model(model, train_loader, val_loader, custom_optimizer, criterion,\n",
        "                num_epochs=10, device='cuda'):\n",
        "    model.to(device)\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    # Mixed precision training for T4 GPU\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        train_bar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "        for batch_idx, (waveforms, labels) in enumerate(train_bar):\n",
        "            waveforms, labels = waveforms.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "            custom_optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(waveforms)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            # Mixed precision backward pass\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(custom_optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Predictions\n",
        "            with torch.no_grad():\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                train_preds.extend(preds.cpu().numpy())\n",
        "                train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            train_bar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'LR': f'{custom_optimizer.get_lr():.6f}',\n",
        "                'GPU': f'{torch.cuda.memory_allocated()/1e9:.1f}GB'\n",
        "            })\n",
        "\n",
        "            # Clear cache every 10 batches to prevent OOM\n",
        "            if batch_idx % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        train_acc = accuracy_score(train_labels, train_preds)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(val_loader, desc=\"Validation\")\n",
        "            for waveforms, labels in val_bar:\n",
        "                waveforms, labels = waveforms.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    logits = model(waveforms)\n",
        "                    loss = criterion(logits, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_acc = accuracy_score(val_labels, val_preds)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.4f}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n",
        "\n",
        "        # Save best model locally\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            model_path = os.path.join(os.getcwd(), \"models\", \"best_chicken_classifier.pth\")\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'epoch': epoch,\n",
        "                'model_config': {\n",
        "                    'num_classes': 2,\n",
        "                    'freeze_encoder': True\n",
        "                }\n",
        "            }, model_path)\n",
        "            print(f\"New best model saved! Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Clear GPU cache after each epoch\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Debug function to check data structure\n",
        "def check_data_structure(data_dir):\n",
        "    print(f\"üîç Checking data structure in: {data_dir}\")\n",
        "\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"‚ùå Directory does not exist: {data_dir}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"üìÅ Contents of {data_dir}:\")\n",
        "    for item in os.listdir(data_dir):\n",
        "        item_path = os.path.join(data_dir, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"   üìÇ {item}/\")\n",
        "            # List audio files in subdirectory\n",
        "            audio_files = [f for f in os.listdir(item_path)\n",
        "                          if f.endswith(('.wav', '.mp3', '.flac', '.m4a', '.aac', '.ogg'))]\n",
        "            print(f\"      üéµ {len(audio_files)} audio files\")\n",
        "            if len(audio_files) > 0:\n",
        "                print(f\"      Examples: {audio_files[:3]}...\")\n",
        "        else:\n",
        "            print(f\"   üìÑ {item}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def main():\n",
        "    # Setup GPU\n",
        "    device = setup_gpu()\n",
        "\n",
        "    # Configuration for local development\n",
        "    DATA_DIR = os.path.join(os.getcwd(), \"data\", \"audio\")  # Local data path\n",
        "    MODEL_DIR = os.path.join(os.getcwd(), \"models\")  # Local model path\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    BATCH_SIZE = 8 if device.type == 'cuda' else 4  # Smaller batch for CPU\n",
        "    NUM_EPOCHS = 15 if device.type == 'cuda' else 5  # Fewer epochs for CPU demo\n",
        "    LEARNING_RATE = 3e-5\n",
        "    MAX_AUDIO_LENGTH = 3  # 3 seconds max for memory efficiency\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Data directory: {DATA_DIR}\")\n",
        "    print(f\"Model directory: {MODEL_DIR}\")\n",
        "\n",
        "    # Debug: Check data structure\n",
        "    if not check_data_structure(DATA_DIR):\n",
        "        print(\"‚ö†Ô∏è  No training data found. Using demo mode.\")\n",
        "        return create_demo_model(device, MODEL_DIR)\n",
        "\n",
        "    # Check if data directory exists\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        print(f\"‚ùå Data directory not found: {DATA_DIR}\")\n",
        "        print(\"Creating demo model instead...\")\n",
        "        return create_demo_model(device, MODEL_DIR)\n",
        "\n",
        "    # Check if subdirectories exist\n",
        "    healthy_dir = os.path.join(DATA_DIR, \"Healthy\")\n",
        "    sick_dir = os.path.join(DATA_DIR, \"Sick\")\n",
        "\n",
        "    if not os.path.exists(healthy_dir) or not os.path.exists(sick_dir):\n",
        "        print(f\"‚ùå Required subdirectories not found\")\n",
        "        print(\"Creating demo model instead...\")\n",
        "        return create_demo_model(device, MODEL_DIR)\n",
        "\n",
        "    print(f\"‚úÖ Found directories:\")\n",
        "    print(f\"   Healthy: {len(os.listdir(healthy_dir))} files\")\n",
        "    print(f\"   Sick: {len(os.listdir(sick_dir))} files\")\n",
        "\n",
        "    # Initialize processor\n",
        "    print(\"Loading Wav2Vec2 processor...\")\n",
        "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "    # Create dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    dataset = ChickenSoundDataset(DATA_DIR, processor, max_length=16000*MAX_AUDIO_LENGTH)\n",
        "\n",
        "    if len(dataset) == 0:\n",
        "        print(\"‚ùå No audio files found! Please check your data directory structure.\")\n",
        "        return\n",
        "\n",
        "    # Split dataset (70% train, 15% val, 15% test)\n",
        "    total_size = len(dataset)\n",
        "    train_size = int(0.7 * total_size)\n",
        "    val_size = int(0.15 * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Create data loaders with optimized settings for T4\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                            shuffle=True, collate_fn=collate_fn,\n",
        "                            num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                          shuffle=False, collate_fn=collate_fn,\n",
        "                          num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                           shuffle=False, collate_fn=collate_fn,\n",
        "                           num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "    print(f\"Train samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing Wav2Vec2 model...\")\n",
        "    model = Wav2Vec2Classifier(freeze_encoder=True)  # Freeze encoder for memory efficiency\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Initialize custom optimizer\n",
        "    custom_optimizer = CustomOptimizer(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=len(train_loader)\n",
        "    )\n",
        "\n",
        "    # Loss function with label smoothing\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    # Train model\n",
        "    print(\"üöÄ Starting training...\")\n",
        "    model = train_model(model, train_loader, val_loader, custom_optimizer,\n",
        "                       criterion, NUM_EPOCHS, device)\n",
        "\n",
        "    print(\"‚úÖ Training completed!\")\n",
        "    print(\"Model saved to /content/drive/MyDrive/best_chicken_classifier.pth\")\n",
        "\n",
        "# Function to test a single audio file with confidence threshold\n",
        "def predict_audio(audio_path, model_path=None, confidence_threshold=0.6):\n",
        "    if model_path is None:\n",
        "        model_path = os.path.join(os.getcwd(), \"models\", \"best_chicken_classifier.pth\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load model\n",
        "    model = Wav2Vec2Classifier(freeze_encoder=True)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load and preprocess audio\n",
        "    waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Trim or pad to 3 seconds\n",
        "    max_length = 16000 * 3\n",
        "    waveform = waveform.squeeze(0)\n",
        "    if len(waveform) > max_length:\n",
        "        waveform = waveform[:max_length]\n",
        "    elif len(waveform) < max_length:\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, max_length - len(waveform)))\n",
        "\n",
        "    # Add batch dimension and predict\n",
        "    waveform = waveform.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(waveform)\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "    # Get probabilities for each class\n",
        "    healthy_prob = probabilities[0][0].item()\n",
        "    sick_prob = probabilities[0][1].item()\n",
        "    max_confidence = max(healthy_prob, sick_prob)\n",
        "\n",
        "    # Apply confidence threshold\n",
        "    if max_confidence < confidence_threshold:\n",
        "        prediction_label = \"üü° Uncertain\"\n",
        "        prediction = -1  # Uncertain class\n",
        "    else:\n",
        "        prediction = torch.argmax(probabilities, dim=1).item()\n",
        "        labels = ['üü¢ Healthy', 'üî¥ Sick']\n",
        "        prediction_label = labels[prediction]\n",
        "\n",
        "    print(f\"üêî Prediction: {prediction_label}\")\n",
        "    print(f\"üìä Max Confidence: {max_confidence:.2%}\")\n",
        "    print(f\"üéØ Confidence Threshold: {confidence_threshold:.2%}\")\n",
        "    print(f\"üìà Probabilities:\")\n",
        "    print(f\"   Healthy: {healthy_prob:.2%}\")\n",
        "    print(f\"   Sick: {sick_prob:.2%}\")\n",
        "\n",
        "    return prediction, max_confidence, healthy_prob, sick_prob\n",
        "\n",
        "# Batch prediction function for testing multiple files\n",
        "def predict_batch(audio_files, model_path=None, confidence_threshold=0.6):\n",
        "    if model_path is None:\n",
        "        model_path = os.path.join(os.getcwd(), \"models\", \"best_chicken_classifier.pth\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load model once\n",
        "    model = Wav2Vec2Classifier(freeze_encoder=True)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for audio_path in audio_files:\n",
        "        try:\n",
        "            # Load and preprocess audio\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "            if sample_rate != 16000:\n",
        "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "                waveform = resampler(waveform)\n",
        "\n",
        "            # Trim or pad to 3 seconds\n",
        "            max_length = 16000 * 3\n",
        "            waveform = waveform.squeeze(0)\n",
        "            if len(waveform) > max_length:\n",
        "                waveform = waveform[:max_length]\n",
        "            elif len(waveform) < max_length:\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, max_length - len(waveform)))\n",
        "\n",
        "            # Add batch dimension and predict\n",
        "            waveform = waveform.unsqueeze(0).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    logits = model(waveform)\n",
        "                    probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "            # Get probabilities\n",
        "            healthy_prob = probabilities[0][0].item()\n",
        "            sick_prob = probabilities[0][1].item()\n",
        "            max_confidence = max(healthy_prob, sick_prob)\n",
        "\n",
        "            # Apply confidence threshold\n",
        "            if max_confidence < confidence_threshold:\n",
        "                prediction = -1  # Uncertain\n",
        "                prediction_label = \"Uncertain\"\n",
        "            else:\n",
        "                prediction = torch.argmax(probabilities, dim=1).item()\n",
        "                prediction_label = ['Healthy', 'Sick'][prediction]\n",
        "\n",
        "            results.append({\n",
        "                'file': audio_path,\n",
        "                'prediction': prediction,\n",
        "                'prediction_label': prediction_label,\n",
        "                'confidence': max_confidence,\n",
        "                'healthy_prob': healthy_prob,\n",
        "                'sick_prob': sick_prob\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {e}\")\n",
        "            results.append({\n",
        "                'file': audio_path,\n",
        "                'prediction': -2,  # Error\n",
        "                'prediction_label': \"Error\",\n",
        "                'confidence': 0.0,\n",
        "                'healthy_prob': 0.0,\n",
        "                'sick_prob': 0.0\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to evaluate model with different confidence thresholds\n",
        "def evaluate_confidence_thresholds(test_loader, model_path=None):\n",
        "    if model_path is None:\n",
        "        model_path = os.path.join(os.getcwd(), \"models\", \"best_chicken_classifier.pth\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load model\n",
        "    model = Wav2Vec2Classifier(freeze_encoder=True)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    # Get all predictions first\n",
        "    with torch.no_grad():\n",
        "        for waveforms, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            waveforms, labels = waveforms.to(device), labels.to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(waveforms)\n",
        "                probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "            all_predictions.extend(torch.argmax(probabilities, dim=1).cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probabilities.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    all_probabilities = np.array(all_probabilities)\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Test different confidence thresholds\n",
        "    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "    print(\"\\nüìä Confidence Threshold Analysis:\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        # Calculate max confidence for each prediction\n",
        "        max_confidences = np.max(all_probabilities, axis=1)\n",
        "\n",
        "        # Apply threshold\n",
        "        confident_mask = max_confidences >= threshold\n",
        "\n",
        "        if np.sum(confident_mask) == 0:\n",
        "            print(f\"Threshold {threshold:.1f}: No confident predictions\")\n",
        "            continue\n",
        "\n",
        "        # Calculate accuracy only on confident predictions\n",
        "        confident_predictions = all_predictions[confident_mask]\n",
        "        confident_labels = all_labels[confident_mask]\n",
        "\n",
        "        accuracy = accuracy_score(confident_labels, confident_predictions)\n",
        "        coverage = np.sum(confident_mask) / len(all_labels)\n",
        "\n",
        "        # Count predictions by class\n",
        "        healthy_count = np.sum(confident_predictions == 0)\n",
        "        sick_count = np.sum(confident_predictions == 1)\n",
        "\n",
        "        print(f\"Threshold {threshold:.1f}: Acc={accuracy:.3f}, Coverage={coverage:.3f} ({np.sum(confident_mask)}/{len(all_labels)}), H={healthy_count}, S={sick_count}\")\n",
        "\n",
        "    # Show distribution of max confidences\n",
        "    print(\"\\nüìà Confidence Distribution:\")\n",
        "    max_confidences = np.max(all_probabilities, axis=1)\n",
        "    print(f\"Mean confidence: {np.mean(max_confidences):.3f}\")\n",
        "    print(f\"Std confidence: {np.std(max_confidences):.3f}\")\n",
        "    print(f\"Min confidence: {np.min(max_confidences):.3f}\")\n",
        "    print(f\"Max confidence: {np.max(max_confidences):.3f}\")\n",
        "\n",
        "    # Show class-wise confidence\n",
        "    healthy_confidences = max_confidences[all_labels == 0]\n",
        "    sick_confidences = max_confidences[all_labels == 1]\n",
        "\n",
        "    print(f\"\\nHealthy samples - Mean confidence: {np.mean(healthy_confidences):.3f}\")\n",
        "    print(f\"Sick samples - Mean confidence: {np.mean(sick_confidences):.3f}\")\n",
        "\n",
        "    return all_predictions, all_labels, all_probabilities\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# Example usage after training:\n",
        "# Test single file with different confidence thresholds\n",
        "# predict_audio(\"/content/drive/MyDrive/test_chicken_sound.wav\", confidence_threshold=0.7)\n",
        "\n",
        "# Test batch of files\n",
        "# audio_files = [\"/path/to/file1.wav\", \"/path/to/file2.wav\"]\n",
        "# results = predict_batch(audio_files, confidence_threshold=0.6)\n",
        "\n",
        "# Evaluate different confidence thresholds on test set\n",
        "# evaluate_confidence_thresholds(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469,
          "referenced_widgets": [
            "7dc36fea453b42e8a041aac002c67a99",
            "167ee80494dd4e48b8145573c46ce9aa",
            "c89ade4490f540fa972b12a79fafcda4",
            "c6f8d96e656e49f69e4ecaf74f896b5a",
            "247d324553744d2b95eaa8cfa378b29e",
            "601301c94df243a093c6c8b719538cbb",
            "493e86025abf407d83d293f652c94f5c",
            "084b64c6ddb2469e8db9a060215bf9b1",
            "543425a6a22b4da58d786c4c01201068",
            "9981da3e32cb4485a383ea120a50a186",
            "79634d26baad4e61876843d98e7afcca",
            "7b908b2f1f0149f68272b647d104db36",
            "3d6a7be1778647a495072c487d4ab42d",
            "fed4260e346942dc9a895c5b536d1ac5",
            "2474a5b6fe0c4144bbe803553fc7afbb",
            "3c89c733d60d4957add368a925b0548f",
            "9b590c458adb4cd0a72d4c33189156bf",
            "e4decd5fef2548b4a67028535be0e6b7",
            "832dc389d7ef4eaeb9e27b821d78d508",
            "4b125576c7064440bbb11cec1b2e8d7b",
            "7bd62c19d3f048b2804429b4dabd3954",
            "51fa22de1cfe4538aa429c803fa74081",
            "131fda7781954326ba65b44dccc841dd",
            "dad78b4371a0453eafffc5b11696ac9a",
            "e656d9acdb99453fa7050f67d8c9dee8",
            "383f3b3183d648c5ba71a16fc41b6bfe",
            "e4cca20e778e411e90221ec578869ac8",
            "a23b418af72c4ed7af016c30460e7be4",
            "66da8e66def64bdaac0f7ad67e70d710",
            "839cbbcafca04203ad9b9e0f02c761fa",
            "5d66468817044d4f8c186622e6272601",
            "5358ab5d281444a78e5ff85915a2b92e",
            "8a61bd9ef841483d919c27247f4c7de1"
          ]
        },
        "id": "DbzHSQWfdwmQ",
        "outputId": "5111010b-9f91-4d69-cf76-950f4dee8cbd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dc36fea453b42e8a041aac002c67a99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating healthy:   0%|          | 0/2139 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b908b2f1f0149f68272b647d104db36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating sick:   0%|          | 0/2121 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "131fda7781954326ba65b44dccc841dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating noise: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Model creation failed: ('Keyword argument not understood:', 'from_tf')\n",
            "ERROR:__main__:Training pipeline failed: ('Keyword argument not understood:', 'from_tf')\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "('Keyword argument not understood:', 'from_tf')",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4184577512.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4184577512.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         model, history = trainer.train(\n\u001b[0m\u001b[1;32m    757\u001b[0m             \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         )\n",
            "\u001b[0;32m/tmp/ipython-input-4184577512.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, class_weights, num_classes)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating and compiling model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4184577512.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(self, num_classes)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0;31m# Load pre-trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             wav2vec2 = TFWav2Vec2Model.from_pretrained(\n\u001b[0m\u001b[1;32m    491\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2932\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2933\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2935\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtf_to_pt_weight_rename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tf_to_pt_weight_rename\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_tf_wav2vec2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTFWav2Vec2Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFWav2Vec2PreTrainedModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWav2Vec2Config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1504\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1505\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwav2vec2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFWav2Vec2MainLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wav2vec2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_tf_wav2vec2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1353\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1354\u001b[0m         logger.warning(\n\u001b[1;32m   1355\u001b[0m             \u001b[0;34mf\"\\n{self.__class__.__name__} has backpropagation operations that are NOT supported on CPU. If you wish \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/generic_utils.py\u001b[0m in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'from_tf')"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Robust Wav2Vec2 Audio Classification Trainer\n",
        "Addresses bias, NaN losses, and class imbalance issues\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torchaudio\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import warnings\n",
        "import json\n",
        "import datetime\n",
        "from typing import List, Tuple, Dict, Optional, Union\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from collections import Counter\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class RobustConfig:\n",
        "    \"\"\"Robust configuration with bias prevention measures\"\"\"\n",
        "\n",
        "    # Model configuration\n",
        "    MODEL_ID: str = \"facebook/wav2vec2-base-960h\"\n",
        "    SAMPLE_RATE: int = 16000\n",
        "    DURATION: float = 3.0  # Reduced for better memory management\n",
        "    MAX_LENGTH: int = 48000  # 3 seconds at 16kHz\n",
        "\n",
        "    # Training configuration - Conservative settings to prevent NaN\n",
        "    BATCH_SIZE: int = 8  # Smaller batch size for stability\n",
        "    EPOCHS: int = 20\n",
        "    LEARNING_RATE: float = 1e-5  # Much lower learning rate\n",
        "    WARMUP_RATIO: float = 0.1\n",
        "    WEIGHT_DECAY: float = 0.01\n",
        "    GRADIENT_CLIP_NORM: float = 1.0  # Gradient clipping to prevent explosion\n",
        "\n",
        "    # Data configuration\n",
        "    TRAIN_SPLIT: float = 0.7\n",
        "    VAL_SPLIT: float = 0.15\n",
        "    TEST_SPLIT: float = 0.15\n",
        "\n",
        "    # Bias prevention configuration\n",
        "    MAX_CLASS_IMBALANCE_RATIO: float = 3.0  # Max ratio between largest and smallest class\n",
        "    MIN_SAMPLES_PER_CLASS: int = 50  # Minimum samples required per class\n",
        "    SYNTHETIC_NOISE_RATIO: float = 0.3  # Max 30% synthetic noise in dataset\n",
        "\n",
        "    # Audio validation configuration\n",
        "    MIN_AUDIO_LENGTH: float = 1.0  # Minimum 1 second\n",
        "    MAX_AUDIO_LENGTH: float = 10.0  # Maximum 10 seconds\n",
        "    MIN_SNR_DB: float = 5.0  # Minimum signal-to-noise ratio\n",
        "\n",
        "    # Augmentation configuration - Reduced to prevent overfitting\n",
        "    USE_AUGMENTATION: bool = True\n",
        "    AUGMENTATION_PROB: float = 0.3  # Reduced probability\n",
        "\n",
        "    # Directory configuration\n",
        "    BASE_DIR: str = './data'\n",
        "    HEALTHY_DIR: str = './data/healthy'\n",
        "    SICK_DIR: str = './data/sick'\n",
        "    NOISE_DIR: str = './data/noise'\n",
        "    OUTPUT_DIR: str = './robust_model_output'\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration\"\"\"\n",
        "        assert self.TRAIN_SPLIT + self.VAL_SPLIT + self.TEST_SPLIT == 1.0\n",
        "        assert self.MAX_CLASS_IMBALANCE_RATIO >= 1.0\n",
        "        assert self.MIN_SAMPLES_PER_CLASS > 0\n",
        "        assert 0 < self.SYNTHETIC_NOISE_RATIO < 1.0\n",
        "\n",
        "\n",
        "class AudioValidator:\n",
        "    \"\"\"Robust audio validation to prevent NaN issues\"\"\"\n",
        "\n",
        "    def __init__(self, config: RobustConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def validate_audio_file(self, file_path: str) -> Tuple[bool, str]:\n",
        "        \"\"\"Comprehensive audio file validation\"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(file_path):\n",
        "                return False, \"File does not exist\"\n",
        "\n",
        "            if os.path.getsize(file_path) < 1000:  # Less than 1KB\n",
        "                return False, \"File too small\"\n",
        "\n",
        "            # Load audio and check basic properties\n",
        "            try:\n",
        "                waveform, sr = librosa.load(file_path, sr=None, mono=True)\n",
        "            except Exception as e:\n",
        "                return False, f\"Cannot load audio: {e}\"\n",
        "\n",
        "            # Check duration\n",
        "            duration = len(waveform) / sr\n",
        "            if duration < self.config.MIN_AUDIO_LENGTH:\n",
        "                return False, f\"Audio too short: {duration:.2f}s\"\n",
        "            if duration > self.config.MAX_AUDIO_LENGTH:\n",
        "                return False, f\"Audio too long: {duration:.2f}s\"\n",
        "\n",
        "            # Check for silence or constant values\n",
        "            if np.std(waveform) < 1e-6:\n",
        "                return False, \"Audio is silent or constant\"\n",
        "\n",
        "            # Check for clipping\n",
        "            clipping_ratio = np.mean(np.abs(waveform) > 0.95)\n",
        "            if clipping_ratio > 0.1:\n",
        "                return False, f\"Audio heavily clipped: {clipping_ratio:.2%}\"\n",
        "\n",
        "            # Check signal-to-noise ratio (rough estimate)\n",
        "            signal_power = np.mean(waveform ** 2)\n",
        "            if signal_power < 1e-8:\n",
        "                return False, \"Signal power too low\"\n",
        "\n",
        "            # Check for NaN or infinite values\n",
        "            if not np.isfinite(waveform).all():\n",
        "                return False, \"Audio contains NaN or infinite values\"\n",
        "\n",
        "            return True, \"Valid\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, f\"Validation error: {e}\"\n",
        "\n",
        "    def estimate_snr(self, waveform: np.ndarray) -> float:\n",
        "        \"\"\"Estimate signal-to-noise ratio\"\"\"\n",
        "        try:\n",
        "            # Simple SNR estimation using signal variance vs noise floor\n",
        "            signal_power = np.var(waveform)\n",
        "\n",
        "            # Estimate noise floor from quietest 10% of samples\n",
        "            sorted_abs = np.sort(np.abs(waveform))\n",
        "            noise_floor = np.mean(sorted_abs[:len(sorted_abs)//10]) ** 2\n",
        "\n",
        "            if noise_floor == 0:\n",
        "                return float('inf')\n",
        "\n",
        "            snr_linear = signal_power / noise_floor\n",
        "            snr_db = 10 * np.log10(max(snr_linear, 1e-10))\n",
        "\n",
        "            return snr_db\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "\n",
        "class RobustAudioProcessor:\n",
        "    \"\"\"Robust audio processing with NaN prevention\"\"\"\n",
        "\n",
        "    def __init__(self, config: RobustConfig):\n",
        "        self.config = config\n",
        "        self.validator = AudioValidator(config)\n",
        "\n",
        "    def load_and_preprocess_audio(self, file_path: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"Load and preprocess audio with robust error handling\"\"\"\n",
        "        try:\n",
        "            # Validate file first\n",
        "            is_valid, reason = self.validator.validate_audio_file(file_path)\n",
        "            if not is_valid:\n",
        "                logger.warning(f\"Invalid audio {file_path}: {reason}\")\n",
        "                return None\n",
        "\n",
        "            # Load audio\n",
        "            waveform, sr = librosa.load(\n",
        "                file_path,\n",
        "                sr=self.config.SAMPLE_RATE,\n",
        "                mono=True,\n",
        "                res_type='kaiser_fast'\n",
        "            )\n",
        "\n",
        "            # Handle length - intelligent cropping/padding\n",
        "            target_length = self.config.MAX_LENGTH\n",
        "\n",
        "            if len(waveform) > target_length:\n",
        "                # For longer audio, take the most energetic segment\n",
        "                hop_length = len(waveform) - target_length\n",
        "                if hop_length > 0:\n",
        "                    # Find the segment with highest energy\n",
        "                    energies = []\n",
        "                    for i in range(0, hop_length + 1, hop_length // 10 + 1):\n",
        "                        segment = waveform[i:i + target_length]\n",
        "                        energy = np.sum(segment ** 2)\n",
        "                        energies.append((energy, i))\n",
        "\n",
        "                    best_start = max(energies)[1]\n",
        "                    waveform = waveform[best_start:best_start + target_length]\n",
        "                else:\n",
        "                    waveform = waveform[:target_length]\n",
        "\n",
        "            elif len(waveform) < target_length:\n",
        "                # Pad with reflection to avoid discontinuities\n",
        "                pad_length = target_length - len(waveform)\n",
        "                if len(waveform) > pad_length:\n",
        "                    # Reflect the signal\n",
        "                    waveform = np.pad(waveform, (0, pad_length), 'reflect')\n",
        "                else:\n",
        "                    # Zero pad if signal is too short for reflection\n",
        "                    waveform = np.pad(waveform, (0, pad_length), 'constant')\n",
        "\n",
        "            # Robust normalization\n",
        "            waveform = self._robust_normalize(waveform)\n",
        "\n",
        "            # Final validation\n",
        "            if not np.isfinite(waveform).all():\n",
        "                logger.warning(f\"NaN/Inf detected in processed audio: {file_path}\")\n",
        "                return None\n",
        "\n",
        "            return waveform.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _robust_normalize(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Robust normalization that prevents NaN\"\"\"\n",
        "        try:\n",
        "            # Remove DC offset\n",
        "            waveform = waveform - np.mean(waveform)\n",
        "\n",
        "            # Robust scaling using percentiles instead of max\n",
        "            p99 = np.percentile(np.abs(waveform), 99)\n",
        "            if p99 > 1e-8:\n",
        "                waveform = waveform / p99 * 0.8  # Scale to 80% to avoid clipping\n",
        "\n",
        "            # Clip to prevent extreme values\n",
        "            waveform = np.clip(waveform, -1.0, 1.0)\n",
        "\n",
        "            return waveform\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Normalization error: {e}\")\n",
        "            return np.zeros_like(waveform)\n",
        "\n",
        "    def apply_augmentation(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Conservative augmentation to prevent overfitting\"\"\"\n",
        "        if not self.config.USE_AUGMENTATION:\n",
        "            return waveform\n",
        "\n",
        "        try:\n",
        "            augmented = waveform.copy()\n",
        "\n",
        "            # Time shifting (small)\n",
        "            if np.random.random() < self.config.AUGMENTATION_PROB:\n",
        "                shift = int(np.random.uniform(-0.05, 0.05) * len(augmented))\n",
        "                augmented = np.roll(augmented, shift)\n",
        "\n",
        "            # Volume scaling (conservative)\n",
        "            if np.random.random() < self.config.AUGMENTATION_PROB:\n",
        "                scale = np.random.uniform(0.8, 1.2)\n",
        "                augmented = augmented * scale\n",
        "\n",
        "            # Add small amount of noise\n",
        "            if np.random.random() < self.config.AUGMENTATION_PROB * 0.5:\n",
        "                noise_level = np.random.uniform(0.001, 0.005)\n",
        "                noise = np.random.normal(0, noise_level, len(augmented))\n",
        "                augmented = augmented + noise\n",
        "\n",
        "            # Ensure no clipping or NaN\n",
        "            augmented = np.clip(augmented, -1.0, 1.0)\n",
        "\n",
        "            if not np.isfinite(augmented).all():\n",
        "                return waveform  # Return original if augmentation failed\n",
        "\n",
        "            return augmented\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Augmentation failed: {e}\")\n",
        "            return waveform\n",
        "\n",
        "\n",
        "class BiasPreventionDatasetBuilder:\n",
        "    \"\"\"Dataset builder with bias prevention measures\"\"\"\n",
        "\n",
        "    def __init__(self, config: RobustConfig):\n",
        "        self.config = config\n",
        "        self.processor = RobustAudioProcessor(config)\n",
        "\n",
        "    def collect_and_balance_data(self) -> Tuple[List[str], List[int], List[str]]:\n",
        "        \"\"\"Collect data with bias prevention\"\"\"\n",
        "        logger.info(\"Collecting and balancing dataset...\")\n",
        "\n",
        "        # Collect files by category\n",
        "        healthy_files = self._collect_files(self.config.HEALTHY_DIR, \"healthy\")\n",
        "        sick_files = self._collect_files(self.config.SICK_DIR, \"sick\")\n",
        "        noise_files = self._collect_files(self.config.NOISE_DIR, \"noise\")\n",
        "\n",
        "        logger.info(f\"Raw file counts - Healthy: {len(healthy_files)}, Sick: {len(sick_files)}, Noise: {len(noise_files)}\")\n",
        "\n",
        "        # Validate files\n",
        "        healthy_files = self._validate_files(healthy_files, \"healthy\")\n",
        "        sick_files = self._validate_files(sick_files, \"sick\")\n",
        "        noise_files = self._validate_files(noise_files, \"noise\")\n",
        "\n",
        "        logger.info(f\"Valid file counts - Healthy: {len(healthy_files)}, Sick: {len(sick_files)}, Noise: {len(noise_files)}\")\n",
        "\n",
        "        # Check minimum requirements\n",
        "        if len(healthy_files) < self.config.MIN_SAMPLES_PER_CLASS:\n",
        "            raise ValueError(f\"Insufficient healthy samples: {len(healthy_files)} < {self.config.MIN_SAMPLES_PER_CLASS}\")\n",
        "        if len(sick_files) < self.config.MIN_SAMPLES_PER_CLASS:\n",
        "            raise ValueError(f\"Insufficient sick samples: {len(sick_files)} < {self.config.MIN_SAMPLES_PER_CLASS}\")\n",
        "\n",
        "        # Balance dataset to prevent bias\n",
        "        healthy_files, sick_files, noise_files = self._balance_classes(\n",
        "            healthy_files, sick_files, noise_files\n",
        "        )\n",
        "\n",
        "        # Determine class configuration\n",
        "        if len(noise_files) > 0:\n",
        "            all_files = healthy_files + sick_files + noise_files\n",
        "            all_labels = ([0] * len(healthy_files) +\n",
        "                         [1] * len(sick_files) +\n",
        "                         [2] * len(noise_files))\n",
        "            class_names = ['healthy', 'sick', 'noise']\n",
        "        else:\n",
        "            all_files = healthy_files + sick_files\n",
        "            all_labels = [0] * len(healthy_files) + [1] * len(sick_files)\n",
        "            class_names = ['healthy', 'sick']\n",
        "\n",
        "        logger.info(f\"Final balanced dataset - Classes: {class_names}\")\n",
        "        for i, name in enumerate(class_names):\n",
        "            count = sum(1 for label in all_labels if label == i)\n",
        "            logger.info(f\"  {name}: {count} samples\")\n",
        "\n",
        "        return all_files, all_labels, class_names\n",
        "\n",
        "    def _collect_files(self, directory: str, category: str) -> List[str]:\n",
        "        \"\"\"Collect audio files from directory\"\"\"\n",
        "        files = []\n",
        "        if os.path.exists(directory):\n",
        "            for ext in ['.wav', '.mp3', '.flac', '.m4a']:\n",
        "                files.extend(Path(directory).rglob(f'*{ext}'))\n",
        "\n",
        "        return [str(f) for f in files]\n",
        "\n",
        "    def _validate_files(self, files: List[str], category: str) -> List[str]:\n",
        "        \"\"\"Validate audio files\"\"\"\n",
        "        valid_files = []\n",
        "\n",
        "        logger.info(f\"Validating {len(files)} {category} files...\")\n",
        "\n",
        "        for file_path in tqdm(files, desc=f\"Validating {category}\"):\n",
        "            is_valid, reason = self.processor.validator.validate_audio_file(file_path)\n",
        "            if is_valid:\n",
        "                valid_files.append(file_path)\n",
        "            else:\n",
        "                logger.debug(f\"Rejected {file_path}: {reason}\")\n",
        "\n",
        "        rejection_rate = (len(files) - len(valid_files)) / len(files) if files else 0\n",
        "        logger.info(f\"{category} validation: {len(valid_files)}/{len(files)} valid ({rejection_rate:.1%} rejected)\")\n",
        "\n",
        "        return valid_files\n",
        "\n",
        "    def _balance_classes(self, healthy_files: List[str], sick_files: List[str],\n",
        "                        noise_files: List[str]) -> Tuple[List[str], List[str], List[str]]:\n",
        "        \"\"\"Balance classes to prevent bias\"\"\"\n",
        "\n",
        "        # Calculate target sizes\n",
        "        counts = [len(healthy_files), len(sick_files)]\n",
        "        if noise_files:\n",
        "            counts.append(len(noise_files))\n",
        "\n",
        "        min_count = min(counts)\n",
        "        max_count = max(counts)\n",
        "\n",
        "        # Check if balancing is needed\n",
        "        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
        "\n",
        "        if imbalance_ratio > self.config.MAX_CLASS_IMBALANCE_RATIO:\n",
        "            logger.info(f\"Balancing classes (ratio: {imbalance_ratio:.2f} > {self.config.MAX_CLASS_IMBALANCE_RATIO})\")\n",
        "\n",
        "            # Calculate target size (use median to avoid extreme reduction)\n",
        "            target_size = int(np.median(counts))\n",
        "\n",
        "            # Limit noise files to prevent overwhelming real data\n",
        "            if noise_files:\n",
        "                max_noise = int(target_size * self.config.SYNTHETIC_NOISE_RATIO / (1 - self.config.SYNTHETIC_NOISE_RATIO))\n",
        "                if len(noise_files) > max_noise:\n",
        "                    noise_files = np.random.choice(noise_files, max_noise, replace=False).tolist()\n",
        "                    logger.info(f\"Limited noise files to {max_noise}\")\n",
        "\n",
        "            # Balance healthy and sick files\n",
        "            if len(healthy_files) > target_size:\n",
        "                healthy_files = np.random.choice(healthy_files, target_size, replace=False).tolist()\n",
        "            if len(sick_files) > target_size:\n",
        "                sick_files = np.random.choice(sick_files, target_size, replace=False).tolist()\n",
        "\n",
        "        logger.info(f\"Balanced dataset - Healthy: {len(healthy_files)}, Sick: {len(sick_files)}, Noise: {len(noise_files)}\")\n",
        "\n",
        "        return healthy_files, sick_files, noise_files\n",
        "\n",
        "\n",
        "class RobustTrainer:\n",
        "    \"\"\"Robust trainer with NaN prevention and bias mitigation\"\"\"\n",
        "\n",
        "    def __init__(self, config: RobustConfig):\n",
        "        self.config = config\n",
        "        self.processor = RobustAudioProcessor(config)\n",
        "\n",
        "        # Setup directories\n",
        "        os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "        # Configure TensorFlow for stability\n",
        "        self._configure_tensorflow()\n",
        "\n",
        "    def _configure_tensorflow(self):\n",
        "        \"\"\"Configure TensorFlow for stable training\"\"\"\n",
        "        # Enable mixed precision with loss scaling\n",
        "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "        tf.keras.mixed_precision.set_global_policy(policy)\n",
        "\n",
        "        # Configure GPU memory growth\n",
        "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                for gpu in gpus:\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            except RuntimeError as e:\n",
        "                logger.warning(f\"GPU configuration warning: {e}\")\n",
        "\n",
        "    def create_dataset(self, file_paths: List[str], labels: List[int],\n",
        "                      is_training: bool = True) -> tf.data.Dataset:\n",
        "        \"\"\"Create robust TensorFlow dataset\"\"\"\n",
        "\n",
        "        def load_audio_tf(file_path, label):\n",
        "            def load_and_process(path):\n",
        "                path_str = path.numpy().decode('utf-8')\n",
        "                waveform = self.processor.load_and_preprocess_audio(path_str)\n",
        "\n",
        "                if waveform is None:\n",
        "                    # Return zeros if loading failed\n",
        "                    waveform = np.zeros(self.config.MAX_LENGTH, dtype=np.float32)\n",
        "                else:\n",
        "                    # Apply augmentation for training\n",
        "                    if is_training:\n",
        "                        waveform = self.processor.apply_augmentation(waveform)\n",
        "\n",
        "                return waveform\n",
        "\n",
        "            audio_data = tf.py_function(\n",
        "                load_and_process,\n",
        "                inp=[file_path],\n",
        "                Tout=tf.float32\n",
        "            )\n",
        "            audio_data.set_shape([self.config.MAX_LENGTH])\n",
        "\n",
        "            return audio_data, label\n",
        "\n",
        "        # Create dataset\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "\n",
        "        if is_training:\n",
        "            dataset = dataset.shuffle(buffer_size=min(1000, len(file_paths)))\n",
        "\n",
        "        dataset = dataset.map(\n",
        "            load_audio_tf,\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "\n",
        "        # Filter out any invalid samples\n",
        "        dataset = dataset.filter(\n",
        "            lambda audio, label: tf.reduce_all(tf.math.is_finite(audio))\n",
        "        )\n",
        "\n",
        "        dataset = dataset.batch(self.config.BATCH_SIZE)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def create_model(self, num_classes: int):\n",
        "        \"\"\"Create robust model with proper initialization\"\"\"\n",
        "        try:\n",
        "            from transformers import TFWav2Vec2Model\n",
        "\n",
        "            # Load pre-trained model\n",
        "            wav2vec2 = TFWav2Vec2Model.from_pretrained(\n",
        "                self.config.MODEL_ID,\n",
        "                from_tf=True\n",
        "            )\n",
        "\n",
        "            # Freeze some layers to prevent overfitting\n",
        "            for layer in wav2vec2.layers[:-2]:\n",
        "                layer.trainable = False\n",
        "\n",
        "            # Create classification head\n",
        "            inputs = tf.keras.Input(shape=(self.config.MAX_LENGTH,), name='input_values')\n",
        "\n",
        "            # Extract features\n",
        "            wav2vec2_outputs = wav2vec2(inputs, training=False)\n",
        "            features = wav2vec2_outputs.last_hidden_state\n",
        "\n",
        "            # Global average pooling\n",
        "            pooled = tf.keras.layers.GlobalAveragePooling1D()(features)\n",
        "\n",
        "            # Classification layers with proper regularization\n",
        "            x = tf.keras.layers.Dropout(0.3)(pooled)\n",
        "            x = tf.keras.layers.Dense(256, activation='relu',\n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "            x = tf.keras.layers.Dropout(0.2)(x)\n",
        "            x = tf.keras.layers.Dense(64, activation='relu',\n",
        "                                    kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "\n",
        "            outputs = tf.keras.layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
        "\n",
        "            model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Model creation failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def compile_model(self, model, class_weights: Dict[int, float]):\n",
        "        \"\"\"Compile model with robust settings\"\"\"\n",
        "\n",
        "        # Use a more stable optimizer\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=self.config.LEARNING_RATE,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.999,\n",
        "            epsilon=1e-8,\n",
        "            clipnorm=self.config.GRADIENT_CLIP_NORM\n",
        "        )\n",
        "\n",
        "        # Use focal loss to handle class imbalance\n",
        "        def focal_loss(alpha=0.25, gamma=2.0):\n",
        "            def focal_loss_fixed(y_true, y_pred):\n",
        "                epsilon = tf.keras.backend.epsilon()\n",
        "                y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
        "\n",
        "                # Convert to one-hot if needed\n",
        "                if len(y_true.shape) == 1:\n",
        "                    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[1])\n",
        "\n",
        "                # Calculate focal loss\n",
        "                ce = -y_true * tf.math.log(y_pred)\n",
        "                weight = alpha * y_true * tf.pow((1 - y_pred), gamma)\n",
        "                fl = weight * ce\n",
        "\n",
        "                return tf.reduce_mean(tf.reduce_sum(fl, axis=1))\n",
        "\n",
        "            return focal_loss_fixed\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=focal_loss(),\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def create_callbacks(self):\n",
        "        \"\"\"Create training callbacks for stability\"\"\"\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=8,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=4,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                os.path.join(self.config.OUTPUT_DIR, 'best_model.h5'),\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            # NaN termination callback\n",
        "            tf.keras.callbacks.TerminateOnNaN(),\n",
        "            # Custom callback for monitoring\n",
        "            RobustTrainingCallback()\n",
        "        ]\n",
        "\n",
        "        return callbacks\n",
        "\n",
        "    def train(self, train_dataset, val_dataset, class_weights, num_classes):\n",
        "        \"\"\"Train model with robust settings\"\"\"\n",
        "\n",
        "        logger.info(\"Creating and compiling model...\")\n",
        "        model = self.create_model(num_classes)\n",
        "        model = self.compile_model(model, class_weights)\n",
        "\n",
        "        logger.info(\"Model architecture:\")\n",
        "        model.summary(print_fn=logger.info)\n",
        "\n",
        "        callbacks = self.create_callbacks()\n",
        "\n",
        "        logger.info(f\"Starting training for {self.config.EPOCHS} epochs...\")\n",
        "\n",
        "        try:\n",
        "            history = model.fit(\n",
        "                train_dataset,\n",
        "                epochs=self.config.EPOCHS,\n",
        "                validation_data=val_dataset,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1,\n",
        "                class_weight=class_weights\n",
        "            )\n",
        "\n",
        "            logger.info(\"Training completed successfully!\")\n",
        "            return model, history\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Training failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def evaluate_model(self, model, test_dataset, class_names):\n",
        "        \"\"\"Evaluate model with comprehensive metrics\"\"\"\n",
        "        logger.info(\"Evaluating model...\")\n",
        "\n",
        "        try:\n",
        "            # Get predictions\n",
        "            predictions = model.predict(test_dataset, verbose=1)\n",
        "\n",
        "            # Extract true labels\n",
        "            y_true = []\n",
        "            for _, labels in test_dataset:\n",
        "                y_true.extend(labels.numpy())\n",
        "            y_true = np.array(y_true)\n",
        "\n",
        "            y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "            logger.info(f\"Balanced Accuracy: {accuracy:.4f}\")\n",
        "            logger.info(\"\\nClassification Report:\")\n",
        "            print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "            # Confusion matrix\n",
        "            cm = confusion_matrix(y_true, y_pred)\n",
        "            self._plot_confusion_matrix(cm, class_names)\n",
        "\n",
        "            return {\n",
        "                'accuracy': accuracy,\n",
        "                'y_true': y_true,\n",
        "                'y_pred': y_pred,\n",
        "                'predictions': predictions,\n",
        "                'confusion_matrix': cm\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Evaluation failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _plot_confusion_matrix(self, cm, class_names):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.config.OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class RobustTrainingCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Custom callback for monitoring training stability\"\"\"\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            logs = {}\n",
        "\n",
        "        # Check for NaN or extreme values\n",
        "        for key, value in logs.items():\n",
        "            if not np.isfinite(value):\n",
        "                logger.error(f\"NaN/Inf detected in {key}: {value}\")\n",
        "                self.model.stop_training = True\n",
        "                return\n",
        "\n",
        "        # Log progress\n",
        "        logger.info(f\"Epoch {epoch + 1}: \"\n",
        "                   f\"loss={logs.get('loss', 0):.4f}, \"\n",
        "                   f\"acc={logs.get('accuracy', 0):.4f}, \"\n",
        "                   f\"val_loss={logs.get('val_loss', 0):.4f}, \"\n",
        "                   f\"val_acc={logs.get('val_accuracy', 0):.4f}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "    logger.info(\"Starting robust Wav2Vec2 training pipeline...\")\n",
        "\n",
        "    # Configuration\n",
        "    config = RobustConfig()\n",
        "\n",
        "    config.BASE_DIR = '/content/smartearsaudio/sm'\n",
        "    config.HEALTHY_DIR = '/content/smartearsaudio/sm/Healthy'\n",
        "    config.SICK_DIR = '/content/smartearsaudio/sm/Sick'\n",
        "\n",
        "    try:\n",
        "        # Build dataset\n",
        "        dataset_builder = BiasPreventionDatasetBuilder(config)\n",
        "        all_files, all_labels, class_names = dataset_builder.collect_and_balance_data()\n",
        "\n",
        "        if len(all_files) == 0:\n",
        "            raise ValueError(\"No valid audio files found!\")\n",
        "\n",
        "        # Split data\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            all_files, all_labels,\n",
        "            test_size=config.TEST_SPLIT,\n",
        "            random_state=42,\n",
        "            stratify=all_labels\n",
        "        )\n",
        "\n",
        "        val_size = config.VAL_SPLIT / (1 - config.TEST_SPLIT)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp,\n",
        "            test_size=val_size,\n",
        "            random_state=42,\n",
        "            stratify=y_temp\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Data split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
        "\n",
        "        # Calculate class weights\n",
        "        class_weights = compute_class_weight(\n",
        "            'balanced',\n",
        "            classes=np.unique(y_train),\n",
        "            y=y_train\n",
        "        )\n",
        "        class_weight_dict = dict(enumerate(class_weights))\n",
        "        logger.info(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "        # Create trainer\n",
        "        trainer = RobustTrainer(config)\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = trainer.create_dataset(X_train, y_train, is_training=True)\n",
        "        val_dataset = trainer.create_dataset(X_val, y_val, is_training=False)\n",
        "        test_dataset = trainer.create_dataset(X_test, y_test, is_training=False)\n",
        "\n",
        "        # Train model\n",
        "        model, history = trainer.train(\n",
        "            train_dataset, val_dataset, class_weight_dict, len(class_names)\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        results = trainer.evaluate_model(model, test_dataset, class_names)\n",
        "\n",
        "        # Save model and results\n",
        "        model.save(os.path.join(config.OUTPUT_DIR, 'final_model.h5'))\n",
        "\n",
        "        with open(os.path.join(config.OUTPUT_DIR, 'training_results.json'), 'w') as f:\n",
        "            json.dump({\n",
        "                'class_names': class_names,\n",
        "                'accuracy': float(results.get('accuracy', 0)),\n",
        "                'class_weights': {str(k): float(v) for k, v in class_weight_dict.items()},\n",
        "                'config': config.__dict__\n",
        "            }, f, indent=2)\n",
        "\n",
        "        logger.info(f\"Training completed! Results saved to {config.OUTPUT_DIR}\")\n",
        "\n",
        "        return model, results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Training pipeline failed: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, results = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9f08f3c17b304109a85f2b10bcd0969f",
            "52d65b2034764ffaa573c7461d6015cc",
            "6a0b8b62014b4132811a72df91eea269",
            "5f9894a73b7346dbaedc7d30a2ee3ea6",
            "4907abcdf10543a6a6b5c4c142b37d78",
            "22ecead0b09f4bc889d9df01391b6a0a",
            "67d6757a0b01458dbb5ac755fdf15d10",
            "d5608b160e934eeca10b7c969cca662f",
            "bd1e7be504b44aa1a06408e00e0b6d3b",
            "26b5583110e04c39a08fb5abf9ba1fdc",
            "fa5cc01f16de42e5a7c6bbf667b5a4b9",
            "df938b337bd64ee49e9a826ea8919958",
            "558e37e9c8794abe88cf21f5f72c3fa5",
            "49225a324a58471a8887792e4f5ee4eb",
            "c37b6eea29fb468daefd4c834ae3127a",
            "1d500e4d516f4dbea0338cc648f92c74",
            "0fb7ee5172904744aa7d81e435763b9d",
            "1fb03067ed7f4886ac078a44fd4b4bf2",
            "2b1aba06426349b3ae03faa8f5be60ed",
            "d60ec66223e54880b6f75edfff5dd77f",
            "e9463558caa74e1bad4cb021b3c03a52",
            "42e1e184ea7540a4a6c62a0d805192bd",
            "f17c05f824324187bb9ac110f40edfff",
            "76d8bf87ba0b4e45bcd20467b18afaf0",
            "fe08440f3bd54f53bc7439532209078e",
            "131d9e5f801c427e999783e0a88c7043",
            "2b5b3d807d514cd2b9727c7ce626f69b",
            "1f71962c63e345749141013882929335",
            "887a8e05fa454d17b8f2727375a5a638",
            "f6a932b0cf404023be9a4a858e86e16b",
            "874d93a74cc649f1864e1c96a78bdc5e",
            "a6eaf2d9d7db445b94ad1e12c5d1f436",
            "0740f24a5d5244dc9c58344a3880bda5"
          ]
        },
        "id": "z4CeTOsoeKg7",
        "outputId": "a8229168-8ced-4dab-8640-4da624776697"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f08f3c17b304109a85f2b10bcd0969f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating healthy:   0%|          | 0/2139 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df938b337bd64ee49e9a826ea8919958",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating sick:   0%|          | 0/2121 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f17c05f824324187bb9ac110f40edfff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating noise: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Error processing /content/smartearsaudio/sm/Healthy/rpi0008-20240428003530.wav: No module named 'resampy'\n",
            "\n",
            "This error is lazily reported, having originally occured in\n",
            "  File /usr/local/lib/python3.11/dist-packages/librosa/core/audio.py, line 33, in <module>\n",
            "\n",
            "----> resampy = lazy.load(\"resampy\")\n",
            "ERROR:__main__:Error processing /content/smartearsaudio/sm/Sick/rpi0007-20240801222530.wav: No module named 'resampy'\n",
            "\n",
            "This error is lazily reported, having originally occured in\n",
            "  File /usr/local/lib/python3.11/dist-packages/librosa/core/audio.py, line 33, in <module>\n",
            "\n",
            "----> resampy = lazy.load(\"resampy\")\n",
            "ERROR:__main__:Error processing /content/smartearsaudio/sm/Healthy/rpi0007-20240603224800.wav: No module named 'resampy'\n",
            "\n",
            "This error is lazily reported, having originally occured in\n",
            "  File /usr/local/lib/python3.11/dist-packages/librosa/core/audio.py, line 33, in <module>\n",
            "\n",
            "----> resampy = lazy.load(\"resampy\")\n",
            "ERROR:__main__:Error processing /content/smartearsaudio/sm/Sick/rpi0102-20240108140800-7.wav: No module named 'resampy'\n",
            "\n",
            "This error is lazily reported, having originally occured in\n",
            "  File /usr/local/lib/python3.11/dist-packages/librosa/core/audio.py, line 33, in <module>\n",
            "\n",
            "----> resampy = lazy.load(\"resampy\")\n",
            "ERROR:__main__:Error processing /content/smartearsaudio/sm/Healthy/rpi0014-20240224223700.wav: No module named 'resampy'\n",
            "\n",
            "This error is lazily reported, having originally occured in\n",
            "  File /usr/local/lib/python3.11/dist-packages/librosa/core/audio.py, line 33, in <module>\n",
            "\n",
            "----> resampy = lazy.load(\"resampy\")\n",
            "ERROR:__main__:Error processing /content/smartearsaudio/sm/Sick/rpi0013-20240428232400.wav: No module named 'resampy'\n",
            "\n",
            "This error is lazily reported, having originally occured in\n",
            "  File /usr/local/lib/python3.11/dist-packages/librosa/core/audio.py, line 33, in <module>\n",
            "\n",
            "----> resampy = lazy.load(\"resampy\")\n",
            "ERROR:__main__:Error processing /content/smartearsaudio/sm/Healthy/rpi0105-20240404195400-10.wav: No module named 'resampy'\n",
            "\n",
            "This error is lazily reported, having originally occured in\n",
            "  File /usr/local/lib/python3.11/dist-packages/librosa/core/audio.py, line 33, in <module>\n",
            "\n",
            "----> resampy = lazy.load(\"resampy\")\n",
            "ERROR:__main__:Error processing /content/smartearsaudio/sm/Sick/rpi0101-20240311123300-1.wav: No module named 'resampy'\n",
            "\n",
            "This error is lazily reported, having originally occured in\n",
            "  File /usr/local/lib/python3.11/dist-packages/librosa/core/audio.py, line 33, in <module>\n",
            "\n",
            "----> resampy = lazy.load(\"resampy\")\n",
            "ERROR:__main__:Error processing /content/smartearsaudio/sm/Sick/rpi0011-20240311215530.wav: No module named 'resampy'\n",
            "\n",
            "This error is lazily reported, having originally occured in\n",
            "  File /usr/local/lib/python3.11/dist-packages/librosa/core/audio.py, line 33, in <module>\n",
            "\n",
            "----> resampy = lazy.load(\"resampy\")\n",
            "ERROR:__main__:Error processing /content/smartearsaudio/sm/Healthy/rpi0105-20240602211400-4.wav: No module named 'resampy'\n",
            "\n",
            "This error is lazily reported, having originally occured in\n",
            "  File /usr/local/lib/python3.11/dist-packages/librosa/core/audio.py, line 33, in <module>\n",
            "\n",
            "----> resampy = lazy.load(\"resampy\")\n",
            "ERROR:__main__:Error processing /content/smartearsaudio/sm/Healthy/rpi0104-20240109014700-9.wav: No module named 'resampy'\n",
            "\n",
            "This error is lazily reported, having originally occured in\n",
            "  File /usr/local/lib/python3.11/dist-packages/librosa/core/audio.py, line 33, in <module>\n",
            "\n",
            "----> resampy = lazy.load(\"resampy\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Training failed: in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/tmp/ipython-input-683553730.py\", line 579, in focal_loss_fixed  *\n",
            "        ce = -y_true * tf.math.log(y_pred)\n",
            "\n",
            "    TypeError: Input 'y' of 'Mul' Op has type float16 that does not match type int32 of argument 'x'.\n",
            "\n",
            "ERROR:__main__:Training pipeline failed: in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/tmp/ipython-input-683553730.py\", line 579, in focal_loss_fixed  *\n",
            "        ce = -y_true * tf.math.log(y_pred)\n",
            "\n",
            "    TypeError: Input 'y' of 'Mul' Op has type float16 that does not match type int32 of argument 'x'.\n",
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipython-input-683553730.py\", line 579, in focal_loss_fixed  *\n        ce = -y_true * tf.math.log(y_pred)\n\n    TypeError: Input 'y' of 'Mul' Op has type float16 that does not match type int32 of argument 'x'.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-683553730.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-683553730.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         model, history = trainer.train(\n\u001b[0m\u001b[1;32m    792\u001b[0m             \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         )\n",
            "\u001b[0;32m/tmp/ipython-input-683553730.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, class_weights, num_classes)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m             history = model.fit(\n\u001b[0m\u001b[1;32m    647\u001b[0m                 \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf__train_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_fileasq7ikvi.py\u001b[0m in \u001b[0;36mtf__focal_loss_fixed\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'y_true'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mfl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipython-input-683553730.py\", line 579, in focal_loss_fixed  *\n        ce = -y_true * tf.math.log(y_pred)\n\n    TypeError: Input 'y' of 'Mul' Op has type float16 that does not match type int32 of argument 'x'.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Robust Wav2Vec2 Audio Classification Trainer - Fixed Version\n",
        "Addresses bias, NaN losses, and compatibility issues\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import warnings\n",
        "import json\n",
        "import datetime\n",
        "from typing import List, Tuple, Dict, Optional, Union\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from collections import Counter\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class RobustConfig:\n",
        "    \"\"\"Robust configuration with bias prevention measures\"\"\"\n",
        "\n",
        "    # Model configuration\n",
        "    MODEL_ID: str = \"facebook/wav2vec2-base-960h\"\n",
        "    SAMPLE_RATE: int = 16000\n",
        "    DURATION: float = 3.0  # Reduced for better memory management\n",
        "    MAX_LENGTH: int = 48000  # 3 seconds at 16kHz\n",
        "\n",
        "    # Training configuration - Conservative settings to prevent NaN\n",
        "    BATCH_SIZE: int = 8  # Smaller batch size for stability\n",
        "    EPOCHS: int = 20\n",
        "    LEARNING_RATE: float = 1e-5  # Much lower learning rate\n",
        "    WARMUP_RATIO: float = 0.1\n",
        "    WEIGHT_DECAY: float = 0.01\n",
        "    GRADIENT_CLIP_NORM: float = 1.0  # Gradient clipping to prevent explosion\n",
        "\n",
        "    # Data configuration\n",
        "    TRAIN_SPLIT: float = 0.7\n",
        "    VAL_SPLIT: float = 0.15\n",
        "    TEST_SPLIT: float = 0.15\n",
        "\n",
        "    # Bias prevention configuration\n",
        "    MAX_CLASS_IMBALANCE_RATIO: float = 3.0  # Max ratio between largest and smallest class\n",
        "    MIN_SAMPLES_PER_CLASS: int = 50  # Minimum samples required per class\n",
        "    SYNTHETIC_NOISE_RATIO: float = 0.3  # Max 30% synthetic noise in dataset\n",
        "\n",
        "    # Audio validation configuration\n",
        "    MIN_AUDIO_LENGTH: float = 1.0  # Minimum 1 second\n",
        "    MAX_AUDIO_LENGTH: float = 10.0  # Maximum 10 seconds\n",
        "    MIN_SNR_DB: float = 5.0  # Minimum signal-to-noise ratio\n",
        "\n",
        "    # Augmentation configuration - Reduced to prevent overfitting\n",
        "    USE_AUGMENTATION: bool = True\n",
        "    AUGMENTATION_PROB: float = 0.3  # Reduced probability\n",
        "\n",
        "    # Directory configuration\n",
        "    BASE_DIR: str = './data'\n",
        "    HEALTHY_DIR: str = './data/healthy'\n",
        "    SICK_DIR: str = './data/sick'\n",
        "    NOISE_DIR: str = './data/noise'\n",
        "    OUTPUT_DIR: str = './robust_model_output'\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration\"\"\"\n",
        "        assert self.TRAIN_SPLIT + self.VAL_SPLIT + self.TEST_SPLIT == 1.0\n",
        "        assert self.MAX_CLASS_IMBALANCE_RATIO >= 1.0\n",
        "        assert self.MIN_SAMPLES_PER_CLASS > 0\n",
        "        assert 0 < self.SYNTHETIC_NOISE_RATIO < 1.0\n",
        "\n",
        "\n",
        "class AudioValidator:\n",
        "    \"\"\"Robust audio validation to prevent NaN issues\"\"\"\n",
        "\n",
        "    def __init__(self, config: RobustConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def validate_audio_file(self, file_path: str) -> Tuple[bool, str]:\n",
        "        \"\"\"Comprehensive audio file validation\"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(file_path):\n",
        "                return False, \"File does not exist\"\n",
        "\n",
        "            if os.path.getsize(file_path) < 1000:  # Less than 1KB\n",
        "                return False, \"File too small\"\n",
        "\n",
        "            # Load audio and check basic properties\n",
        "            try:\n",
        "                waveform, sr = librosa.load(file_path, sr=None, mono=True)\n",
        "            except Exception as e:\n",
        "                return False, f\"Cannot load audio: {e}\"\n",
        "\n",
        "            # Check duration\n",
        "            duration = len(waveform) / sr\n",
        "            if duration < self.config.MIN_AUDIO_LENGTH:\n",
        "                return False, f\"Audio too short: {duration:.2f}s\"\n",
        "            if duration > self.config.MAX_AUDIO_LENGTH:\n",
        "                return False, f\"Audio too long: {duration:.2f}s\"\n",
        "\n",
        "            # Check for silence or constant values\n",
        "            if np.std(waveform) < 1e-6:\n",
        "                return False, \"Audio is silent or constant\"\n",
        "\n",
        "            # Check for clipping\n",
        "            clipping_ratio = np.mean(np.abs(waveform) > 0.95)\n",
        "            if clipping_ratio > 0.1:\n",
        "                return False, f\"Audio heavily clipped: {clipping_ratio:.2%}\"\n",
        "\n",
        "            # Check signal-to-noise ratio (rough estimate)\n",
        "            signal_power = np.mean(waveform ** 2)\n",
        "            if signal_power < 1e-8:\n",
        "                return False, \"Signal power too low\"\n",
        "\n",
        "            # Check for NaN or infinite values\n",
        "            if not np.isfinite(waveform).all():\n",
        "                return False, \"Audio contains NaN or infinite values\"\n",
        "\n",
        "            return True, \"Valid\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, f\"Validation error: {e}\"\n",
        "\n",
        "\n",
        "class RobustAudioProcessor:\n",
        "    \"\"\"Robust audio processing with NaN prevention\"\"\"\n",
        "\n",
        "    def __init__(self, config: RobustConfig):\n",
        "        self.config = config\n",
        "        self.validator = AudioValidator(config)\n",
        "\n",
        "        # Initialize feature extractor\n",
        "        try:\n",
        "            from transformers import AutoFeatureExtractor\n",
        "            self.feature_extractor = AutoFeatureExtractor.from_pretrained(config.MODEL_ID)\n",
        "            logger.info(\"‚úÖ Feature extractor loaded successfully\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not load feature extractor: {e}\")\n",
        "            self.feature_extractor = None\n",
        "\n",
        "    def load_and_preprocess_audio(self, file_path: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"Load and preprocess audio with robust error handling\"\"\"\n",
        "        try:\n",
        "            # Validate file first\n",
        "            is_valid, reason = self.validator.validate_audio_file(file_path)\n",
        "            if not is_valid:\n",
        "                logger.warning(f\"Invalid audio {file_path}: {reason}\")\n",
        "                return None\n",
        "\n",
        "            # Load audio\n",
        "            waveform, sr = librosa.load(\n",
        "                file_path,\n",
        "                sr=self.config.SAMPLE_RATE,\n",
        "                mono=True,\n",
        "                res_type='kaiser_fast'\n",
        "            )\n",
        "\n",
        "            # Handle length - intelligent cropping/padding\n",
        "            target_length = self.config.MAX_LENGTH\n",
        "\n",
        "            if len(waveform) > target_length:\n",
        "                # For longer audio, take the most energetic segment\n",
        "                hop_length = len(waveform) - target_length\n",
        "                if hop_length > 0:\n",
        "                    # Find the segment with highest energy\n",
        "                    energies = []\n",
        "                    for i in range(0, hop_length + 1, hop_length // 10 + 1):\n",
        "                        segment = waveform[i:i + target_length]\n",
        "                        energy = np.sum(segment ** 2)\n",
        "                        energies.append((energy, i))\n",
        "\n",
        "                    best_start = max(energies)[1]\n",
        "                    waveform = waveform[best_start:best_start + target_length]\n",
        "                else:\n",
        "                    waveform = waveform[:target_length]\n",
        "\n",
        "            elif len(waveform) < target_length:\n",
        "                # Pad with reflection to avoid discontinuities\n",
        "                pad_length = target_length - len(waveform)\n",
        "                if len(waveform) > pad_length:\n",
        "                    # Reflect the signal\n",
        "                    waveform = np.pad(waveform, (0, pad_length), 'reflect')\n",
        "                else:\n",
        "                    # Zero pad if signal is too short for reflection\n",
        "                    waveform = np.pad(waveform, (0, pad_length), 'constant')\n",
        "\n",
        "            # Robust normalization\n",
        "            waveform = self._robust_normalize(waveform)\n",
        "\n",
        "            # Final validation\n",
        "            if not np.isfinite(waveform).all():\n",
        "                logger.warning(f\"NaN/Inf detected in processed audio: {file_path}\")\n",
        "                return None\n",
        "\n",
        "            return waveform.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _robust_normalize(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Robust normalization that prevents NaN\"\"\"\n",
        "        try:\n",
        "            # Remove DC offset\n",
        "            waveform = waveform - np.mean(waveform)\n",
        "\n",
        "            # Robust scaling using percentiles instead of max\n",
        "            p99 = np.percentile(np.abs(waveform), 99)\n",
        "            if p99 > 1e-8:\n",
        "                waveform = waveform / p99 * 0.8  # Scale to 80% to avoid clipping\n",
        "\n",
        "            # Clip to prevent extreme values\n",
        "            waveform = np.clip(waveform, -1.0, 1.0)\n",
        "\n",
        "            return waveform\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Normalization error: {e}\")\n",
        "            return np.zeros_like(waveform)\n",
        "\n",
        "    def apply_augmentation(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Conservative augmentation to prevent overfitting\"\"\"\n",
        "        if not self.config.USE_AUGMENTATION:\n",
        "            return waveform\n",
        "\n",
        "        try:\n",
        "            augmented = waveform.copy()\n",
        "\n",
        "            # Time shifting (small)\n",
        "            if np.random.random() < self.config.AUGMENTATION_PROB:\n",
        "                shift = int(np.random.uniform(-0.05, 0.05) * len(augmented))\n",
        "                augmented = np.roll(augmented, shift)\n",
        "\n",
        "            # Volume scaling (conservative)\n",
        "            if np.random.random() < self.config.AUGMENTATION_PROB:\n",
        "                scale = np.random.uniform(0.8, 1.2)\n",
        "                augmented = augmented * scale\n",
        "\n",
        "            # Add small amount of noise\n",
        "            if np.random.random() < self.config.AUGMENTATION_PROB * 0.5:\n",
        "                noise_level = np.random.uniform(0.001, 0.005)\n",
        "                noise = np.random.normal(0, noise_level, len(augmented))\n",
        "                augmented = augmented + noise\n",
        "\n",
        "            # Ensure no clipping or NaN\n",
        "            augmented = np.clip(augmented, -1.0, 1.0)\n",
        "\n",
        "            if not np.isfinite(augmented).all():\n",
        "                return waveform  # Return original if augmentation failed\n",
        "\n",
        "            return augmented\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Augmentation failed: {e}\")\n",
        "            return waveform\n",
        "\n",
        "    def extract_features(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Extract features using transformers or fallback to manual extraction\"\"\"\n",
        "        try:\n",
        "            if self.feature_extractor is not None:\n",
        "                # Use transformers feature extractor\n",
        "                inputs = self.feature_extractor(\n",
        "                    waveform,\n",
        "                    sampling_rate=self.config.SAMPLE_RATE,\n",
        "                    return_tensors=\"np\",\n",
        "                    padding=True,\n",
        "                    max_length=self.config.MAX_LENGTH,\n",
        "                    truncation=True\n",
        "                )\n",
        "                return inputs['input_values'][0]\n",
        "            else:\n",
        "                # Fallback to manual feature extraction\n",
        "                return self._manual_feature_extraction(waveform)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Feature extraction failed, using fallback: {e}\")\n",
        "            return self._manual_feature_extraction(waveform)\n",
        "\n",
        "    def _manual_feature_extraction(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Manual feature extraction as fallback\"\"\"\n",
        "        try:\n",
        "            # Simple spectral features\n",
        "            # Compute mel spectrogram\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=waveform,\n",
        "                sr=self.config.SAMPLE_RATE,\n",
        "                n_mels=80,\n",
        "                hop_length=512,\n",
        "                n_fft=1024\n",
        "            )\n",
        "\n",
        "            # Convert to log scale\n",
        "            log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "            # Flatten and pad/truncate to fixed size\n",
        "            features = log_mel.flatten()\n",
        "            target_size = 1000  # Fixed feature size\n",
        "\n",
        "            if len(features) > target_size:\n",
        "                features = features[:target_size]\n",
        "            else:\n",
        "                features = np.pad(features, (0, target_size - len(features)), 'constant')\n",
        "\n",
        "            return features.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Manual feature extraction failed: {e}\")\n",
        "            # Return zeros as last resort\n",
        "            return np.zeros(1000, dtype=np.float32)\n",
        "\n",
        "\n",
        "class BiasPreventionDatasetBuilder:\n",
        "    \"\"\"Dataset builder with bias prevention measures\"\"\"\n",
        "\n",
        "    def __init__(self, config: RobustConfig):\n",
        "        self.config = config\n",
        "        self.processor = RobustAudioProcessor(config)\n",
        "\n",
        "    def collect_and_balance_data(self) -> Tuple[List[str], List[int], List[str]]:\n",
        "        \"\"\"Collect data with bias prevention\"\"\"\n",
        "        logger.info(\"Collecting and balancing dataset...\")\n",
        "\n",
        "        # Collect files by category\n",
        "        healthy_files = self._collect_files(self.config.HEALTHY_DIR, \"healthy\")\n",
        "        sick_files = self._collect_files(self.config.SICK_DIR, \"sick\")\n",
        "        noise_files = self._collect_files(self.config.NOISE_DIR, \"noise\")\n",
        "\n",
        "        logger.info(f\"Raw file counts - Healthy: {len(healthy_files)}, Sick: {len(sick_files)}, Noise: {len(noise_files)}\")\n",
        "\n",
        "        # Validate files\n",
        "        healthy_files = self._validate_files(healthy_files, \"healthy\")\n",
        "        sick_files = self._validate_files(sick_files, \"sick\")\n",
        "        noise_files = self._validate_files(noise_files, \"noise\")\n",
        "\n",
        "        logger.info(f\"Valid file counts - Healthy: {len(healthy_files)}, Sick: {len(sick_files)}, Noise: {len(noise_files)}\")\n",
        "\n",
        "        # Check minimum requirements\n",
        "        if len(healthy_files) < self.config.MIN_SAMPLES_PER_CLASS:\n",
        "            raise ValueError(f\"Insufficient healthy samples: {len(healthy_files)} < {self.config.MIN_SAMPLES_PER_CLASS}\")\n",
        "        if len(sick_files) < self.config.MIN_SAMPLES_PER_CLASS:\n",
        "            raise ValueError(f\"Insufficient sick samples: {len(sick_files)} < {self.config.MIN_SAMPLES_PER_CLASS}\")\n",
        "\n",
        "        # Balance dataset to prevent bias\n",
        "        healthy_files, sick_files, noise_files = self._balance_classes(\n",
        "            healthy_files, sick_files, noise_files\n",
        "        )\n",
        "\n",
        "        # Determine class configuration\n",
        "        if len(noise_files) > 0:\n",
        "            all_files = healthy_files + sick_files + noise_files\n",
        "            all_labels = ([0] * len(healthy_files) +\n",
        "                         [1] * len(sick_files) +\n",
        "                         [2] * len(noise_files))\n",
        "            class_names = ['healthy', 'sick', 'noise']\n",
        "        else:\n",
        "            all_files = healthy_files + sick_files\n",
        "            all_labels = [0] * len(healthy_files) + [1] * len(sick_files)\n",
        "            class_names = ['healthy', 'sick']\n",
        "\n",
        "        logger.info(f\"Final balanced dataset - Classes: {class_names}\")\n",
        "        for i, name in enumerate(class_names):\n",
        "            count = sum(1 for label in all_labels if label == i)\n",
        "            logger.info(f\"  {name}: {count} samples\")\n",
        "\n",
        "        return all_files, all_labels, class_names\n",
        "\n",
        "    def _collect_files(self, directory: str, category: str) -> List[str]:\n",
        "        \"\"\"Collect audio files from directory\"\"\"\n",
        "        files = []\n",
        "        if os.path.exists(directory):\n",
        "            for ext in ['.wav', '.mp3', '.flac', '.m4a']:\n",
        "                files.extend(Path(directory).rglob(f'*{ext}'))\n",
        "\n",
        "        return [str(f) for f in files]\n",
        "\n",
        "    def _validate_files(self, files: List[str], category: str) -> List[str]:\n",
        "        \"\"\"Validate audio files\"\"\"\n",
        "        valid_files = []\n",
        "\n",
        "        logger.info(f\"Validating {len(files)} {category} files...\")\n",
        "\n",
        "        for file_path in tqdm(files, desc=f\"Validating {category}\"):\n",
        "            is_valid, reason = self.processor.validator.validate_audio_file(file_path)\n",
        "            if is_valid:\n",
        "                valid_files.append(file_path)\n",
        "            else:\n",
        "                logger.debug(f\"Rejected {file_path}: {reason}\")\n",
        "\n",
        "        rejection_rate = (len(files) - len(valid_files)) / len(files) if files else 0\n",
        "        logger.info(f\"{category} validation: {len(valid_files)}/{len(files)} valid ({rejection_rate:.1%} rejected)\")\n",
        "\n",
        "        return valid_files\n",
        "\n",
        "    def _balance_classes(self, healthy_files: List[str], sick_files: List[str],\n",
        "                        noise_files: List[str]) -> Tuple[List[str], List[str], List[str]]:\n",
        "        \"\"\"Balance classes to prevent bias\"\"\"\n",
        "\n",
        "        # Calculate target sizes\n",
        "        counts = [len(healthy_files), len(sick_files)]\n",
        "        if noise_files:\n",
        "            counts.append(len(noise_files))\n",
        "\n",
        "        min_count = min(counts)\n",
        "        max_count = max(counts)\n",
        "\n",
        "        # Check if balancing is needed\n",
        "        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
        "\n",
        "        if imbalance_ratio > self.config.MAX_CLASS_IMBALANCE_RATIO:\n",
        "            logger.info(f\"Balancing classes (ratio: {imbalance_ratio:.2f} > {self.config.MAX_CLASS_IMBALANCE_RATIO})\")\n",
        "\n",
        "            # Calculate target size (use median to avoid extreme reduction)\n",
        "            target_size = int(np.median(counts))\n",
        "\n",
        "            # Limit noise files to prevent overwhelming real data\n",
        "            if noise_files:\n",
        "                max_noise = int(target_size * self.config.SYNTHETIC_NOISE_RATIO / (1 - self.config.SYNTHETIC_NOISE_RATIO))\n",
        "                if len(noise_files) > max_noise:\n",
        "                    noise_files = np.random.choice(noise_files, max_noise, replace=False).tolist()\n",
        "                    logger.info(f\"Limited noise files to {max_noise}\")\n",
        "\n",
        "            # Balance healthy and sick files\n",
        "            if len(healthy_files) > target_size:\n",
        "                healthy_files = np.random.choice(healthy_files, target_size, replace=False).tolist()\n",
        "            if len(sick_files) > target_size:\n",
        "                sick_files = np.random.choice(sick_files, target_size, replace=False).tolist()\n",
        "\n",
        "        logger.info(f\"Balanced dataset - Healthy: {len(healthy_files)}, Sick: {len(sick_files)}, Noise: {len(noise_files)}\")\n",
        "\n",
        "        return healthy_files, sick_files, noise_files\n",
        "\n",
        "\n",
        "class SimpleRobustTrainer:\n",
        "    \"\"\"Simplified robust trainer with better compatibility\"\"\"\n",
        "\n",
        "    def __init__(self, config: RobustConfig):\n",
        "        self.config = config\n",
        "        self.processor = RobustAudioProcessor(config)\n",
        "\n",
        "        # Setup directories\n",
        "        os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "        # Configure TensorFlow for stability\n",
        "        self._configure_tensorflow()\n",
        "\n",
        "    def _configure_tensorflow(self):\n",
        "        \"\"\"Configure TensorFlow for stable training\"\"\"\n",
        "        # Configure GPU memory growth\n",
        "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                for gpu in gpus:\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "                logger.info(\"‚úÖ GPU memory growth enabled\")\n",
        "            except RuntimeError as e:\n",
        "                logger.warning(f\"GPU configuration warning: {e}\")\n",
        "\n",
        "    def create_dataset(self, file_paths: List[str], labels: List[int],\n",
        "                      is_training: bool = True) -> tf.data.Dataset:\n",
        "        \"\"\"Create robust TensorFlow dataset\"\"\"\n",
        "\n",
        "        def load_audio_tf(file_path, label):\n",
        "            def load_and_process(path):\n",
        "                path_str = path.numpy().decode('utf-8')\n",
        "                waveform = self.processor.load_and_preprocess_audio(path_str)\n",
        "\n",
        "                if waveform is None:\n",
        "                    # Return zeros if loading failed\n",
        "                    waveform = np.zeros(self.config.MAX_LENGTH, dtype=np.float32)\n",
        "                else:\n",
        "                    # Apply augmentation for training\n",
        "                    if is_training:\n",
        "                        waveform = self.processor.apply_augmentation(waveform)\n",
        "\n",
        "                # Extract features\n",
        "                features = self.processor.extract_features(waveform)\n",
        "                return features\n",
        "\n",
        "            audio_data = tf.py_function(\n",
        "                load_and_process,\n",
        "                inp=[file_path],\n",
        "                Tout=tf.float32\n",
        "            )\n",
        "\n",
        "            # Set shape based on feature extraction method\n",
        "            if self.processor.feature_extractor is not None:\n",
        "                audio_data.set_shape([self.config.MAX_LENGTH])\n",
        "            else:\n",
        "                audio_data.set_shape([1000])  # Manual feature size\n",
        "\n",
        "            return audio_data, label\n",
        "\n",
        "        # Create dataset\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "\n",
        "        if is_training:\n",
        "            dataset = dataset.shuffle(buffer_size=min(1000, len(file_paths)))\n",
        "\n",
        "        dataset = dataset.map(\n",
        "            load_audio_tf,\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "\n",
        "        # Filter out any invalid samples\n",
        "        dataset = dataset.filter(\n",
        "            lambda audio, label: tf.reduce_all(tf.math.is_finite(audio))\n",
        "        )\n",
        "\n",
        "        dataset = dataset.batch(self.config.BATCH_SIZE)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def create_simple_model(self, num_classes: int, input_shape: Tuple[int]):\n",
        "        \"\"\"Create a simple, robust model\"\"\"\n",
        "        try:\n",
        "            model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Input(shape=input_shape),\n",
        "\n",
        "                # If using raw audio, add some conv layers\n",
        "                tf.keras.layers.Reshape((-1, 1)) if len(input_shape) == 1 and input_shape[0] > 1000 else tf.keras.layers.Lambda(lambda x: x),\n",
        "\n",
        "                # Dense layers for classification\n",
        "                tf.keras.layers.Flatten(),\n",
        "                tf.keras.layers.Dense(512, activation='relu'),\n",
        "                tf.keras.layers.Dropout(0.3),\n",
        "                tf.keras.layers.Dense(256, activation='relu'),\n",
        "                tf.keras.layers.Dropout(0.2),\n",
        "                tf.keras.layers.Dense(128, activation='relu'),\n",
        "                tf.keras.layers.Dropout(0.1),\n",
        "                tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            logger.info(\"‚úÖ Simple model created successfully\")\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Model creation failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def compile_model(self, model, class_weights: Dict[int, float]):\n",
        "        \"\"\"Compile model with robust settings\"\"\"\n",
        "\n",
        "        # Use a stable optimizer\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=self.config.LEARNING_RATE,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.999,\n",
        "            epsilon=1e-8,\n",
        "            clipnorm=self.config.GRADIENT_CLIP_NORM\n",
        "        )\n",
        "\n",
        "        # Use focal loss to handle class imbalance\n",
        "        def focal_loss(alpha=0.25, gamma=2.0):\n",
        "            def focal_loss_fixed(y_true, y_pred):\n",
        "                epsilon = tf.keras.backend.epsilon()\n",
        "                y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
        "\n",
        "                # Convert to one-hot if needed\n",
        "                if len(y_true.shape) == 1:\n",
        "                    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[1])\n",
        "\n",
        "                # Calculate focal loss\n",
        "                ce = -y_true * tf.math.log(y_pred)\n",
        "                weight = alpha * y_true * tf.pow((1 - y_pred), gamma)\n",
        "                fl = weight * ce\n",
        "\n",
        "                return tf.reduce_mean(tf.reduce_sum(fl, axis=1))\n",
        "\n",
        "            return focal_loss_fixed\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=focal_loss(),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def create_callbacks(self):\n",
        "        \"\"\"Create training callbacks for stability\"\"\"\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=8,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=4,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                os.path.join(self.config.OUTPUT_DIR, 'best_model.h5'),\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            # NaN termination callback\n",
        "            tf.keras.callbacks.TerminateOnNaN(),\n",
        "            # Custom callback for monitoring\n",
        "            RobustTrainingCallback()\n",
        "        ]\n",
        "\n",
        "        return callbacks\n",
        "\n",
        "    def train(self, train_dataset, val_dataset, class_weights, num_classes):\n",
        "        \"\"\"Train model with robust settings\"\"\"\n",
        "\n",
        "        logger.info(\"Creating and compiling model...\")\n",
        "\n",
        "        # Determine input shape from dataset\n",
        "        sample_batch = next(iter(train_dataset))\n",
        "        input_shape = sample_batch[0].shape[1:]\n",
        "        logger.info(f\"Input shape: {input_shape}\")\n",
        "\n",
        "        model = self.create_simple_model(num_classes, input_shape)\n",
        "        model = self.compile_model(model, class_weights)\n",
        "\n",
        "        logger.info(\"Model architecture:\")\n",
        "        model.summary(print_fn=logger.info)\n",
        "\n",
        "        callbacks = self.create_callbacks()\n",
        "\n",
        "        logger.info(f\"Starting training for {self.config.EPOCHS} epochs...\")\n",
        "\n",
        "        try:\n",
        "            history = model.fit(\n",
        "                train_dataset,\n",
        "                epochs=self.config.EPOCHS,\n",
        "                validation_data=val_dataset,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1,\n",
        "                class_weight=class_weights\n",
        "            )\n",
        "\n",
        "            logger.info(\"Training completed successfully!\")\n",
        "            return model, history\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Training failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def evaluate_model(self, model, test_dataset, class_names):\n",
        "        \"\"\"Evaluate model with comprehensive metrics\"\"\"\n",
        "        logger.info(\"Evaluating model...\")\n",
        "\n",
        "        try:\n",
        "            # Get predictions\n",
        "            predictions = model.predict(test_dataset, verbose=1)\n",
        "\n",
        "            # Extract true labels\n",
        "            y_true = []\n",
        "            for _, labels in test_dataset:\n",
        "                y_true.extend(labels.numpy())\n",
        "            y_true = np.array(y_true)\n",
        "\n",
        "            y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "            logger.info(f\"Balanced Accuracy: {accuracy:.4f}\")\n",
        "            logger.info(\"\\nClassification Report:\")\n",
        "            print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "            # Confusion matrix\n",
        "            cm = confusion_matrix(y_true, y_pred)\n",
        "            self._plot_confusion_matrix(cm, class_names)\n",
        "\n",
        "            return {\n",
        "                'accuracy': accuracy,\n",
        "                'y_true': y_true,\n",
        "                'y_pred': y_pred,\n",
        "                'predictions': predictions,\n",
        "                'confusion_matrix': cm\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Evaluation failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _plot_confusion_matrix(self, cm, class_names):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.config.OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class RobustTrainingCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Custom callback for monitoring training stability\"\"\"\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            logs = {}\n",
        "\n",
        "        # Check for NaN or extreme values\n",
        "        for key, value in logs.items():\n",
        "            if not np.isfinite(value):\n",
        "                logger.error(f\"NaN/Inf detected in {key}: {value}\")\n",
        "                self.model.stop_training = True\n",
        "                return\n",
        "\n",
        "        # Log progress\n",
        "        logger.info(f\"Epoch {epoch + 1}: \"\n",
        "                   f\"loss={logs.get('loss', 0):.4f}, \"\n",
        "                   f\"acc={logs.get('accuracy', 0):.4f}, \"\n",
        "                   f\"val_loss={logs.get('val_loss', 0):.4f}, \"\n",
        "                   f\"val_acc={logs.get('val_accuracy', 0):.4f}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "    logger.info(\"Starting robust Wav2Vec2 training pipeline...\")\n",
        "\n",
        "    # Configuration\n",
        "    config = RobustConfig()\n",
        "\n",
        "    config.BASE_DIR = '/content/smartearsaudio/sm'\n",
        "    config.HEALTHY_DIR = '/content/smartearsaudio/sm/Healthy'\n",
        "    config.SICK_DIR = '/content/smartearsaudio/sm/Sick'\n",
        "\n",
        "    try:\n",
        "        # Build dataset\n",
        "        dataset_builder = BiasPreventionDatasetBuilder(config)\n",
        "        all_files, all_labels, class_names = dataset_builder.collect_and_balance_data()\n",
        "\n",
        "        if len(all_files) == 0:\n",
        "            raise ValueError(\"No valid audio files found!\")\n",
        "\n",
        "        # Split data\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            all_files, all_labels,\n",
        "            test_size=config.TEST_SPLIT,\n",
        "            random_state=42,\n",
        "            stratify=all_labels\n",
        "        )\n",
        "\n",
        "        val_size = config.VAL_SPLIT / (1 - config.TEST_SPLIT)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp,\n",
        "            test_size=val_size,\n",
        "            random_state=42,\n",
        "            stratify=y_temp\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Data split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
        "\n",
        "        # Calculate class weights\n",
        "        class_weights = compute_class_weight(\n",
        "            'balanced',\n",
        "            classes=np.unique(y_train),\n",
        "            y=y_train\n",
        "        )\n",
        "        class_weight_dict = dict(enumerate(class_weights))\n",
        "        logger.info(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "        # Create trainer\n",
        "        trainer = SimpleRobustTrainer(config)\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = trainer.create_dataset(X_train, y_train, is_training=True)\n",
        "        val_dataset = trainer.create_dataset(X_val, y_val, is_training=False)\n",
        "        test_dataset = trainer.create_dataset(X_test, y_test, is_training=False)\n",
        "\n",
        "        # Train model\n",
        "        model, history = trainer.train(\n",
        "            train_dataset, val_dataset, class_weight_dict, len(class_names)\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        results = trainer.evaluate_model(model, test_dataset, class_names)\n",
        "\n",
        "        # Save model and results\n",
        "        model.save(os.path.join(config.OUTPUT_DIR, 'final_model.h5'))\n",
        "\n",
        "        with open(os.path.join(config.OUTPUT_DIR, 'training_results.json'), 'w') as f:\n",
        "            json.dump({\n",
        "                'class_names': class_names,\n",
        "                'accuracy': float(results.get('accuracy', 0)),\n",
        "                'class_weights': {str(k): float(v) for k, v in class_weight_dict.items()},\n",
        "                'config': config.__dict__\n",
        "            }, f, indent=2)\n",
        "\n",
        "        logger.info(f\"Training completed! Results saved to {config.OUTPUT_DIR}\")\n",
        "\n",
        "        return model, results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Training pipeline failed: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, results = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El4wJ9Edi6px",
        "outputId": "1094dc2f-46e2-4e3c-91c5-97ff9fcd2750"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipython-input-1841238859.py:55: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ COLAB GPU DIAGNOSTIC & FIX\n",
            "======================================================================\n",
            "üîç GPU DIAGNOSTIC REPORT\n",
            "==================================================\n",
            "‚úÖ NVIDIA driver working\n",
            "Thu Aug  7 12:40:07 2025       \n",
            "‚úÖ CUDA available: Cuda compilation tools, release 12.5, V12.5.82\n",
            "\n",
            "üîß TensorFlow version: 2.19.0\n",
            "üì± Physical GPUs: 1\n",
            "  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "‚úÖ TensorFlow built with CUDA\n",
            "‚úÖ GPU available to TensorFlow\n",
            "‚úÖ GPU computation test passed\n",
            "\n",
            "üîß APPLYING GPU FIXES\n",
            "==================================================\n",
            "‚úÖ Cleared TensorFlow session\n",
            "‚úÖ Enabled GPU memory growth\n",
            "‚ö†Ô∏è Memory limit warning: Virtual devices cannot be modified after being initialized\n",
            "‚úÖ Disabled mixed precision\n",
            "‚úÖ Enabled deterministic operations\n",
            "\n",
            "üì¶ CHECKING PACKAGES\n",
            "==================================================\n",
            "üì¶ Installing resampy...\n",
            "‚úÖ resampy installed\n",
            "‚úÖ scipy installed\n",
            "\n",
            "üß™ FINAL GPU TEST\n",
            "==================================================\n",
            "‚ùå GPU training test failed: Random ops require a seed to be set when determinism is enabled. Please set a seed before running the op, e.g. by calling tf.random.set_seed(1).\n",
            "\n",
            "Try restarting the runtime and running this diagnostic again.\n",
            "\n",
            "üí° TROUBLESHOOTING TIPS:\n",
            "1. Restart runtime: Runtime > Restart runtime\n",
            "2. Check GPU allocation: Runtime > Change runtime type\n",
            "3. Try running this diagnostic again\n",
            "4. If still failing, use the CPU version instead\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "GPU Diagnostic Script for Colab\n",
        "Run this first to check and fix GPU issues\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def check_gpu_setup():\n",
        "    \"\"\"Comprehensive GPU diagnostic\"\"\"\n",
        "    print(\"üîç GPU DIAGNOSTIC REPORT\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check NVIDIA driver\n",
        "    try:\n",
        "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ NVIDIA driver working\")\n",
        "            print(result.stdout.split('\\n')[0])  # First line with driver info\n",
        "        else:\n",
        "            print(\"‚ùå NVIDIA driver not working\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå nvidia-smi not found\")\n",
        "\n",
        "    # Check CUDA\n",
        "    try:\n",
        "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            cuda_version = [line for line in result.stdout.split('\\n') if 'release' in line]\n",
        "            if cuda_version:\n",
        "                print(f\"‚úÖ CUDA available: {cuda_version[0].strip()}\")\n",
        "        else:\n",
        "            print(\"‚ùå CUDA not available\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå nvcc not found\")\n",
        "\n",
        "    # Check TensorFlow GPU\n",
        "    print(f\"\\nüîß TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "    # List physical devices\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    print(f\"üì± Physical GPUs: {len(gpus)}\")\n",
        "    for i, gpu in enumerate(gpus):\n",
        "        print(f\"  GPU {i}: {gpu}\")\n",
        "\n",
        "    # Check if TensorFlow can see GPU\n",
        "    if tf.test.is_built_with_cuda():\n",
        "        print(\"‚úÖ TensorFlow built with CUDA\")\n",
        "    else:\n",
        "        print(\"‚ùå TensorFlow NOT built with CUDA\")\n",
        "\n",
        "    if tf.test.is_gpu_available():\n",
        "        print(\"‚úÖ GPU available to TensorFlow\")\n",
        "    else:\n",
        "        print(\"‚ùå GPU NOT available to TensorFlow\")\n",
        "\n",
        "    # Test GPU computation\n",
        "    try:\n",
        "        with tf.device('/GPU:0'):\n",
        "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
        "            b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
        "            c = tf.matmul(a, b)\n",
        "        print(\"‚úÖ GPU computation test passed\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GPU computation test failed: {e}\")\n",
        "\n",
        "    return len(gpus) > 0\n",
        "\n",
        "def fix_colab_gpu_issues():\n",
        "    \"\"\"Apply common fixes for Colab GPU issues\"\"\"\n",
        "    print(\"\\nüîß APPLYING GPU FIXES\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Clear TensorFlow session\n",
        "    tf.keras.backend.clear_session()\n",
        "    print(\"‚úÖ Cleared TensorFlow session\")\n",
        "\n",
        "    # Reset memory growth\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(\"‚úÖ Enabled GPU memory growth\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"‚ö†Ô∏è Memory growth warning: {e}\")\n",
        "\n",
        "    # Set memory limit\n",
        "    if gpus:\n",
        "        try:\n",
        "            tf.config.experimental.set_virtual_device_configuration(\n",
        "                gpus[0],\n",
        "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8192)]\n",
        "            )\n",
        "            print(\"‚úÖ Set GPU memory limit to 8GB\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Memory limit warning: {e}\")\n",
        "\n",
        "    # Disable mixed precision\n",
        "    tf.keras.mixed_precision.set_global_policy('float32')\n",
        "    print(\"‚úÖ Disabled mixed precision\")\n",
        "\n",
        "    # Enable deterministic operations\n",
        "    try:\n",
        "        tf.config.experimental.enable_op_determinism()\n",
        "        print(\"‚úÖ Enabled deterministic operations\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Determinism warning: {e}\")\n",
        "\n",
        "def install_missing_packages():\n",
        "    \"\"\"Install any missing packages\"\"\"\n",
        "    print(\"\\nüì¶ CHECKING PACKAGES\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    packages = ['resampy', 'scipy']\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"‚úÖ {package} installed\")\n",
        "        except ImportError:\n",
        "            print(f\"üì¶ Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "            print(f\"‚úÖ {package} installed\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main diagnostic\"\"\"\n",
        "    print(\"üöÄ COLAB GPU DIAGNOSTIC & FIX\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Check current GPU setup\n",
        "    has_gpu = check_gpu_setup()\n",
        "\n",
        "    if not has_gpu:\n",
        "        print(\"\\n‚ùå NO GPU DETECTED!\")\n",
        "        print(\"Make sure you're using a GPU runtime:\")\n",
        "        print(\"1. Go to Runtime > Change runtime type\")\n",
        "        print(\"2. Select 'GPU' as Hardware accelerator\")\n",
        "        print(\"3. Click Save and restart the runtime\")\n",
        "        return False\n",
        "\n",
        "    # Apply fixes\n",
        "    fix_colab_gpu_issues()\n",
        "\n",
        "    # Install packages\n",
        "    install_missing_packages()\n",
        "\n",
        "    # Final test\n",
        "    print(\"\\nüß™ FINAL GPU TEST\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Test a simple model\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(10, activation='relu', input_shape=(5,)),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "        # Generate dummy data\n",
        "        import numpy as np\n",
        "        X = np.random.random((100, 5))\n",
        "        y = np.random.randint(2, size=(100, 1))\n",
        "\n",
        "        # Train for 1 epoch\n",
        "        with tf.device('/GPU:0'):\n",
        "            model.fit(X, y, epochs=1, verbose=0)\n",
        "\n",
        "        print(\"‚úÖ GPU training test passed!\")\n",
        "        print(\"\\nüéâ GPU is ready for training!\")\n",
        "        print(\"You can now run: python colab_gpu_trainer.py\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GPU training test failed: {e}\")\n",
        "        print(\"\\nTry restarting the runtime and running this diagnostic again.\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    success = main()\n",
        "    if not success:\n",
        "        print(\"\\nüí° TROUBLESHOOTING TIPS:\")\n",
        "        print(\"1. Restart runtime: Runtime > Restart runtime\")\n",
        "        print(\"2. Check GPU allocation: Runtime > Change runtime type\")\n",
        "        print(\"3. Try running this diagnostic again\")\n",
        "        print(\"4. If still failing, use the CPU version instead\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVnqhu04jbI4",
        "outputId": "d2aae59b-428a-41cf-ed50-2e1596d75fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ COLAB GPU DIAGNOSTIC & FIX\n",
            "======================================================================\n",
            "üîç GPU DIAGNOSTIC REPORT\n",
            "==================================================\n",
            "‚úÖ NVIDIA driver working\n",
            "Thu Aug  7 12:42:21 2025       \n",
            "‚úÖ CUDA available: Cuda compilation tools, release 12.5, V12.5.82\n",
            "\n",
            "üîß TensorFlow version: 2.19.0\n",
            "üì± Physical GPUs: 1\n",
            "  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "‚úÖ TensorFlow built with CUDA\n",
            "‚úÖ GPU available to TensorFlow\n",
            "‚úÖ GPU computation test passed\n",
            "\n",
            "üîß APPLYING GPU FIXES\n",
            "==================================================\n",
            "‚úÖ Cleared TensorFlow session\n",
            "‚úÖ Enabled GPU memory growth\n",
            "‚ö†Ô∏è Memory limit warning: Virtual devices cannot be modified after being initialized\n",
            "‚úÖ Disabled mixed precision\n",
            "‚úÖ Enabled deterministic operations with seed\n",
            "\n",
            "üì¶ CHECKING PACKAGES\n",
            "==================================================\n",
            "‚úÖ resampy installed\n",
            "‚úÖ scipy installed\n",
            "\n",
            "üß™ FINAL GPU TEST\n",
            "==================================================\n",
            "‚ùå GPU training test failed: Graph execution error:\n",
            "\n",
            "Detected at node Adam/StatefulPartitionedCall_2 defined at (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "\n",
            "  File \"/tmp/ipython-input-3104489477.py\", line 187, in <cell line: 0>\n",
            "\n",
            "  File \"/tmp/ipython-input-3104489477.py\", line 174, in main\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/engine/training.py\", line 1151, in train_step\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 623, in minimize\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 1309, in apply_gradients\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 731, in apply_gradients\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 1339, in _internal_apply_gradients\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 1431, in _distributed_apply_gradients_fn\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tf_keras/src/optimizers/optimizer.py\", line 1426, in apply_grad_to_update_var\n",
            "\n",
            "DNN library initialization failed. Look at the errors above for more details.\n",
            "\t [[{{node Adam/StatefulPartitionedCall_2}}]] [Op:__inference_train_function_23652]\n",
            "\n",
            "Try restarting the runtime and running this diagnostic again.\n",
            "\n",
            "üí° TROUBLESHOOTING TIPS:\n",
            "1. Restart runtime: Runtime > Restart runtime\n",
            "2. Check GPU allocation: Runtime > Change runtime type\n",
            "3. Try running this diagnostic again\n",
            "4. If still failing, use the CPU version instead\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "GPU Diagnostic Script for Colab\n",
        "Run this first to check and fix GPU issues\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def check_gpu_setup():\n",
        "    \"\"\"Comprehensive GPU diagnostic\"\"\"\n",
        "    print(\"üîç GPU DIAGNOSTIC REPORT\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check NVIDIA driver\n",
        "    try:\n",
        "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ NVIDIA driver working\")\n",
        "            print(result.stdout.split('\\n')[0])  # First line with driver info\n",
        "        else:\n",
        "            print(\"‚ùå NVIDIA driver not working\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå nvidia-smi not found\")\n",
        "\n",
        "    # Check CUDA\n",
        "    try:\n",
        "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            cuda_version = [line for line in result.stdout.split('\\n') if 'release' in line]\n",
        "            if cuda_version:\n",
        "                print(f\"‚úÖ CUDA available: {cuda_version[0].strip()}\")\n",
        "        else:\n",
        "            print(\"‚ùå CUDA not available\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå nvcc not found\")\n",
        "\n",
        "    # Check TensorFlow GPU\n",
        "    print(f\"\\nüîß TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "    # List physical devices\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    print(f\"üì± Physical GPUs: {len(gpus)}\")\n",
        "    for i, gpu in enumerate(gpus):\n",
        "        print(f\"  GPU {i}: {gpu}\")\n",
        "\n",
        "    # Check if TensorFlow can see GPU\n",
        "    if tf.test.is_built_with_cuda():\n",
        "        print(\"‚úÖ TensorFlow built with CUDA\")\n",
        "    else:\n",
        "        print(\"‚ùå TensorFlow NOT built with CUDA\")\n",
        "\n",
        "    if tf.test.is_gpu_available():\n",
        "        print(\"‚úÖ GPU available to TensorFlow\")\n",
        "    else:\n",
        "        print(\"‚ùå GPU NOT available to TensorFlow\")\n",
        "\n",
        "    # Test GPU computation\n",
        "    try:\n",
        "        with tf.device('/GPU:0'):\n",
        "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
        "            b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
        "            c = tf.matmul(a, b)\n",
        "        print(\"‚úÖ GPU computation test passed\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GPU computation test failed: {e}\")\n",
        "\n",
        "    return len(gpus) > 0\n",
        "\n",
        "def fix_colab_gpu_issues():\n",
        "    \"\"\"Apply common fixes for Colab GPU issues\"\"\"\n",
        "    print(\"\\nüîß APPLYING GPU FIXES\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Clear TensorFlow session\n",
        "    tf.keras.backend.clear_session()\n",
        "    print(\"‚úÖ Cleared TensorFlow session\")\n",
        "\n",
        "    # Reset memory growth\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(\"‚úÖ Enabled GPU memory growth\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"‚ö†Ô∏è Memory growth warning: {e}\")\n",
        "\n",
        "    # Set memory limit\n",
        "    if gpus:\n",
        "        try:\n",
        "            tf.config.experimental.set_virtual_device_configuration(\n",
        "                gpus[0],\n",
        "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8192)]\n",
        "            )\n",
        "            print(\"‚úÖ Set GPU memory limit to 8GB\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Memory limit warning: {e}\")\n",
        "\n",
        "    # Disable mixed precision\n",
        "    tf.keras.mixed_precision.set_global_policy('float32')\n",
        "    print(\"‚úÖ Disabled mixed precision\")\n",
        "\n",
        "    # Enable deterministic operations with seed\n",
        "    try:\n",
        "        tf.config.experimental.enable_op_determinism()\n",
        "        tf.random.set_seed(42)  # Set seed for deterministic operations\n",
        "        print(\"‚úÖ Enabled deterministic operations with seed\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Determinism warning: {e}\")\n",
        "\n",
        "def install_missing_packages():\n",
        "    \"\"\"Install any missing packages\"\"\"\n",
        "    print(\"\\nüì¶ CHECKING PACKAGES\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    packages = ['resampy', 'scipy']\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"‚úÖ {package} installed\")\n",
        "        except ImportError:\n",
        "            print(f\"üì¶ Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "            print(f\"‚úÖ {package} installed\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main diagnostic\"\"\"\n",
        "    print(\"üöÄ COLAB GPU DIAGNOSTIC & FIX\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Check current GPU setup\n",
        "    has_gpu = check_gpu_setup()\n",
        "\n",
        "    if not has_gpu:\n",
        "        print(\"\\n‚ùå NO GPU DETECTED!\")\n",
        "        print(\"Make sure you're using a GPU runtime:\")\n",
        "        print(\"1. Go to Runtime > Change runtime type\")\n",
        "        print(\"2. Select 'GPU' as Hardware accelerator\")\n",
        "        print(\"3. Click Save and restart the runtime\")\n",
        "        return False\n",
        "\n",
        "    # Apply fixes\n",
        "    fix_colab_gpu_issues()\n",
        "\n",
        "    # Install packages\n",
        "    install_missing_packages()\n",
        "\n",
        "    # Final test\n",
        "    print(\"\\nüß™ FINAL GPU TEST\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Test a simple model\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(10, activation='relu', input_shape=(5,)),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "        # Generate dummy data with seed\n",
        "        import numpy as np\n",
        "        np.random.seed(42)\n",
        "        tf.random.set_seed(42)\n",
        "        X = np.random.random((100, 5))\n",
        "        y = np.random.randint(2, size=(100, 1))\n",
        "\n",
        "        # Train for 1 epoch\n",
        "        with tf.device('/GPU:0'):\n",
        "            model.fit(X, y, epochs=1, verbose=0)\n",
        "\n",
        "        print(\"‚úÖ GPU training test passed!\")\n",
        "        print(\"\\nüéâ GPU is ready for training!\")\n",
        "        print(\"You can now run: python colab_gpu_trainer.py\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GPU training test failed: {e}\")\n",
        "        print(\"\\nTry restarting the runtime and running this diagnostic again.\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    success = main()\n",
        "    if not success:\n",
        "        print(\"\\nüí° TROUBLESHOOTING TIPS:\")\n",
        "        print(\"1. Restart runtime: Runtime > Restart runtime\")\n",
        "        print(\"2. Check GPU allocation: Runtime > Change runtime type\")\n",
        "        print(\"3. Try running this diagnostic again\")\n",
        "        print(\"4. If still failing, use the CPU version instead\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvDMykUrjybd",
        "outputId": "51b1d9f6-143c-4e74-c773-0fc1fa2d7bc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ COLAB GPU DIAGNOSTIC & FIX\n",
            "======================================================================\n",
            "üîç GPU DIAGNOSTIC REPORT\n",
            "==================================================\n",
            "‚úÖ NVIDIA driver working\n",
            "Thu Aug  7 12:47:46 2025       \n",
            "‚úÖ CUDA available: Cuda compilation tools, release 12.5, V12.5.82\n",
            "\n",
            "üîß TensorFlow version: 2.19.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tmp/ipython-input-3104489477.py:55: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì± Physical GPUs: 1\n",
            "  GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "‚úÖ TensorFlow built with CUDA\n",
            "‚úÖ GPU available to TensorFlow\n",
            "‚úÖ GPU computation test passed\n",
            "\n",
            "üîß APPLYING GPU FIXES\n",
            "==================================================\n",
            "‚úÖ Cleared TensorFlow session\n",
            "‚ö†Ô∏è Memory growth warning: Physical devices cannot be modified after being initialized\n",
            "‚ö†Ô∏è Memory limit warning: Virtual devices cannot be modified after being initialized\n",
            "‚úÖ Disabled mixed precision\n",
            "‚úÖ Enabled deterministic operations with seed\n",
            "\n",
            "üì¶ CHECKING PACKAGES\n",
            "==================================================\n",
            "‚úÖ resampy installed\n",
            "‚úÖ scipy installed\n",
            "\n",
            "üß™ FINAL GPU TEST\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ GPU training test passed!\n",
            "\n",
            "üéâ GPU is ready for training!\n",
            "You can now run: python colab_gpu_trainer.py\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "GPU Diagnostic Script for Colab\n",
        "Run this first to check and fix GPU issues\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def check_gpu_setup():\n",
        "    \"\"\"Comprehensive GPU diagnostic\"\"\"\n",
        "    print(\"üîç GPU DIAGNOSTIC REPORT\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check NVIDIA driver\n",
        "    try:\n",
        "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ NVIDIA driver working\")\n",
        "            print(result.stdout.split('\\n')[0])  # First line with driver info\n",
        "        else:\n",
        "            print(\"‚ùå NVIDIA driver not working\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå nvidia-smi not found\")\n",
        "\n",
        "    # Check CUDA\n",
        "    try:\n",
        "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            cuda_version = [line for line in result.stdout.split('\\n') if 'release' in line]\n",
        "            if cuda_version:\n",
        "                print(f\"‚úÖ CUDA available: {cuda_version[0].strip()}\")\n",
        "        else:\n",
        "            print(\"‚ùå CUDA not available\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå nvcc not found\")\n",
        "\n",
        "    # Check TensorFlow GPU\n",
        "    print(f\"\\nüîß TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "    # List physical devices\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    print(f\"üì± Physical GPUs: {len(gpus)}\")\n",
        "    for i, gpu in enumerate(gpus):\n",
        "        print(f\"  GPU {i}: {gpu}\")\n",
        "\n",
        "    # Check if TensorFlow can see GPU\n",
        "    if tf.test.is_built_with_cuda():\n",
        "        print(\"‚úÖ TensorFlow built with CUDA\")\n",
        "    else:\n",
        "        print(\"‚ùå TensorFlow NOT built with CUDA\")\n",
        "\n",
        "    if tf.test.is_gpu_available():\n",
        "        print(\"‚úÖ GPU available to TensorFlow\")\n",
        "    else:\n",
        "        print(\"‚ùå GPU NOT available to TensorFlow\")\n",
        "\n",
        "    # Test GPU computation\n",
        "    try:\n",
        "        with tf.device('/GPU:0'):\n",
        "            a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
        "            b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
        "            c = tf.matmul(a, b)\n",
        "        print(\"‚úÖ GPU computation test passed\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GPU computation test failed: {e}\")\n",
        "\n",
        "    return len(gpus) > 0\n",
        "\n",
        "def fix_colab_gpu_issues():\n",
        "    \"\"\"Apply common fixes for Colab GPU issues\"\"\"\n",
        "    print(\"\\nüîß APPLYING GPU FIXES\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Clear TensorFlow session\n",
        "    tf.keras.backend.clear_session()\n",
        "    print(\"‚úÖ Cleared TensorFlow session\")\n",
        "\n",
        "    # Reset memory growth\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            print(\"‚úÖ Enabled GPU memory growth\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"‚ö†Ô∏è Memory growth warning: {e}\")\n",
        "\n",
        "    # Set memory limit\n",
        "    if gpus:\n",
        "        try:\n",
        "            tf.config.experimental.set_virtual_device_configuration(\n",
        "                gpus[0],\n",
        "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8192)]\n",
        "            )\n",
        "            print(\"‚úÖ Set GPU memory limit to 8GB\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Memory limit warning: {e}\")\n",
        "\n",
        "    # Disable mixed precision\n",
        "    tf.keras.mixed_precision.set_global_policy('float32')\n",
        "    print(\"‚úÖ Disabled mixed precision\")\n",
        "\n",
        "    # Enable deterministic operations with seed\n",
        "    try:\n",
        "        tf.config.experimental.enable_op_determinism()\n",
        "        tf.random.set_seed(42)  # Set seed for deterministic operations\n",
        "        print(\"‚úÖ Enabled deterministic operations with seed\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Determinism warning: {e}\")\n",
        "\n",
        "def install_missing_packages():\n",
        "    \"\"\"Install any missing packages\"\"\"\n",
        "    print(\"\\nüì¶ CHECKING PACKAGES\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    packages = ['resampy', 'scipy']\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"‚úÖ {package} installed\")\n",
        "        except ImportError:\n",
        "            print(f\"üì¶ Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "            print(f\"‚úÖ {package} installed\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main diagnostic\"\"\"\n",
        "    print(\"üöÄ COLAB GPU DIAGNOSTIC & FIX\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Check current GPU setup\n",
        "    has_gpu = check_gpu_setup()\n",
        "\n",
        "    if not has_gpu:\n",
        "        print(\"\\n‚ùå NO GPU DETECTED!\")\n",
        "        print(\"Make sure you're using a GPU runtime:\")\n",
        "        print(\"1. Go to Runtime > Change runtime type\")\n",
        "        print(\"2. Select 'GPU' as Hardware accelerator\")\n",
        "        print(\"3. Click Save and restart the runtime\")\n",
        "        return False\n",
        "\n",
        "    # Apply fixes\n",
        "    fix_colab_gpu_issues()\n",
        "\n",
        "    # Install packages\n",
        "    install_missing_packages()\n",
        "\n",
        "    # Final test\n",
        "    print(\"\\nüß™ FINAL GPU TEST\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Test a simple model\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(10, activation='relu', input_shape=(5,)),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "        # Generate dummy data with seed\n",
        "        import numpy as np\n",
        "        np.random.seed(42)\n",
        "        tf.random.set_seed(42)\n",
        "        X = np.random.random((100, 5))\n",
        "        y = np.random.randint(2, size=(100, 1))\n",
        "\n",
        "        # Train for 1 epoch\n",
        "        with tf.device('/GPU:0'):\n",
        "            model.fit(X, y, epochs=1, verbose=0)\n",
        "\n",
        "        print(\"‚úÖ GPU training test passed!\")\n",
        "        print(\"\\nüéâ GPU is ready for training!\")\n",
        "        print(\"You can now run: python colab_gpu_trainer.py\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GPU training test failed: {e}\")\n",
        "        print(\"\\nTry restarting the runtime and running this diagnostic again.\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    success = main()\n",
        "    if not success:\n",
        "        print(\"\\nüí° TROUBLESHOOTING TIPS:\")\n",
        "        print(\"1. Restart runtime: Runtime > Restart runtime\")\n",
        "        print(\"2. Check GPU allocation: Runtime > Change runtime type\")\n",
        "        print(\"3. Try running this diagnostic again\")\n",
        "        print(\"4. If still failing, use the CPU version instead\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "764811ea9e1b4119931bb7963c971af6",
            "8c3567c085bd48aead16f45ce1e0e1ba",
            "8d33673a829640188fb9739ea172a95d",
            "92619e279a154c7393b31d193a87cd97",
            "e8b331cfd8fe477590d0db6271c16d70",
            "cebf7351eafe41bba4e3dc6f716e5611",
            "6c9770922ff94ad3a6fcf0801ae08d3b",
            "a9127fcadf6445be8a009aeea6713163",
            "46ffeed30add4a30908d5409bd6e48df",
            "3dc584ffa8de4058ba968da54123d684",
            "3a527bac78914b959f2d21e0c050922b",
            "616f9f7121f946ae89268df99134a5d7",
            "705ab46ed5104157a9ca50c6a75221d1",
            "329eab22e37840ab94348d23924355f5",
            "7f42e1c1ebe64b90a4e7b6ba8183dd4d",
            "f4c6cd6131234420970facd9e1e62707",
            "513323030429496bab88edfdb3c86b6a",
            "7379a4a1c3a442f2b43b583ef9450714",
            "9c5bcce35b2f4fb4828b5fc33ef96d10",
            "a5637f1de4d94692a9bef07b68c69b59",
            "33b929e304e04a34bfc85596f4ddc17c",
            "e9f080cf72004611b9ac72b6089e08b8"
          ]
        },
        "id": "3GA1kYTOk37_",
        "outputId": "015656d5-2d22-4320-8d8a-98e069bb867e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "764811ea9e1b4119931bb7963c971af6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating healthy:   0%|          | 0/2139 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "616f9f7121f946ae89268df99134a5d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating sick:   0%|          | 0/2121 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:GPU configuration warning: Physical devices cannot be modified after being initialized\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "    706/Unknown \u001b[1m97s\u001b[0m 115ms/step - accuracy: 0.4662 - loss: 1.1592\n",
            "Epoch 1: val_accuracy improved from -inf to 0.52893, saving model to ./colab_gpu_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 135ms/step - accuracy: 0.4663 - loss: 1.1591 - val_accuracy: 0.5289 - val_loss: 1.0225 - learning_rate: 5.0000e-06\n",
            "Epoch 2/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.5527 - loss: 0.9479\n",
            "Epoch 2: val_accuracy improved from 0.52893 to 0.59339, saving model to ./colab_gpu_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 150ms/step - accuracy: 0.5527 - loss: 0.9478 - val_accuracy: 0.5934 - val_loss: 0.9543 - learning_rate: 5.0000e-06\n",
            "Epoch 3/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.5364 - loss: 0.9404\n",
            "Epoch 3: val_accuracy improved from 0.59339 to 0.61157, saving model to ./colab_gpu_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 201ms/step - accuracy: 0.5364 - loss: 0.9404 - val_accuracy: 0.6116 - val_loss: 0.9084 - learning_rate: 5.0000e-06\n",
            "Epoch 4/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.5503 - loss: 0.9336\n",
            "Epoch 4: val_accuracy improved from 0.61157 to 0.63636, saving model to ./colab_gpu_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 129ms/step - accuracy: 0.5503 - loss: 0.9336 - val_accuracy: 0.6364 - val_loss: 0.7924 - learning_rate: 5.0000e-06\n",
            "Epoch 5/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.5945 - loss: 0.8529\n",
            "Epoch 5: val_accuracy did not improve from 0.63636\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 128ms/step - accuracy: 0.5945 - loss: 0.8529 - val_accuracy: 0.6248 - val_loss: 0.8110 - learning_rate: 5.0000e-06\n",
            "Epoch 6/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.5910 - loss: 0.8528\n",
            "Epoch 6: val_accuracy did not improve from 0.63636\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 201ms/step - accuracy: 0.5910 - loss: 0.8528 - val_accuracy: 0.6331 - val_loss: 0.8054 - learning_rate: 5.0000e-06\n",
            "Epoch 7/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.6040 - loss: 0.8379\n",
            "Epoch 7: val_accuracy improved from 0.63636 to 0.65124, saving model to ./colab_gpu_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 128ms/step - accuracy: 0.6040 - loss: 0.8379 - val_accuracy: 0.6512 - val_loss: 0.7626 - learning_rate: 5.0000e-06\n",
            "Epoch 8/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.6036 - loss: 0.8883\n",
            "Epoch 8: val_accuracy improved from 0.65124 to 0.68926, saving model to ./colab_gpu_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 212ms/step - accuracy: 0.6036 - loss: 0.8882 - val_accuracy: 0.6893 - val_loss: 0.7720 - learning_rate: 5.0000e-06\n",
            "Epoch 9/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.6186 - loss: 0.8461\n",
            "Epoch 9: val_accuracy did not improve from 0.68926\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 129ms/step - accuracy: 0.6186 - loss: 0.8461 - val_accuracy: 0.6694 - val_loss: 0.7516 - learning_rate: 5.0000e-06\n",
            "Epoch 10/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.6292 - loss: 0.8362\n",
            "Epoch 10: val_accuracy did not improve from 0.68926\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 128ms/step - accuracy: 0.6292 - loss: 0.8361 - val_accuracy: 0.6579 - val_loss: 0.7967 - learning_rate: 5.0000e-06\n",
            "Epoch 11/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.6240 - loss: 0.8303\n",
            "Epoch 11: val_accuracy did not improve from 0.68926\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 129ms/step - accuracy: 0.6240 - loss: 0.8303 - val_accuracy: 0.6529 - val_loss: 0.7739 - learning_rate: 5.0000e-06\n",
            "Epoch 12/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.5998 - loss: 0.8482\n",
            "Epoch 12: val_accuracy did not improve from 0.68926\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 127ms/step - accuracy: 0.5998 - loss: 0.8482 - val_accuracy: 0.6810 - val_loss: 0.7879 - learning_rate: 5.0000e-06\n",
            "Epoch 13/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.6190 - loss: 0.8010\n",
            "Epoch 13: val_accuracy improved from 0.68926 to 0.69752, saving model to ./colab_gpu_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 124ms/step - accuracy: 0.6190 - loss: 0.8010 - val_accuracy: 0.6975 - val_loss: 0.7587 - learning_rate: 5.0000e-06\n",
            "Epoch 14/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.6455 - loss: 0.7970\n",
            "Epoch 14: val_accuracy did not improve from 0.69752\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 123ms/step - accuracy: 0.6455 - loss: 0.7970 - val_accuracy: 0.6926 - val_loss: 0.7356 - learning_rate: 5.0000e-06\n",
            "Epoch 15/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.6278 - loss: 0.7890\n",
            "Epoch 15: val_accuracy did not improve from 0.69752\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 124ms/step - accuracy: 0.6278 - loss: 0.7890 - val_accuracy: 0.6959 - val_loss: 0.7470 - learning_rate: 5.0000e-06\n",
            "Epoch 16/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.6067 - loss: 0.8120\n",
            "Epoch 16: val_accuracy improved from 0.69752 to 0.70083, saving model to ./colab_gpu_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 201ms/step - accuracy: 0.6068 - loss: 0.8121 - val_accuracy: 0.7008 - val_loss: 0.7278 - learning_rate: 5.0000e-06\n",
            "Epoch 17/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.6362 - loss: 0.7739\n",
            "Epoch 17: val_accuracy did not improve from 0.70083\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 201ms/step - accuracy: 0.6362 - loss: 0.7739 - val_accuracy: 0.6893 - val_loss: 0.7635 - learning_rate: 5.0000e-06\n",
            "Epoch 18/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.6319 - loss: 0.7923\n",
            "Epoch 18: val_accuracy improved from 0.70083 to 0.70413, saving model to ./colab_gpu_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 202ms/step - accuracy: 0.6319 - loss: 0.7923 - val_accuracy: 0.7041 - val_loss: 0.7185 - learning_rate: 5.0000e-06\n",
            "Epoch 19/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.6225 - loss: 0.8019\n",
            "Epoch 19: val_accuracy improved from 0.70413 to 0.72066, saving model to ./colab_gpu_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 202ms/step - accuracy: 0.6225 - loss: 0.8019 - val_accuracy: 0.7207 - val_loss: 0.6892 - learning_rate: 5.0000e-06\n",
            "Epoch 20/20\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.6236 - loss: 0.7845\n",
            "Epoch 20: val_accuracy did not improve from 0.72066\n",
            "\u001b[1m706/706\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 201ms/step - accuracy: 0.6236 - loss: 0.7845 - val_accuracy: 0.6959 - val_loss: 0.6933 - learning_rate: 5.0000e-06\n",
            "Restoring model weights from the end of the best epoch: 19.\n",
            "\u001b[1m152/152\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 85ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     healthy       0.62      0.67      0.64       303\n",
            "        sick       0.64      0.60      0.62       303\n",
            "\n",
            "    accuracy                           0.63       606\n",
            "   macro avg       0.63      0.63      0.63       606\n",
            "weighted avg       0.63      0.63      0.63       606\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Colab GPU-Optimized Robust Audio Classification Trainer\n",
        "Fixes GPU/CUDA issues specifically for Google Colab\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import warnings\n",
        "import json\n",
        "import datetime\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class ColabGPUConfig:\n",
        "    \"\"\"Colab GPU-optimized configuration\"\"\"\n",
        "\n",
        "    # Model configuration\n",
        "    SAMPLE_RATE: int = 16000\n",
        "    DURATION: float = 3.0\n",
        "    MAX_LENGTH: int = 48000\n",
        "\n",
        "    # Training configuration - GPU optimized for Colab\n",
        "    BATCH_SIZE: int = 4  # Small batch to avoid OOM\n",
        "    EPOCHS: int = 20\n",
        "    LEARNING_RATE: float = 5e-6  # Very conservative for stability\n",
        "\n",
        "    # Data configuration\n",
        "    TRAIN_SPLIT: float = 0.7\n",
        "    VAL_SPLIT: float = 0.15\n",
        "    TEST_SPLIT: float = 0.15\n",
        "\n",
        "    # Bias prevention\n",
        "    MAX_CLASS_IMBALANCE_RATIO: float = 3.0\n",
        "    MIN_SAMPLES_PER_CLASS: int = 50\n",
        "\n",
        "    # Audio validation\n",
        "    MIN_AUDIO_LENGTH: float = 1.0\n",
        "    MAX_AUDIO_LENGTH: float = 10.0\n",
        "\n",
        "    # Augmentation\n",
        "    USE_AUGMENTATION: bool = True\n",
        "    AUGMENTATION_PROB: float = 0.3\n",
        "\n",
        "    # Directory configuration\n",
        "    BASE_DIR: str = '/content/smartearsaudio/sm'\n",
        "    HEALTHY_DIR: str = '/content/smartearsaudio/sm/Healthy'\n",
        "    SICK_DIR: str = '/content/smartearsaudio/sm/Sick'\n",
        "    NOISE_DIR: str = '/content/smartearsaudio/sm/noise'\n",
        "    OUTPUT_DIR: str = './colab_gpu_model_output'\n",
        "\n",
        "\n",
        "def fix_colab_gpu():\n",
        "    \"\"\"Fix GPU issues specific to Google Colab\"\"\"\n",
        "    logger.info(\"üîß Fixing Colab GPU configuration...\")\n",
        "\n",
        "    # Clear any existing TF state\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Reset TensorFlow\n",
        "    try:\n",
        "        tf.config.experimental.reset_memory_growth_v2()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Configure GPU memory growth\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            # Enable memory growth for all GPUs\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "            # Set a reasonable memory limit (adjust based on your Colab GPU)\n",
        "            tf.config.experimental.set_virtual_device_configuration(\n",
        "                gpus[0],\n",
        "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8192)]  # 8GB limit\n",
        "            )\n",
        "\n",
        "            logger.info(f\"‚úÖ GPU configured: {len(gpus)} GPU(s) available\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            logger.warning(f\"GPU configuration warning: {e}\")\n",
        "    else:\n",
        "        logger.warning(\"‚ö†Ô∏è No GPU detected\")\n",
        "\n",
        "    # Disable mixed precision to avoid type issues\n",
        "    tf.keras.mixed_precision.set_global_policy('float32')\n",
        "\n",
        "    # Set TensorFlow to use deterministic operations with seed\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "    tf.random.set_seed(42)  # Set seed for deterministic operations\n",
        "\n",
        "    logger.info(\"‚úÖ Colab GPU configuration complete\")\n",
        "\n",
        "\n",
        "class ColabAudioProcessor:\n",
        "    \"\"\"Colab-optimized audio processing\"\"\"\n",
        "\n",
        "    def __init__(self, config: ColabGPUConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def validate_audio_file(self, file_path: str) -> Tuple[bool, str]:\n",
        "        \"\"\"Validate audio file\"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(file_path):\n",
        "                return False, \"File does not exist\"\n",
        "\n",
        "            if os.path.getsize(file_path) < 1000:\n",
        "                return False, \"File too small\"\n",
        "\n",
        "            # Load and check basic properties\n",
        "            try:\n",
        "                waveform, sr = librosa.load(file_path, sr=None, mono=True)\n",
        "            except Exception as e:\n",
        "                return False, f\"Cannot load audio: {e}\"\n",
        "\n",
        "            duration = len(waveform) / sr\n",
        "            if duration < self.config.MIN_AUDIO_LENGTH:\n",
        "                return False, f\"Audio too short: {duration:.2f}s\"\n",
        "            if duration > self.config.MAX_AUDIO_LENGTH:\n",
        "                return False, f\"Audio too long: {duration:.2f}s\"\n",
        "\n",
        "            if np.std(waveform) < 1e-6:\n",
        "                return False, \"Audio is silent\"\n",
        "\n",
        "            if not np.isfinite(waveform).all():\n",
        "                return False, \"Audio contains NaN/Inf\"\n",
        "\n",
        "            return True, \"Valid\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, f\"Validation error: {e}\"\n",
        "\n",
        "    def load_and_preprocess_audio(self, file_path: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"Load and preprocess audio with Colab-specific fixes\"\"\"\n",
        "        try:\n",
        "            is_valid, reason = self.validate_audio_file(file_path)\n",
        "            if not is_valid:\n",
        "                return None\n",
        "\n",
        "            # Load audio with fallback for resampling issues\n",
        "            try:\n",
        "                waveform, sr = librosa.load(\n",
        "                    file_path,\n",
        "                    sr=self.config.SAMPLE_RATE,\n",
        "                    mono=True,\n",
        "                    res_type='scipy'  # Use scipy instead of resampy\n",
        "                )\n",
        "            except Exception as e:\n",
        "                if 'resampy' in str(e) or 'res_type' in str(e):\n",
        "                    # Fallback without resampling\n",
        "                    waveform, original_sr = librosa.load(file_path, sr=None, mono=True)\n",
        "                    if original_sr != self.config.SAMPLE_RATE:\n",
        "                        waveform = self._scipy_resample(waveform, original_sr, self.config.SAMPLE_RATE)\n",
        "                else:\n",
        "                    return None\n",
        "\n",
        "            # Handle length\n",
        "            target_length = self.config.MAX_LENGTH\n",
        "\n",
        "            if len(waveform) > target_length:\n",
        "                # Take center portion\n",
        "                start = (len(waveform) - target_length) // 2\n",
        "                waveform = waveform[start:start + target_length]\n",
        "            elif len(waveform) < target_length:\n",
        "                # Pad with zeros\n",
        "                waveform = np.pad(waveform, (0, target_length - len(waveform)), 'constant')\n",
        "\n",
        "            # Normalize\n",
        "            waveform = self._normalize_audio(waveform)\n",
        "\n",
        "            if not np.isfinite(waveform).all():\n",
        "                return None\n",
        "\n",
        "            return waveform.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.debug(f\"Error processing {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _scipy_resample(self, waveform: np.ndarray, orig_sr: int, target_sr: int) -> np.ndarray:\n",
        "        \"\"\"Resample using scipy\"\"\"\n",
        "        try:\n",
        "            from scipy import signal\n",
        "            if orig_sr == target_sr:\n",
        "                return waveform\n",
        "\n",
        "            ratio = target_sr / orig_sr\n",
        "            new_length = int(len(waveform) * ratio)\n",
        "            resampled = signal.resample(waveform, new_length)\n",
        "            return resampled.astype(np.float32)\n",
        "\n",
        "        except ImportError:\n",
        "            # Fallback to linear interpolation\n",
        "            ratio = target_sr / orig_sr\n",
        "            new_length = int(len(waveform) * ratio)\n",
        "            old_indices = np.linspace(0, len(waveform) - 1, len(waveform))\n",
        "            new_indices = np.linspace(0, len(waveform) - 1, new_length)\n",
        "            resampled = np.interp(new_indices, old_indices, waveform)\n",
        "            return resampled.astype(np.float32)\n",
        "\n",
        "    def _normalize_audio(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Robust normalization\"\"\"\n",
        "        try:\n",
        "            # Remove DC offset\n",
        "            waveform = waveform - np.mean(waveform)\n",
        "\n",
        "            # Scale using 99th percentile\n",
        "            p99 = np.percentile(np.abs(waveform), 99)\n",
        "            if p99 > 1e-8:\n",
        "                waveform = waveform / p99 * 0.8\n",
        "\n",
        "            # Clip to prevent extreme values\n",
        "            waveform = np.clip(waveform, -1.0, 1.0)\n",
        "\n",
        "            return waveform\n",
        "\n",
        "        except Exception:\n",
        "            return np.zeros_like(waveform)\n",
        "\n",
        "    def apply_augmentation(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Light augmentation\"\"\"\n",
        "        if not self.config.USE_AUGMENTATION:\n",
        "            return waveform\n",
        "\n",
        "        try:\n",
        "            augmented = waveform.copy()\n",
        "\n",
        "            # Time shifting\n",
        "            if np.random.random() < self.config.AUGMENTATION_PROB:\n",
        "                shift = int(np.random.uniform(-0.05, 0.05) * len(augmented))\n",
        "                augmented = np.roll(augmented, shift)\n",
        "\n",
        "            # Volume scaling\n",
        "            if np.random.random() < self.config.AUGMENTATION_PROB:\n",
        "                scale = np.random.uniform(0.8, 1.2)\n",
        "                augmented = augmented * scale\n",
        "\n",
        "            # Add noise\n",
        "            if np.random.random() < self.config.AUGMENTATION_PROB * 0.5:\n",
        "                noise_level = np.random.uniform(0.001, 0.005)\n",
        "                noise = np.random.normal(0, noise_level, len(augmented))\n",
        "                augmented = augmented + noise\n",
        "\n",
        "            augmented = np.clip(augmented, -1.0, 1.0)\n",
        "\n",
        "            if not np.isfinite(augmented).all():\n",
        "                return waveform\n",
        "\n",
        "            return augmented\n",
        "\n",
        "        except Exception:\n",
        "            return waveform\n",
        "\n",
        "    def extract_features(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Extract mel spectrogram features\"\"\"\n",
        "        try:\n",
        "            # Compute mel spectrogram\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=waveform,\n",
        "                sr=self.config.SAMPLE_RATE,\n",
        "                n_mels=80,\n",
        "                hop_length=512,\n",
        "                n_fft=1024\n",
        "            )\n",
        "\n",
        "            # Convert to log scale\n",
        "            log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "            # Flatten and pad/truncate to fixed size\n",
        "            features = log_mel.flatten()\n",
        "            target_size = 800  # Fixed feature size\n",
        "\n",
        "            if len(features) > target_size:\n",
        "                features = features[:target_size]\n",
        "            else:\n",
        "                features = np.pad(features, (0, target_size - len(features)), 'constant')\n",
        "\n",
        "            return features.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Feature extraction failed: {e}\")\n",
        "            return np.zeros(800, dtype=np.float32)\n",
        "\n",
        "\n",
        "class ColabDatasetBuilder:\n",
        "    \"\"\"Colab-optimized dataset builder\"\"\"\n",
        "\n",
        "    def __init__(self, config: ColabGPUConfig):\n",
        "        self.config = config\n",
        "        self.processor = ColabAudioProcessor(config)\n",
        "\n",
        "    def collect_and_balance_data(self) -> Tuple[List[str], List[int], List[str]]:\n",
        "        \"\"\"Collect and balance dataset\"\"\"\n",
        "        logger.info(\"Collecting and balancing dataset...\")\n",
        "\n",
        "        # Collect files\n",
        "        healthy_files = self._collect_files(self.config.HEALTHY_DIR, \"healthy\")\n",
        "        sick_files = self._collect_files(self.config.SICK_DIR, \"sick\")\n",
        "\n",
        "        logger.info(f\"Raw counts - Healthy: {len(healthy_files)}, Sick: {len(sick_files)}\")\n",
        "\n",
        "        # Validate files\n",
        "        healthy_files = self._validate_files(healthy_files, \"healthy\")\n",
        "        sick_files = self._validate_files(sick_files, \"sick\")\n",
        "\n",
        "        logger.info(f\"Valid counts - Healthy: {len(healthy_files)}, Sick: {len(sick_files)}\")\n",
        "\n",
        "        # Check minimum requirements\n",
        "        if len(healthy_files) < self.config.MIN_SAMPLES_PER_CLASS:\n",
        "            raise ValueError(f\"Insufficient healthy samples: {len(healthy_files)}\")\n",
        "        if len(sick_files) < self.config.MIN_SAMPLES_PER_CLASS:\n",
        "            raise ValueError(f\"Insufficient sick samples: {len(sick_files)}\")\n",
        "\n",
        "        # Balance classes\n",
        "        healthy_files, sick_files = self._balance_classes(healthy_files, sick_files)\n",
        "\n",
        "        # Create final dataset\n",
        "        all_files = healthy_files + sick_files\n",
        "        all_labels = [0] * len(healthy_files) + [1] * len(sick_files)\n",
        "        class_names = ['healthy', 'sick']\n",
        "\n",
        "        logger.info(f\"Final dataset - Healthy: {len(healthy_files)}, Sick: {len(sick_files)}\")\n",
        "\n",
        "        return all_files, all_labels, class_names\n",
        "\n",
        "    def _collect_files(self, directory: str, category: str) -> List[str]:\n",
        "        \"\"\"Collect audio files\"\"\"\n",
        "        files = []\n",
        "        if os.path.exists(directory):\n",
        "            for ext in ['.wav', '.mp3', '.flac']:\n",
        "                files.extend(Path(directory).rglob(f'*{ext}'))\n",
        "        return [str(f) for f in files]\n",
        "\n",
        "    def _validate_files(self, files: List[str], category: str) -> List[str]:\n",
        "        \"\"\"Validate audio files\"\"\"\n",
        "        valid_files = []\n",
        "\n",
        "        logger.info(f\"Validating {len(files)} {category} files...\")\n",
        "\n",
        "        for file_path in tqdm(files, desc=f\"Validating {category}\"):\n",
        "            is_valid, reason = self.processor.validate_audio_file(file_path)\n",
        "            if is_valid:\n",
        "                valid_files.append(file_path)\n",
        "\n",
        "        return valid_files\n",
        "\n",
        "    def _balance_classes(self, healthy_files: List[str], sick_files: List[str]) -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"Balance classes to prevent bias\"\"\"\n",
        "        counts = [len(healthy_files), len(sick_files)]\n",
        "        min_count = min(counts)\n",
        "        max_count = max(counts)\n",
        "\n",
        "        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
        "\n",
        "        if imbalance_ratio > self.config.MAX_CLASS_IMBALANCE_RATIO:\n",
        "            logger.info(f\"Balancing classes (ratio: {imbalance_ratio:.2f})\")\n",
        "            target_size = min_count\n",
        "\n",
        "            if len(healthy_files) > target_size:\n",
        "                healthy_files = np.random.choice(healthy_files, target_size, replace=False).tolist()\n",
        "            if len(sick_files) > target_size:\n",
        "                sick_files = np.random.choice(sick_files, target_size, replace=False).tolist()\n",
        "\n",
        "        return healthy_files, sick_files\n",
        "\n",
        "\n",
        "class ColabGPUTrainer:\n",
        "    \"\"\"Colab GPU-optimized trainer\"\"\"\n",
        "\n",
        "    def __init__(self, config: ColabGPUConfig):\n",
        "        self.config = config\n",
        "        self.processor = ColabAudioProcessor(config)\n",
        "\n",
        "        os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "        # Fix GPU configuration\n",
        "        fix_colab_gpu()\n",
        "\n",
        "    def create_dataset(self, file_paths: List[str], labels: List[int], is_training: bool = True) -> tf.data.Dataset:\n",
        "        \"\"\"Create GPU-optimized dataset\"\"\"\n",
        "\n",
        "        def load_audio_tf(file_path, label):\n",
        "            def load_and_process(path):\n",
        "                path_str = path.numpy().decode('utf-8')\n",
        "                waveform = self.processor.load_and_preprocess_audio(path_str)\n",
        "\n",
        "                if waveform is None:\n",
        "                    waveform = np.zeros(self.config.MAX_LENGTH, dtype=np.float32)\n",
        "                else:\n",
        "                    if is_training:\n",
        "                        waveform = self.processor.apply_augmentation(waveform)\n",
        "\n",
        "                # Extract features\n",
        "                features = self.processor.extract_features(waveform)\n",
        "                return features\n",
        "\n",
        "            audio_data = tf.py_function(\n",
        "                load_and_process,\n",
        "                inp=[file_path],\n",
        "                Tout=tf.float32\n",
        "            )\n",
        "            audio_data.set_shape([800])  # Fixed feature size\n",
        "\n",
        "            return audio_data, label\n",
        "\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "\n",
        "        if is_training:\n",
        "            dataset = dataset.shuffle(buffer_size=min(1000, len(file_paths)))\n",
        "\n",
        "        dataset = dataset.map(load_audio_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.filter(lambda audio, label: tf.reduce_all(tf.math.is_finite(audio)))\n",
        "        dataset = dataset.batch(self.config.BATCH_SIZE)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def create_model(self, num_classes: int) -> tf.keras.Model:\n",
        "        \"\"\"Create GPU-optimized model\"\"\"\n",
        "\n",
        "        # Use tf.keras.utils.get_custom_objects() to avoid issues\n",
        "        with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
        "            model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Input(shape=(800,)),\n",
        "\n",
        "                # Dense layers with proper initialization\n",
        "                tf.keras.layers.Dense(\n",
        "                    512,\n",
        "                    activation='relu',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    bias_initializer='zeros'\n",
        "                ),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Dropout(0.3),\n",
        "\n",
        "                tf.keras.layers.Dense(\n",
        "                    256,\n",
        "                    activation='relu',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    bias_initializer='zeros'\n",
        "                ),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "                tf.keras.layers.Dense(\n",
        "                    128,\n",
        "                    activation='relu',\n",
        "                    kernel_initializer='he_normal',\n",
        "                    bias_initializer='zeros'\n",
        "                ),\n",
        "                tf.keras.layers.Dropout(0.1),\n",
        "\n",
        "                tf.keras.layers.Dense(\n",
        "                    num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='glorot_uniform',\n",
        "                    bias_initializer='zeros',\n",
        "                    dtype='float32'  # Ensure float32 output\n",
        "                )\n",
        "            ])\n",
        "\n",
        "        logger.info(\"‚úÖ GPU model created\")\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, model: tf.keras.Model) -> tf.keras.Model:\n",
        "        \"\"\"Compile model with GPU-optimized settings\"\"\"\n",
        "\n",
        "        # Use a very stable optimizer configuration\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=self.config.LEARNING_RATE,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.999,\n",
        "            epsilon=1e-7,  # Slightly larger epsilon for stability\n",
        "            clipnorm=0.5   # Aggressive gradient clipping\n",
        "        )\n",
        "\n",
        "        # Simple loss function to avoid type issues\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "            metrics=['accuracy'],\n",
        "            run_eagerly=False  # Ensure graph mode for GPU efficiency\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def create_callbacks(self) -> List[tf.keras.callbacks.Callback]:\n",
        "        \"\"\"Create training callbacks\"\"\"\n",
        "        return [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=10,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                os.path.join(self.config.OUTPUT_DIR, 'best_model.h5'),\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.TerminateOnNaN()\n",
        "        ]\n",
        "\n",
        "    def train(self, train_dataset, val_dataset, class_weights, num_classes):\n",
        "        \"\"\"Train model on GPU\"\"\"\n",
        "        logger.info(\"Creating and compiling model...\")\n",
        "\n",
        "        model = self.create_model(num_classes)\n",
        "        model = self.compile_model(model)\n",
        "\n",
        "        logger.info(\"Model architecture:\")\n",
        "        model.summary(print_fn=logger.info)\n",
        "\n",
        "        callbacks = self.create_callbacks()\n",
        "\n",
        "        logger.info(f\"Starting GPU training for {self.config.EPOCHS} epochs...\")\n",
        "\n",
        "        try:\n",
        "            # Use tf.distribute.MirroredStrategy if multiple GPUs\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "\n",
        "            with strategy.scope():\n",
        "                history = model.fit(\n",
        "                    train_dataset,\n",
        "                    epochs=self.config.EPOCHS,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks,\n",
        "                    verbose=1,\n",
        "                    class_weight=class_weights\n",
        "                )\n",
        "\n",
        "            logger.info(\"‚úÖ GPU training completed successfully!\")\n",
        "            return model, history\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Training failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def evaluate_model(self, model, test_dataset, class_names):\n",
        "        \"\"\"Evaluate model\"\"\"\n",
        "        logger.info(\"Evaluating model...\")\n",
        "\n",
        "        try:\n",
        "            predictions = model.predict(test_dataset, verbose=1)\n",
        "\n",
        "            y_true = []\n",
        "            for _, labels in test_dataset:\n",
        "                y_true.extend(labels.numpy())\n",
        "            y_true = np.array(y_true)\n",
        "\n",
        "            y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "            accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "            logger.info(f\"Balanced Accuracy: {accuracy:.4f}\")\n",
        "            logger.info(\"\\nClassification Report:\")\n",
        "            print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "            # Confusion matrix\n",
        "            cm = confusion_matrix(y_true, y_pred)\n",
        "            self._plot_confusion_matrix(cm, class_names)\n",
        "\n",
        "            return {\n",
        "                'accuracy': accuracy,\n",
        "                'y_true': y_true,\n",
        "                'y_pred': y_pred,\n",
        "                'predictions': predictions,\n",
        "                'confusion_matrix': cm\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Evaluation failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _plot_confusion_matrix(self, cm, class_names):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.config.OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main Colab GPU training pipeline\"\"\"\n",
        "    logger.info(\"üöÄ Starting Colab GPU-optimized audio classification...\")\n",
        "\n",
        "    config = ColabGPUConfig()\n",
        "\n",
        "    try:\n",
        "        # Build dataset\n",
        "        dataset_builder = ColabDatasetBuilder(config)\n",
        "        all_files, all_labels, class_names = dataset_builder.collect_and_balance_data()\n",
        "\n",
        "        if len(all_files) == 0:\n",
        "            raise ValueError(\"No valid audio files found!\")\n",
        "\n",
        "        # Split data\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            all_files, all_labels,\n",
        "            test_size=config.TEST_SPLIT,\n",
        "            random_state=42,\n",
        "            stratify=all_labels\n",
        "        )\n",
        "\n",
        "        val_size = config.VAL_SPLIT / (1 - config.TEST_SPLIT)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp,\n",
        "            test_size=val_size,\n",
        "            random_state=42,\n",
        "            stratify=y_temp\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Data split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
        "\n",
        "        # Calculate class weights\n",
        "        class_weights = compute_class_weight(\n",
        "            'balanced',\n",
        "            classes=np.unique(y_train),\n",
        "            y=y_train\n",
        "        )\n",
        "        class_weight_dict = dict(enumerate(class_weights))\n",
        "        logger.info(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "        # Create trainer\n",
        "        trainer = ColabGPUTrainer(config)\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = trainer.create_dataset(X_train, y_train, is_training=True)\n",
        "        val_dataset = trainer.create_dataset(X_val, y_val, is_training=False)\n",
        "        test_dataset = trainer.create_dataset(X_test, y_test, is_training=False)\n",
        "\n",
        "        # Train model\n",
        "        model, history = trainer.train(\n",
        "            train_dataset, val_dataset, class_weight_dict, len(class_names)\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        results = trainer.evaluate_model(model, test_dataset, class_names)\n",
        "\n",
        "        # Save results\n",
        "        model.save(os.path.join(config.OUTPUT_DIR, 'final_model.h5'))\n",
        "\n",
        "        with open(os.path.join(config.OUTPUT_DIR, 'training_results.json'), 'w') as f:\n",
        "            json.dump({\n",
        "                'class_names': class_names,\n",
        "                'accuracy': float(results.get('accuracy', 0)),\n",
        "                'class_weights': {str(k): float(v) for k, v in class_weight_dict.items()},\n",
        "                'training_mode': 'GPU',\n",
        "                'timestamp': datetime.datetime.now().isoformat()\n",
        "            }, f, indent=2)\n",
        "\n",
        "        logger.info(f\"üéâ GPU training completed! Results saved to {config.OUTPUT_DIR}\")\n",
        "        logger.info(f\"üìä Final Accuracy: {results.get('accuracy', 0):.2%}\")\n",
        "\n",
        "        return model, results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Training pipeline failed: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, results = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "LF7qf9Eswy0D",
        "outputId": "1439a1c3-fbb4-4ac5-f5da-e577edeaede2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'predict_audio_file' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4174768593.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_audio_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/smartearsaudio/sm/Healthy/1.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prediction: {result['prediction']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Confidence: {result['confidence']:.2%}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict_audio_file' is not defined"
          ]
        }
      ],
      "source": [
        "result = predict_audio_file('/content/smartearsaudio/sm/Healthy/1.wav')\n",
        "print(f\"Prediction: {result['prediction']}\")\n",
        "print(f\"Confidence: {result['confidence']:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTwjXecUyeKf",
        "outputId": "428a86ad-9e46-4a38-fbf4-5755fed84bb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Quick Model Test\n",
            "==================================================\n",
            "\n",
            "üéµ Testing: /content/smartearsaudio/sm/Healthy/1.wav\n",
            "ü§ñ Loading model from ./colab_gpu_model_output/best_model.h5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model loaded successfully!\n",
            "üéµ Processing audio: /content/smartearsaudio/sm/Healthy/1.wav\n",
            "üîÆ Making prediction...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéØ PREDICTION RESULTS:\n",
            "   File: 1.wav\n",
            "   Prediction: HEALTHY\n",
            "   Confidence: 61.05%\n",
            "   Probabilities:\n",
            "     Healthy: 61.05%\n",
            "     Sick: 38.95%\n",
            "\n",
            "üîç Testing on sample files from each class...\n",
            "\n",
            "--- HEALTHY SAMPLES ---\n",
            "üîç Testing model on files from: /content/smartearsaudio/sm/Healthy\n",
            "üìÅ Found 3 audio files to test\n",
            "\n",
            "--- Testing file 1/3 ---\n",
            "ü§ñ Loading model from ./colab_gpu_model_output/best_model.h5...\n",
            "‚úÖ Model loaded successfully!\n",
            "üéµ Processing audio: /content/smartearsaudio/sm/Healthy/rpi0016-20241126011900.wav\n",
            "üîÆ Making prediction...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéØ PREDICTION RESULTS:\n",
            "   File: rpi0016-20241126011900.wav\n",
            "   Prediction: HEALTHY\n",
            "   Confidence: 97.91%\n",
            "   Probabilities:\n",
            "     Healthy: 97.91%\n",
            "     Sick: 2.09%\n",
            "\n",
            "--- Testing file 2/3 ---\n",
            "ü§ñ Loading model from ./colab_gpu_model_output/best_model.h5...\n",
            "‚úÖ Model loaded successfully!\n",
            "üéµ Processing audio: /content/smartearsaudio/sm/Healthy/rpi0013-20240505220030.wav\n",
            "üîÆ Making prediction...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 155 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b08a007b100> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéØ PREDICTION RESULTS:\n",
            "   File: rpi0013-20240505220030.wav\n",
            "   Prediction: SICK\n",
            "   Confidence: 69.62%\n",
            "   Probabilities:\n",
            "     Healthy: 30.38%\n",
            "     Sick: 69.62%\n",
            "\n",
            "--- Testing file 3/3 ---\n",
            "ü§ñ Loading model from ./colab_gpu_model_output/best_model.h5...\n",
            "‚úÖ Model loaded successfully!\n",
            "üéµ Processing audio: /content/smartearsaudio/sm/Healthy/rpi0013-20240331100000.wav\n",
            "üîÆ Making prediction...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 156 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b088dfab600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéØ PREDICTION RESULTS:\n",
            "   File: rpi0013-20240331100000.wav\n",
            "   Prediction: HEALTHY\n",
            "   Confidence: 83.77%\n",
            "   Probabilities:\n",
            "     Healthy: 83.77%\n",
            "     Sick: 16.23%\n",
            "\n",
            "üìä SUMMARY OF 3 PREDICTIONS:\n",
            "   Healthy: 2\n",
            "   Sick: 1\n",
            "   Errors: 0\n",
            "\n",
            "--- SICK SAMPLES ---\n",
            "üîç Testing model on files from: /content/smartearsaudio/sm/Sick\n",
            "üìÅ Found 3 audio files to test\n",
            "\n",
            "--- Testing file 1/3 ---\n",
            "ü§ñ Loading model from ./colab_gpu_model_output/best_model.h5...\n",
            "‚úÖ Model loaded successfully!\n",
            "üéµ Processing audio: /content/smartearsaudio/sm/Sick/rpi0106-20240406141200-0.wav\n",
            "üîÆ Making prediction...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéØ PREDICTION RESULTS:\n",
            "   File: rpi0106-20240406141200-0.wav\n",
            "   Prediction: SICK\n",
            "   Confidence: 70.19%\n",
            "   Probabilities:\n",
            "     Healthy: 29.81%\n",
            "     Sick: 70.19%\n",
            "\n",
            "--- Testing file 2/3 ---\n",
            "ü§ñ Loading model from ./colab_gpu_model_output/best_model.h5...\n",
            "‚úÖ Model loaded successfully!\n",
            "üéµ Processing audio: /content/smartearsaudio/sm/Sick/rpi0012-20240306222330.wav\n",
            "üîÆ Making prediction...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéØ PREDICTION RESULTS:\n",
            "   File: rpi0012-20240306222330.wav\n",
            "   Prediction: HEALTHY\n",
            "   Confidence: 50.53%\n",
            "   Probabilities:\n",
            "     Healthy: 50.53%\n",
            "     Sick: 49.47%\n",
            "\n",
            "--- Testing file 3/3 ---\n",
            "ü§ñ Loading model from ./colab_gpu_model_output/best_model.h5...\n",
            "‚úÖ Model loaded successfully!\n",
            "üéµ Processing audio: /content/smartearsaudio/sm/Sick/20.wav\n",
            "üîÆ Making prediction...\n",
            "\n",
            "üéØ PREDICTION RESULTS:\n",
            "   File: 20.wav\n",
            "   Prediction: SICK\n",
            "   Confidence: 93.52%\n",
            "   Probabilities:\n",
            "     Healthy: 6.48%\n",
            "     Sick: 93.52%\n",
            "\n",
            "üìä SUMMARY OF 3 PREDICTIONS:\n",
            "   Healthy: 1\n",
            "   Sick: 2\n",
            "   Errors: 0\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Quick Model Testing Script\n",
        "Test your trained Colab GPU model on new audio files\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pathlib import Path\n",
        "\n",
        "def predict_audio_file(audio_path, model_path='./colab_gpu_model_output/best_model.h5'):\n",
        "    \"\"\"\n",
        "    Predict chicken sound classification for a single audio file\n",
        "\n",
        "    Args:\n",
        "        audio_path: Path to the audio file to classify\n",
        "        model_path: Path to the trained model\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with prediction results\n",
        "    \"\"\"\n",
        "\n",
        "    # Configuration (same as training)\n",
        "    SAMPLE_RATE = 16000\n",
        "    DURATION = 3.0\n",
        "    MAX_LENGTH = int(SAMPLE_RATE * DURATION)\n",
        "\n",
        "    try:\n",
        "        # Load the trained model\n",
        "        print(f\"ü§ñ Loading model from {model_path}...\")\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "        # Load and preprocess audio\n",
        "        print(f\"üéµ Processing audio: {audio_path}\")\n",
        "\n",
        "        # Load audio file\n",
        "        try:\n",
        "            waveform, sr = librosa.load(audio_path, sr=SAMPLE_RATE, mono=True)\n",
        "        except Exception as e:\n",
        "            return {'error': f\"Could not load audio file: {e}\"}\n",
        "\n",
        "        # Handle length - pad or truncate to 3 seconds\n",
        "        if len(waveform) > MAX_LENGTH:\n",
        "            # Take center portion\n",
        "            start = (len(waveform) - MAX_LENGTH) // 2\n",
        "            waveform = waveform[start:start + MAX_LENGTH]\n",
        "        elif len(waveform) < MAX_LENGTH:\n",
        "            # Pad with zeros\n",
        "            waveform = np.pad(waveform, (0, MAX_LENGTH - len(waveform)), 'constant')\n",
        "\n",
        "        # Normalize audio\n",
        "        if np.max(np.abs(waveform)) > 0:\n",
        "            waveform = waveform / np.max(np.abs(waveform)) * 0.8\n",
        "\n",
        "        # Extract mel spectrogram features (same as training)\n",
        "        mel_spec = librosa.feature.melspectrogram(\n",
        "            y=waveform,\n",
        "            sr=SAMPLE_RATE,\n",
        "            n_mels=80,\n",
        "            hop_length=512,\n",
        "            n_fft=1024\n",
        "        )\n",
        "\n",
        "        # Convert to log scale\n",
        "        log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        # Flatten and pad/truncate to fixed size\n",
        "        features = log_mel.flatten()\n",
        "        target_size = 800  # Same as training\n",
        "\n",
        "        if len(features) > target_size:\n",
        "            features = features[:target_size]\n",
        "        else:\n",
        "            features = np.pad(features, (0, target_size - len(features)), 'constant')\n",
        "\n",
        "        # Prepare for prediction\n",
        "        features = features.astype(np.float32)\n",
        "        features = np.expand_dims(features, axis=0)  # Add batch dimension\n",
        "\n",
        "        # Make prediction\n",
        "        print(\"üîÆ Making prediction...\")\n",
        "        predictions = model.predict(features, verbose=0)\n",
        "\n",
        "        # Get probabilities and predicted class\n",
        "        probabilities = predictions[0]\n",
        "        predicted_class = np.argmax(probabilities)\n",
        "        confidence = probabilities[predicted_class]\n",
        "\n",
        "        # Class names\n",
        "        class_names = ['healthy', 'sick']\n",
        "        predicted_label = class_names[predicted_class]\n",
        "\n",
        "        # Prepare results\n",
        "        result = {\n",
        "            'audio_file': audio_path,\n",
        "            'prediction': predicted_label,\n",
        "            'predicted_class': int(predicted_class),\n",
        "            'confidence': float(confidence),\n",
        "            'probabilities': {\n",
        "                'healthy': float(probabilities[0]),\n",
        "                'sick': float(probabilities[1])\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Display results\n",
        "        print(f\"\\nüéØ PREDICTION RESULTS:\")\n",
        "        print(f\"   File: {os.path.basename(audio_path)}\")\n",
        "        print(f\"   Prediction: {predicted_label.upper()}\")\n",
        "        print(f\"   Confidence: {confidence:.2%}\")\n",
        "        print(f\"   Probabilities:\")\n",
        "        print(f\"     Healthy: {probabilities[0]:.2%}\")\n",
        "        print(f\"     Sick: {probabilities[1]:.2%}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Prediction failed: {e}\"\n",
        "        print(f\"‚ùå {error_msg}\")\n",
        "        return {'error': error_msg}\n",
        "\n",
        "\n",
        "def test_multiple_files(directory_path, model_path='./colab_gpu_model_output/best_model.h5', limit=10):\n",
        "    \"\"\"\n",
        "    Test the model on multiple audio files from a directory\n",
        "\n",
        "    Args:\n",
        "        directory_path: Path to directory containing audio files\n",
        "        model_path: Path to the trained model\n",
        "        limit: Maximum number of files to test\n",
        "\n",
        "    Returns:\n",
        "        List of prediction results\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"üîç Testing model on files from: {directory_path}\")\n",
        "\n",
        "    # Find audio files\n",
        "    audio_extensions = ['.wav', '.mp3', '.flac', '.m4a']\n",
        "    audio_files = []\n",
        "\n",
        "    for ext in audio_extensions:\n",
        "        audio_files.extend(Path(directory_path).glob(f'*{ext}'))\n",
        "\n",
        "    if not audio_files:\n",
        "        print(f\"‚ùå No audio files found in {directory_path}\")\n",
        "        return []\n",
        "\n",
        "    # Limit number of files\n",
        "    audio_files = audio_files[:limit]\n",
        "    print(f\"üìÅ Found {len(audio_files)} audio files to test\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, audio_file in enumerate(audio_files, 1):\n",
        "        print(f\"\\n--- Testing file {i}/{len(audio_files)} ---\")\n",
        "        result = predict_audio_file(str(audio_file), model_path)\n",
        "        results.append(result)\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\nüìä SUMMARY OF {len(results)} PREDICTIONS:\")\n",
        "    healthy_count = sum(1 for r in results if r.get('prediction') == 'healthy')\n",
        "    sick_count = sum(1 for r in results if r.get('prediction') == 'sick')\n",
        "    error_count = sum(1 for r in results if 'error' in r)\n",
        "\n",
        "    print(f\"   Healthy: {healthy_count}\")\n",
        "    print(f\"   Sick: {sick_count}\")\n",
        "    print(f\"   Errors: {error_count}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def quick_test():\n",
        "    \"\"\"Quick test function with sample files\"\"\"\n",
        "\n",
        "    # Test paths\n",
        "    test_paths = [\n",
        "        '/content/smartearsaudio/sm/Healthy/1.wav',\n",
        "        '/content/smartearsaudio/sm/Sick/1.wav',\n",
        "        '/content/smartearsaudio/sm/Healthy/rpi0008-20240428003530.wav',\n",
        "        '/content/smartearsaudio/sm/Sick/rpi0007-20240801222530.wav'\n",
        "    ]\n",
        "\n",
        "    print(\"üöÄ Quick Model Test\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for test_path in test_paths:\n",
        "        if os.path.exists(test_path):\n",
        "            print(f\"\\nüéµ Testing: {test_path}\")\n",
        "            result = predict_audio_file(test_path)\n",
        "            break\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è File not found: {test_path}\")\n",
        "\n",
        "    # Test on a few files from each directory\n",
        "    print(f\"\\nüîç Testing on sample files from each class...\")\n",
        "\n",
        "    healthy_dir = '/content/smartearsaudio/sm/Healthy'\n",
        "    sick_dir = '/content/smartearsaudio/sm/Sick'\n",
        "\n",
        "    if os.path.exists(healthy_dir):\n",
        "        print(f\"\\n--- HEALTHY SAMPLES ---\")\n",
        "        test_multiple_files(healthy_dir, limit=3)\n",
        "\n",
        "    if os.path.exists(sick_dir):\n",
        "        print(f\"\\n--- SICK SAMPLES ---\")\n",
        "        test_multiple_files(sick_dir, limit=3)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    quick_test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86RksnUyzcy-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1da1e1b377bc47378e28396ec4b7876a",
            "37cbfd4352c849828e34a100dc5128a2",
            "af20e0a6914f4f6ea7e4d90c962d8176",
            "7a32b19d67904fc7899d1bc2a7d0e0e4",
            "1b9646f6da58422387286060c906abee",
            "93999d60a6774ad0890adcb2ed55d261",
            "5478e133b5fc4baeb5bb93a8a9bfdf94",
            "f1569e55917c4092905b2a8fb129a09d",
            "f732a86f455a4490b02c5760f9acd6d2",
            "e679bdb9a0874acabb39b050df1b4022",
            "f26029b67a3b4291a22be85aadbfde8c",
            "165cc6f915fa41f69bffc066dd04a14c",
            "f6010f9693344d3db0deb390bed42d3e",
            "80ec78492b924209b2a005c88055e527",
            "2246c9d7bad24a3eaaea6a68f6604fdf",
            "1ffdd112d13c42c995dbc709c5f6cba1",
            "5ae3752dd8a34787aba3336a6fc50200",
            "eca04068842846aca0e8d5b3047e0418",
            "9a319ab143aa4fbb85ad7c07752e5e4d",
            "0407b5c65ae542d3bccd37b35c28d1cf",
            "163d26ba92e24d29970a6098029249e7",
            "09fa54cb218b47008475fa3709034917",
            "891176dfbc5b4addb3f9906372e23130",
            "5c1019d0928c4a839f53314a5afca5be",
            "2774fe3e3e9e49dfb1aab5102c2dc086",
            "7a0b53deff16422b818c70acb13a28a1",
            "919dac3d94fa4f00a9db9f48dd42e214",
            "2fe4cbe71a44414f9eaa690cc0ffd295",
            "e493d52515f1486aa17ea125ca4848db",
            "bcebc5e8aec94ee8bd2bbf76a6fdd20e",
            "a8a2860485c54fc385bfd9dfa1bf6c00",
            "e44c47b87a0d4bfb82f6adb6374145be",
            "aea948ec609b40cbb8abaaeee5670709"
          ]
        },
        "id": "dRWV780Ugt9U",
        "outputId": "233b7eb7-b403-4416-9900-e67d787480f6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1da1e1b377bc47378e28396ec4b7876a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating healthy:   0%|          | 0/2139 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "165cc6f915fa41f69bffc066dd04a14c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating sick:   0%|          | 0/2121 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "891176dfbc5b4addb3f9906372e23130",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validating noise: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:GPU configuration warning: Physical devices cannot be modified after being initialized\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "    353/Unknown \u001b[1m192s\u001b[0m 536ms/step - accuracy: 0.5475 - loss: 0.1125\n",
            "Epoch 1: val_accuracy improved from -inf to 0.61983, saving model to ./robust_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 663ms/step - accuracy: 0.5476 - loss: 0.1125 - val_accuracy: 0.6198 - val_loss: 0.0645 - learning_rate: 1.0000e-05\n",
            "Epoch 2/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527ms/step - accuracy: 0.6468 - loss: 0.0774\n",
            "Epoch 2: val_accuracy improved from 0.61983 to 0.64959, saving model to ./robust_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 654ms/step - accuracy: 0.6468 - loss: 0.0774 - val_accuracy: 0.6496 - val_loss: 0.0615 - learning_rate: 1.0000e-05\n",
            "Epoch 3/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526ms/step - accuracy: 0.7229 - loss: 0.0649\n",
            "Epoch 3: val_accuracy improved from 0.64959 to 0.66116, saving model to ./robust_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 649ms/step - accuracy: 0.7229 - loss: 0.0648 - val_accuracy: 0.6612 - val_loss: 0.0601 - learning_rate: 1.0000e-05\n",
            "Epoch 4/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527ms/step - accuracy: 0.7604 - loss: 0.0505\n",
            "Epoch 4: val_accuracy improved from 0.66116 to 0.67107, saving model to ./robust_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 850ms/step - accuracy: 0.7604 - loss: 0.0505 - val_accuracy: 0.6711 - val_loss: 0.0656 - learning_rate: 1.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529ms/step - accuracy: 0.7760 - loss: 0.0457\n",
            "Epoch 5: val_accuracy improved from 0.67107 to 0.69256, saving model to ./robust_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 707ms/step - accuracy: 0.7760 - loss: 0.0457 - val_accuracy: 0.6926 - val_loss: 0.0620 - learning_rate: 1.0000e-05\n",
            "Epoch 6/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530ms/step - accuracy: 0.7934 - loss: 0.0462\n",
            "Epoch 6: val_accuracy improved from 0.69256 to 0.69752, saving model to ./robust_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 643ms/step - accuracy: 0.7934 - loss: 0.0462 - val_accuracy: 0.6975 - val_loss: 0.0644 - learning_rate: 1.0000e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 538ms/step - accuracy: 0.8244 - loss: 0.0411\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
            "\n",
            "Epoch 7: val_accuracy improved from 0.69752 to 0.70083, saving model to ./robust_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 657ms/step - accuracy: 0.8244 - loss: 0.0411 - val_accuracy: 0.7008 - val_loss: 0.0676 - learning_rate: 1.0000e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531ms/step - accuracy: 0.8446 - loss: 0.0375\n",
            "Epoch 8: val_accuracy improved from 0.70083 to 0.72231, saving model to ./robust_model_output/best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 644ms/step - accuracy: 0.8446 - loss: 0.0375 - val_accuracy: 0.7223 - val_loss: 0.0651 - learning_rate: 5.0000e-06\n",
            "Epoch 9/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528ms/step - accuracy: 0.8319 - loss: 0.0359\n",
            "Epoch 9: val_accuracy did not improve from 0.72231\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 637ms/step - accuracy: 0.8319 - loss: 0.0359 - val_accuracy: 0.7140 - val_loss: 0.0628 - learning_rate: 5.0000e-06\n",
            "Epoch 10/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527ms/step - accuracy: 0.8594 - loss: 0.0306\n",
            "Epoch 10: val_accuracy did not improve from 0.72231\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 742ms/step - accuracy: 0.8593 - loss: 0.0306 - val_accuracy: 0.7091 - val_loss: 0.0645 - learning_rate: 5.0000e-06\n",
            "Epoch 11/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530ms/step - accuracy: 0.8417 - loss: 0.0370\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n",
            "\n",
            "Epoch 11: val_accuracy did not improve from 0.72231\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 744ms/step - accuracy: 0.8417 - loss: 0.0370 - val_accuracy: 0.7174 - val_loss: 0.0601 - learning_rate: 5.0000e-06\n",
            "Epoch 12/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529ms/step - accuracy: 0.8595 - loss: 0.0315\n",
            "Epoch 12: val_accuracy did not improve from 0.72231\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 744ms/step - accuracy: 0.8595 - loss: 0.0315 - val_accuracy: 0.7174 - val_loss: 0.0596 - learning_rate: 2.5000e-06\n",
            "Epoch 13/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531ms/step - accuracy: 0.8629 - loss: 0.0283\n",
            "Epoch 13: val_accuracy did not improve from 0.72231\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 743ms/step - accuracy: 0.8629 - loss: 0.0283 - val_accuracy: 0.7074 - val_loss: 0.0598 - learning_rate: 2.5000e-06\n",
            "Epoch 14/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 529ms/step - accuracy: 0.8535 - loss: 0.0313\n",
            "Epoch 14: val_accuracy did not improve from 0.72231\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 638ms/step - accuracy: 0.8535 - loss: 0.0313 - val_accuracy: 0.7190 - val_loss: 0.0593 - learning_rate: 2.5000e-06\n",
            "Epoch 15/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530ms/step - accuracy: 0.8690 - loss: 0.0294\n",
            "Epoch 15: val_accuracy did not improve from 0.72231\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 808ms/step - accuracy: 0.8690 - loss: 0.0294 - val_accuracy: 0.7124 - val_loss: 0.0580 - learning_rate: 2.5000e-06\n",
            "Epoch 16/20\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531ms/step - accuracy: 0.8692 - loss: 0.0290\n",
            "Epoch 16: val_accuracy did not improve from 0.72231\n",
            "\u001b[1m353/353\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 743ms/step - accuracy: 0.8692 - loss: 0.0290 - val_accuracy: 0.7107 - val_loss: 0.0584 - learning_rate: 2.5000e-06\n",
            "Epoch 16: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 511ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     healthy       0.69      0.75      0.72       303\n",
            "        sick       0.73      0.67      0.70       303\n",
            "\n",
            "    accuracy                           0.71       606\n",
            "   macro avg       0.71      0.71      0.71       606\n",
            "weighted avg       0.71      0.71      0.71       606\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Robust Wav2Vec2 Audio Classification Trainer - Fixed Version\n",
        "Addresses bias, NaN losses, and compatibility issues\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import warnings\n",
        "import json\n",
        "import datetime\n",
        "from typing import List, Tuple, Dict, Optional, Union\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from collections import Counter\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class RobustConfig:\n",
        "    \"\"\"Robust configuration with bias prevention measures\"\"\"\n",
        "\n",
        "    # Model configuration\n",
        "    MODEL_ID: str = \"facebook/wav2vec2-base-960h\"\n",
        "    SAMPLE_RATE: int = 16000\n",
        "    DURATION: float = 3.0  # Reduced for better memory management\n",
        "    MAX_LENGTH: int = 48000  # 3 seconds at 16kHz\n",
        "\n",
        "    # Training configuration - Conservative settings to prevent NaN\n",
        "    BATCH_SIZE: int = 8  # Smaller batch size for stability\n",
        "    EPOCHS: int = 20\n",
        "    LEARNING_RATE: float = 1e-5  # Much lower learning rate\n",
        "    WARMUP_RATIO: float = 0.1\n",
        "    WEIGHT_DECAY: float = 0.01\n",
        "    GRADIENT_CLIP_NORM: float = 1.0  # Gradient clipping to prevent explosion\n",
        "\n",
        "    # Data configuration\n",
        "    TRAIN_SPLIT: float = 0.7\n",
        "    VAL_SPLIT: float = 0.15\n",
        "    TEST_SPLIT: float = 0.15\n",
        "\n",
        "    # Bias prevention configuration\n",
        "    MAX_CLASS_IMBALANCE_RATIO: float = 3.0  # Max ratio between largest and smallest class\n",
        "    MIN_SAMPLES_PER_CLASS: int = 50  # Minimum samples required per class\n",
        "    SYNTHETIC_NOISE_RATIO: float = 0.3  # Max 30% synthetic noise in dataset\n",
        "\n",
        "    # Audio validation configuration\n",
        "    MIN_AUDIO_LENGTH: float = 1.0  # Minimum 1 second\n",
        "    MAX_AUDIO_LENGTH: float = 10.0  # Maximum 10 seconds\n",
        "    MIN_SNR_DB: float = 5.0  # Minimum signal-to-noise ratio\n",
        "\n",
        "    # Augmentation configuration - Reduced to prevent overfitting\n",
        "    USE_AUGMENTATION: bool = True\n",
        "    AUGMENTATION_PROB: float = 0.3  # Reduced probability\n",
        "\n",
        "    # Directory configuration\n",
        "    BASE_DIR: str = './data'\n",
        "    HEALTHY_DIR: str = './data/healthy'\n",
        "    SICK_DIR: str = './data/sick'\n",
        "    NOISE_DIR: str = './data/noise'\n",
        "    OUTPUT_DIR: str = './robust_model_output'\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration\"\"\"\n",
        "        assert self.TRAIN_SPLIT + self.VAL_SPLIT + self.TEST_SPLIT == 1.0\n",
        "        assert self.MAX_CLASS_IMBALANCE_RATIO >= 1.0\n",
        "        assert self.MIN_SAMPLES_PER_CLASS > 0\n",
        "        assert 0 < self.SYNTHETIC_NOISE_RATIO < 1.0\n",
        "\n",
        "\n",
        "class AudioValidator:\n",
        "    \"\"\"Robust audio validation to prevent NaN issues\"\"\"\n",
        "\n",
        "    def __init__(self, config: RobustConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def validate_audio_file(self, file_path: str) -> Tuple[bool, str]:\n",
        "        \"\"\"Comprehensive audio file validation\"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(file_path):\n",
        "                return False, \"File does not exist\"\n",
        "\n",
        "            if os.path.getsize(file_path) < 1000:  # Less than 1KB\n",
        "                return False, \"File too small\"\n",
        "\n",
        "            # Load audio and check basic properties\n",
        "            try:\n",
        "                waveform, sr = librosa.load(file_path, sr=None, mono=True)\n",
        "            except Exception as e:\n",
        "                return False, f\"Cannot load audio: {e}\"\n",
        "\n",
        "            # Check duration\n",
        "            duration = len(waveform) / sr\n",
        "            if duration < self.config.MIN_AUDIO_LENGTH:\n",
        "                return False, f\"Audio too short: {duration:.2f}s\"\n",
        "            if duration > self.config.MAX_AUDIO_LENGTH:\n",
        "                return False, f\"Audio too long: {duration:.2f}s\"\n",
        "\n",
        "            # Check for silence or constant values\n",
        "            if np.std(waveform) < 1e-6:\n",
        "                return False, \"Audio is silent or constant\"\n",
        "\n",
        "            # Check for clipping\n",
        "            clipping_ratio = np.mean(np.abs(waveform) > 0.95)\n",
        "            if clipping_ratio > 0.1:\n",
        "                return False, f\"Audio heavily clipped: {clipping_ratio:.2%}\"\n",
        "\n",
        "            # Check signal-to-noise ratio (rough estimate)\n",
        "            signal_power = np.mean(waveform ** 2)\n",
        "            if signal_power < 1e-8:\n",
        "                return False, \"Signal power too low\"\n",
        "\n",
        "            # Check for NaN or infinite values\n",
        "            if not np.isfinite(waveform).all():\n",
        "                return False, \"Audio contains NaN or infinite values\"\n",
        "\n",
        "            return True, \"Valid\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, f\"Validation error: {e}\"\n",
        "\n",
        "\n",
        "class RobustAudioProcessor:\n",
        "    \"\"\"Robust audio processing with NaN prevention\"\"\"\n",
        "\n",
        "    def __init__(self, config: RobustConfig):\n",
        "        self.config = config\n",
        "        self.validator = AudioValidator(config)\n",
        "\n",
        "        # Initialize feature extractor\n",
        "        try:\n",
        "            from transformers import AutoFeatureExtractor\n",
        "            self.feature_extractor = AutoFeatureExtractor.from_pretrained(config.MODEL_ID)\n",
        "            logger.info(\"‚úÖ Feature extractor loaded successfully\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not load feature extractor: {e}\")\n",
        "            self.feature_extractor = None\n",
        "\n",
        "    def load_and_preprocess_audio(self, file_path: str) -> Optional[np.ndarray]:\n",
        "        \"\"\"Load and preprocess audio with robust error handling\"\"\"\n",
        "        try:\n",
        "            # Validate file first\n",
        "            is_valid, reason = self.validator.validate_audio_file(file_path)\n",
        "            if not is_valid:\n",
        "                logger.warning(f\"Invalid audio {file_path}: {reason}\")\n",
        "                return None\n",
        "\n",
        "            # Load audio with fallback resampling\n",
        "            try:\n",
        "                waveform, sr = librosa.load(\n",
        "                    file_path,\n",
        "                    sr=self.config.SAMPLE_RATE,\n",
        "                    mono=True,\n",
        "                    res_type='kaiser_fast'\n",
        "                )\n",
        "            except Exception as e:\n",
        "                if 'resampy' in str(e):\n",
        "                    # Fallback: load without resampling, then resample manually\n",
        "                    logger.debug(f\"Using fallback resampling for {file_path}\")\n",
        "                    waveform, original_sr = librosa.load(file_path, sr=None, mono=True)\n",
        "                    if original_sr != self.config.SAMPLE_RATE:\n",
        "                        waveform = self._manual_resample(waveform, original_sr, self.config.SAMPLE_RATE)\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "            # Handle length - intelligent cropping/padding\n",
        "            target_length = self.config.MAX_LENGTH\n",
        "\n",
        "            if len(waveform) > target_length:\n",
        "                # For longer audio, take the most energetic segment\n",
        "                hop_length = len(waveform) - target_length\n",
        "                if hop_length > 0:\n",
        "                    # Find the segment with highest energy\n",
        "                    energies = []\n",
        "                    for i in range(0, hop_length + 1, hop_length // 10 + 1):\n",
        "                        segment = waveform[i:i + target_length]\n",
        "                        energy = np.sum(segment ** 2)\n",
        "                        energies.append((energy, i))\n",
        "\n",
        "                    best_start = max(energies)[1]\n",
        "                    waveform = waveform[best_start:best_start + target_length]\n",
        "                else:\n",
        "                    waveform = waveform[:target_length]\n",
        "\n",
        "            elif len(waveform) < target_length:\n",
        "                # Pad with reflection to avoid discontinuities\n",
        "                pad_length = target_length - len(waveform)\n",
        "                if len(waveform) > pad_length:\n",
        "                    # Reflect the signal\n",
        "                    waveform = np.pad(waveform, (0, pad_length), 'reflect')\n",
        "                else:\n",
        "                    # Zero pad if signal is too short for reflection\n",
        "                    waveform = np.pad(waveform, (0, pad_length), 'constant')\n",
        "\n",
        "            # Robust normalization\n",
        "            waveform = self._robust_normalize(waveform)\n",
        "\n",
        "            # Final validation\n",
        "            if not np.isfinite(waveform).all():\n",
        "                logger.warning(f\"NaN/Inf detected in processed audio: {file_path}\")\n",
        "                return None\n",
        "\n",
        "            return waveform.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing {file_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _robust_normalize(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Robust normalization that prevents NaN\"\"\"\n",
        "        try:\n",
        "            # Remove DC offset\n",
        "            waveform = waveform - np.mean(waveform)\n",
        "\n",
        "            # Robust scaling using percentiles instead of max\n",
        "            p99 = np.percentile(np.abs(waveform), 99)\n",
        "            if p99 > 1e-8:\n",
        "                waveform = waveform / p99 * 0.8  # Scale to 80% to avoid clipping\n",
        "\n",
        "            # Clip to prevent extreme values\n",
        "            waveform = np.clip(waveform, -1.0, 1.0)\n",
        "\n",
        "            return waveform\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Normalization error: {e}\")\n",
        "            return np.zeros_like(waveform)\n",
        "\n",
        "    def apply_augmentation(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Conservative augmentation to prevent overfitting\"\"\"\n",
        "        if not self.config.USE_AUGMENTATION:\n",
        "            return waveform\n",
        "\n",
        "        try:\n",
        "            augmented = waveform.copy()\n",
        "\n",
        "            # Time shifting (small)\n",
        "            if np.random.random() < self.config.AUGMENTATION_PROB:\n",
        "                shift = int(np.random.uniform(-0.05, 0.05) * len(augmented))\n",
        "                augmented = np.roll(augmented, shift)\n",
        "\n",
        "            # Volume scaling (conservative)\n",
        "            if np.random.random() < self.config.AUGMENTATION_PROB:\n",
        "                scale = np.random.uniform(0.8, 1.2)\n",
        "                augmented = augmented * scale\n",
        "\n",
        "            # Add small amount of noise\n",
        "            if np.random.random() < self.config.AUGMENTATION_PROB * 0.5:\n",
        "                noise_level = np.random.uniform(0.001, 0.005)\n",
        "                noise = np.random.normal(0, noise_level, len(augmented))\n",
        "                augmented = augmented + noise\n",
        "\n",
        "            # Ensure no clipping or NaN\n",
        "            augmented = np.clip(augmented, -1.0, 1.0)\n",
        "\n",
        "            if not np.isfinite(augmented).all():\n",
        "                return waveform  # Return original if augmentation failed\n",
        "\n",
        "            return augmented\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Augmentation failed: {e}\")\n",
        "            return waveform\n",
        "\n",
        "    def extract_features(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Extract features using transformers or fallback to manual extraction\"\"\"\n",
        "        try:\n",
        "            if self.feature_extractor is not None:\n",
        "                # Use transformers feature extractor\n",
        "                inputs = self.feature_extractor(\n",
        "                    waveform,\n",
        "                    sampling_rate=self.config.SAMPLE_RATE,\n",
        "                    return_tensors=\"np\",\n",
        "                    padding=True,\n",
        "                    max_length=self.config.MAX_LENGTH,\n",
        "                    truncation=True\n",
        "                )\n",
        "                return inputs['input_values'][0]\n",
        "            else:\n",
        "                # Fallback to manual feature extraction\n",
        "                return self._manual_feature_extraction(waveform)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Feature extraction failed, using fallback: {e}\")\n",
        "            return self._manual_feature_extraction(waveform)\n",
        "\n",
        "    def _manual_resample(self, waveform: np.ndarray, orig_sr: int, target_sr: int) -> np.ndarray:\n",
        "        \"\"\"Manual resampling when resampy is not available\"\"\"\n",
        "        try:\n",
        "            from scipy import signal\n",
        "            # Calculate resampling ratio\n",
        "            ratio = target_sr / orig_sr\n",
        "\n",
        "            if ratio == 1.0:\n",
        "                return waveform\n",
        "\n",
        "            # Use scipy's resample for basic resampling\n",
        "            new_length = int(len(waveform) * ratio)\n",
        "            resampled = signal.resample(waveform, new_length)\n",
        "\n",
        "            return resampled.astype(np.float32)\n",
        "\n",
        "        except ImportError:\n",
        "            # If scipy is also not available, use simple linear interpolation\n",
        "            logger.warning(\"Using basic linear interpolation for resampling\")\n",
        "            ratio = target_sr / orig_sr\n",
        "            new_length = int(len(waveform) * ratio)\n",
        "\n",
        "            # Simple linear interpolation\n",
        "            old_indices = np.linspace(0, len(waveform) - 1, len(waveform))\n",
        "            new_indices = np.linspace(0, len(waveform) - 1, new_length)\n",
        "            resampled = np.interp(new_indices, old_indices, waveform)\n",
        "\n",
        "            return resampled.astype(np.float32)\n",
        "\n",
        "    def _manual_feature_extraction(self, waveform: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Manual feature extraction as fallback\"\"\"\n",
        "        try:\n",
        "            # Simple spectral features\n",
        "            # Compute mel spectrogram\n",
        "            mel_spec = librosa.feature.melspectrogram(\n",
        "                y=waveform,\n",
        "                sr=self.config.SAMPLE_RATE,\n",
        "                n_mels=80,\n",
        "                hop_length=512,\n",
        "                n_fft=1024\n",
        "            )\n",
        "\n",
        "            # Convert to log scale\n",
        "            log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "            # Flatten and pad/truncate to fixed size\n",
        "            features = log_mel.flatten()\n",
        "            target_size = 1000  # Fixed feature size\n",
        "\n",
        "            if len(features) > target_size:\n",
        "                features = features[:target_size]\n",
        "            else:\n",
        "                features = np.pad(features, (0, target_size - len(features)), 'constant')\n",
        "\n",
        "            return features.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Manual feature extraction failed: {e}\")\n",
        "            # Return zeros as last resort\n",
        "            return np.zeros(1000, dtype=np.float32)\n",
        "\n",
        "\n",
        "class BiasPreventionDatasetBuilder:\n",
        "    \"\"\"Dataset builder with bias prevention measures\"\"\"\n",
        "\n",
        "    def __init__(self, config: RobustConfig):\n",
        "        self.config = config\n",
        "        self.processor = RobustAudioProcessor(config)\n",
        "\n",
        "    def collect_and_balance_data(self) -> Tuple[List[str], List[int], List[str]]:\n",
        "        \"\"\"Collect data with bias prevention\"\"\"\n",
        "        logger.info(\"Collecting and balancing dataset...\")\n",
        "\n",
        "        # Collect files by category\n",
        "        healthy_files = self._collect_files(self.config.HEALTHY_DIR, \"healthy\")\n",
        "        sick_files = self._collect_files(self.config.SICK_DIR, \"sick\")\n",
        "        noise_files = self._collect_files(self.config.NOISE_DIR, \"noise\")\n",
        "\n",
        "        logger.info(f\"Raw file counts - Healthy: {len(healthy_files)}, Sick: {len(sick_files)}, Noise: {len(noise_files)}\")\n",
        "\n",
        "        # Validate files\n",
        "        healthy_files = self._validate_files(healthy_files, \"healthy\")\n",
        "        sick_files = self._validate_files(sick_files, \"sick\")\n",
        "        noise_files = self._validate_files(noise_files, \"noise\")\n",
        "\n",
        "        logger.info(f\"Valid file counts - Healthy: {len(healthy_files)}, Sick: {len(sick_files)}, Noise: {len(noise_files)}\")\n",
        "\n",
        "        # Check minimum requirements\n",
        "        if len(healthy_files) < self.config.MIN_SAMPLES_PER_CLASS:\n",
        "            raise ValueError(f\"Insufficient healthy samples: {len(healthy_files)} < {self.config.MIN_SAMPLES_PER_CLASS}\")\n",
        "        if len(sick_files) < self.config.MIN_SAMPLES_PER_CLASS:\n",
        "            raise ValueError(f\"Insufficient sick samples: {len(sick_files)} < {self.config.MIN_SAMPLES_PER_CLASS}\")\n",
        "\n",
        "        # Balance dataset to prevent bias\n",
        "        healthy_files, sick_files, noise_files = self._balance_classes(\n",
        "            healthy_files, sick_files, noise_files\n",
        "        )\n",
        "\n",
        "        # Determine class configuration\n",
        "        if len(noise_files) > 0:\n",
        "            all_files = healthy_files + sick_files + noise_files\n",
        "            all_labels = ([0] * len(healthy_files) +\n",
        "                         [1] * len(sick_files) +\n",
        "                         [2] * len(noise_files))\n",
        "            class_names = ['healthy', 'sick', 'noise']\n",
        "        else:\n",
        "            all_files = healthy_files + sick_files\n",
        "            all_labels = [0] * len(healthy_files) + [1] * len(sick_files)\n",
        "            class_names = ['healthy', 'sick']\n",
        "\n",
        "        logger.info(f\"Final balanced dataset - Classes: {class_names}\")\n",
        "        for i, name in enumerate(class_names):\n",
        "            count = sum(1 for label in all_labels if label == i)\n",
        "            logger.info(f\"  {name}: {count} samples\")\n",
        "\n",
        "        return all_files, all_labels, class_names\n",
        "\n",
        "    def _collect_files(self, directory: str, category: str) -> List[str]:\n",
        "        \"\"\"Collect audio files from directory\"\"\"\n",
        "        files = []\n",
        "        if os.path.exists(directory):\n",
        "            for ext in ['.wav', '.mp3', '.flac', '.m4a']:\n",
        "                files.extend(Path(directory).rglob(f'*{ext}'))\n",
        "\n",
        "        return [str(f) for f in files]\n",
        "\n",
        "    def _validate_files(self, files: List[str], category: str) -> List[str]:\n",
        "        \"\"\"Validate audio files\"\"\"\n",
        "        valid_files = []\n",
        "\n",
        "        logger.info(f\"Validating {len(files)} {category} files...\")\n",
        "\n",
        "        for file_path in tqdm(files, desc=f\"Validating {category}\"):\n",
        "            is_valid, reason = self.processor.validator.validate_audio_file(file_path)\n",
        "            if is_valid:\n",
        "                valid_files.append(file_path)\n",
        "            else:\n",
        "                logger.debug(f\"Rejected {file_path}: {reason}\")\n",
        "\n",
        "        rejection_rate = (len(files) - len(valid_files)) / len(files) if files else 0\n",
        "        logger.info(f\"{category} validation: {len(valid_files)}/{len(files)} valid ({rejection_rate:.1%} rejected)\")\n",
        "\n",
        "        return valid_files\n",
        "\n",
        "    def _balance_classes(self, healthy_files: List[str], sick_files: List[str],\n",
        "                        noise_files: List[str]) -> Tuple[List[str], List[str], List[str]]:\n",
        "        \"\"\"Balance classes to prevent bias\"\"\"\n",
        "\n",
        "        # Calculate target sizes\n",
        "        counts = [len(healthy_files), len(sick_files)]\n",
        "        if noise_files:\n",
        "            counts.append(len(noise_files))\n",
        "\n",
        "        min_count = min(counts)\n",
        "        max_count = max(counts)\n",
        "\n",
        "        # Check if balancing is needed\n",
        "        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
        "\n",
        "        if imbalance_ratio > self.config.MAX_CLASS_IMBALANCE_RATIO:\n",
        "            logger.info(f\"Balancing classes (ratio: {imbalance_ratio:.2f} > {self.config.MAX_CLASS_IMBALANCE_RATIO})\")\n",
        "\n",
        "            # Calculate target size (use median to avoid extreme reduction)\n",
        "            target_size = int(np.median(counts))\n",
        "\n",
        "            # Limit noise files to prevent overwhelming real data\n",
        "            if noise_files:\n",
        "                max_noise = int(target_size * self.config.SYNTHETIC_NOISE_RATIO / (1 - self.config.SYNTHETIC_NOISE_RATIO))\n",
        "                if len(noise_files) > max_noise:\n",
        "                    noise_files = np.random.choice(noise_files, max_noise, replace=False).tolist()\n",
        "                    logger.info(f\"Limited noise files to {max_noise}\")\n",
        "\n",
        "            # Balance healthy and sick files\n",
        "            if len(healthy_files) > target_size:\n",
        "                healthy_files = np.random.choice(healthy_files, target_size, replace=False).tolist()\n",
        "            if len(sick_files) > target_size:\n",
        "                sick_files = np.random.choice(sick_files, target_size, replace=False).tolist()\n",
        "\n",
        "        logger.info(f\"Balanced dataset - Healthy: {len(healthy_files)}, Sick: {len(sick_files)}, Noise: {len(noise_files)}\")\n",
        "\n",
        "        return healthy_files, sick_files, noise_files\n",
        "\n",
        "\n",
        "class SimpleRobustTrainer:\n",
        "    \"\"\"Simplified robust trainer with better compatibility\"\"\"\n",
        "\n",
        "    def __init__(self, config: RobustConfig):\n",
        "        self.config = config\n",
        "        self.processor = RobustAudioProcessor(config)\n",
        "\n",
        "        # Setup directories\n",
        "        os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "        # Configure TensorFlow for stability\n",
        "        self._configure_tensorflow()\n",
        "\n",
        "    def _configure_tensorflow(self):\n",
        "        \"\"\"Configure TensorFlow for stable training\"\"\"\n",
        "        # Disable mixed precision to avoid type issues\n",
        "        tf.keras.mixed_precision.set_global_policy('float32')\n",
        "\n",
        "        # Configure GPU memory growth\n",
        "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            try:\n",
        "                for gpu in gpus:\n",
        "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "                logger.info(\"‚úÖ GPU memory growth enabled\")\n",
        "            except RuntimeError as e:\n",
        "                logger.warning(f\"GPU configuration warning: {e}\")\n",
        "\n",
        "    def create_dataset(self, file_paths: List[str], labels: List[int],\n",
        "                      is_training: bool = True) -> tf.data.Dataset:\n",
        "        \"\"\"Create robust TensorFlow dataset\"\"\"\n",
        "\n",
        "        def load_audio_tf(file_path, label):\n",
        "            def load_and_process(path):\n",
        "                path_str = path.numpy().decode('utf-8')\n",
        "                waveform = self.processor.load_and_preprocess_audio(path_str)\n",
        "\n",
        "                if waveform is None:\n",
        "                    # Return zeros if loading failed\n",
        "                    waveform = np.zeros(self.config.MAX_LENGTH, dtype=np.float32)\n",
        "                else:\n",
        "                    # Apply augmentation for training\n",
        "                    if is_training:\n",
        "                        waveform = self.processor.apply_augmentation(waveform)\n",
        "\n",
        "                # Extract features\n",
        "                features = self.processor.extract_features(waveform)\n",
        "                return features\n",
        "\n",
        "            audio_data = tf.py_function(\n",
        "                load_and_process,\n",
        "                inp=[file_path],\n",
        "                Tout=tf.float32\n",
        "            )\n",
        "\n",
        "            # Set shape based on feature extraction method\n",
        "            if self.processor.feature_extractor is not None:\n",
        "                audio_data.set_shape([self.config.MAX_LENGTH])\n",
        "            else:\n",
        "                audio_data.set_shape([1000])  # Manual feature size\n",
        "\n",
        "            return audio_data, label\n",
        "\n",
        "        # Create dataset\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "\n",
        "        if is_training:\n",
        "            dataset = dataset.shuffle(buffer_size=min(1000, len(file_paths)))\n",
        "\n",
        "        dataset = dataset.map(\n",
        "            load_audio_tf,\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "\n",
        "        # Filter out any invalid samples\n",
        "        dataset = dataset.filter(\n",
        "            lambda audio, label: tf.reduce_all(tf.math.is_finite(audio))\n",
        "        )\n",
        "\n",
        "        dataset = dataset.batch(self.config.BATCH_SIZE)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def create_simple_model(self, num_classes: int, input_shape: Tuple[int]):\n",
        "        \"\"\"Create a simple, robust model\"\"\"\n",
        "        try:\n",
        "            model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Input(shape=input_shape),\n",
        "\n",
        "                # If using raw audio, add some conv layers\n",
        "                tf.keras.layers.Reshape((-1, 1)) if len(input_shape) == 1 and input_shape[0] > 1000 else tf.keras.layers.Lambda(lambda x: x),\n",
        "\n",
        "                # Dense layers for classification\n",
        "                tf.keras.layers.Flatten(),\n",
        "                tf.keras.layers.Dense(512, activation='relu'),\n",
        "                tf.keras.layers.Dropout(0.3),\n",
        "                tf.keras.layers.Dense(256, activation='relu'),\n",
        "                tf.keras.layers.Dropout(0.2),\n",
        "                tf.keras.layers.Dense(128, activation='relu'),\n",
        "                tf.keras.layers.Dropout(0.1),\n",
        "                tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "            ])\n",
        "\n",
        "            logger.info(\"‚úÖ Simple model created successfully\")\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Model creation failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def compile_model(self, model, class_weights: Dict[int, float]):\n",
        "        \"\"\"Compile model with robust settings\"\"\"\n",
        "\n",
        "        # Use a stable optimizer\n",
        "        optimizer = tf.keras.optimizers.Adam(\n",
        "            learning_rate=self.config.LEARNING_RATE,\n",
        "            beta_1=0.9,\n",
        "            beta_2=0.999,\n",
        "            epsilon=1e-8,\n",
        "            clipnorm=self.config.GRADIENT_CLIP_NORM\n",
        "        )\n",
        "\n",
        "        # Use focal loss to handle class imbalance with proper type handling\n",
        "        def focal_loss(alpha=0.25, gamma=2.0):\n",
        "            def focal_loss_fixed(y_true, y_pred):\n",
        "                epsilon = tf.keras.backend.epsilon()\n",
        "\n",
        "                # Ensure consistent dtypes - cast everything to float32\n",
        "                y_pred = tf.cast(y_pred, tf.float32)\n",
        "                y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
        "\n",
        "                # Convert to one-hot if needed and ensure float32\n",
        "                if len(y_true.shape) == 1:\n",
        "                    y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=tf.shape(y_pred)[1])\n",
        "                y_true = tf.cast(y_true, tf.float32)\n",
        "\n",
        "                # Calculate focal loss with consistent types\n",
        "                ce = -y_true * tf.math.log(y_pred)\n",
        "                weight = tf.cast(alpha, tf.float32) * y_true * tf.pow((1 - y_pred), tf.cast(gamma, tf.float32))\n",
        "                fl = weight * ce\n",
        "\n",
        "                return tf.reduce_mean(tf.reduce_sum(fl, axis=1))\n",
        "\n",
        "            return focal_loss_fixed\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=focal_loss(),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def create_callbacks(self):\n",
        "        \"\"\"Create training callbacks for stability\"\"\"\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=8,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=4,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                os.path.join(self.config.OUTPUT_DIR, 'best_model.h5'),\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            # NaN termination callback\n",
        "            tf.keras.callbacks.TerminateOnNaN(),\n",
        "            # Custom callback for monitoring\n",
        "            RobustTrainingCallback()\n",
        "        ]\n",
        "\n",
        "        return callbacks\n",
        "\n",
        "    def train(self, train_dataset, val_dataset, class_weights, num_classes):\n",
        "        \"\"\"Train model with robust settings\"\"\"\n",
        "\n",
        "        logger.info(\"Creating and compiling model...\")\n",
        "\n",
        "        # Determine input shape from dataset\n",
        "        sample_batch = next(iter(train_dataset))\n",
        "        input_shape = sample_batch[0].shape[1:]\n",
        "        logger.info(f\"Input shape: {input_shape}\")\n",
        "\n",
        "        model = self.create_simple_model(num_classes, input_shape)\n",
        "        model = self.compile_model(model, class_weights)\n",
        "\n",
        "        logger.info(\"Model architecture:\")\n",
        "        model.summary(print_fn=logger.info)\n",
        "\n",
        "        callbacks = self.create_callbacks()\n",
        "\n",
        "        logger.info(f\"Starting training for {self.config.EPOCHS} epochs...\")\n",
        "\n",
        "        try:\n",
        "            history = model.fit(\n",
        "                train_dataset,\n",
        "                epochs=self.config.EPOCHS,\n",
        "                validation_data=val_dataset,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1,\n",
        "                class_weight=class_weights\n",
        "            )\n",
        "\n",
        "            logger.info(\"Training completed successfully!\")\n",
        "            return model, history\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Training failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def evaluate_model(self, model, test_dataset, class_names):\n",
        "        \"\"\"Evaluate model with comprehensive metrics\"\"\"\n",
        "        logger.info(\"Evaluating model...\")\n",
        "\n",
        "        try:\n",
        "            # Get predictions\n",
        "            predictions = model.predict(test_dataset, verbose=1)\n",
        "\n",
        "            # Extract true labels\n",
        "            y_true = []\n",
        "            for _, labels in test_dataset:\n",
        "                y_true.extend(labels.numpy())\n",
        "            y_true = np.array(y_true)\n",
        "\n",
        "            y_pred = np.argmax(predictions, axis=1)\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "            logger.info(f\"Balanced Accuracy: {accuracy:.4f}\")\n",
        "            logger.info(\"\\nClassification Report:\")\n",
        "            print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "            # Confusion matrix\n",
        "            cm = confusion_matrix(y_true, y_pred)\n",
        "            self._plot_confusion_matrix(cm, class_names)\n",
        "\n",
        "            return {\n",
        "                'accuracy': accuracy,\n",
        "                'y_true': y_true,\n",
        "                'y_pred': y_pred,\n",
        "                'predictions': predictions,\n",
        "                'confusion_matrix': cm\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Evaluation failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _plot_confusion_matrix(self, cm, class_names):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.config.OUTPUT_DIR, 'confusion_matrix.png'))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class RobustTrainingCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Custom callback for monitoring training stability\"\"\"\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is None:\n",
        "            logs = {}\n",
        "\n",
        "        # Check for NaN or extreme values\n",
        "        for key, value in logs.items():\n",
        "            if not np.isfinite(value):\n",
        "                logger.error(f\"NaN/Inf detected in {key}: {value}\")\n",
        "                self.model.stop_training = True\n",
        "                return\n",
        "\n",
        "        # Log progress\n",
        "        logger.info(f\"Epoch {epoch + 1}: \"\n",
        "                   f\"loss={logs.get('loss', 0):.4f}, \"\n",
        "                   f\"acc={logs.get('accuracy', 0):.4f}, \"\n",
        "                   f\"val_loss={logs.get('val_loss', 0):.4f}, \"\n",
        "                   f\"val_acc={logs.get('val_accuracy', 0):.4f}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "    logger.info(\"Starting robust Wav2Vec2 training pipeline...\")\n",
        "\n",
        "    # Configuration\n",
        "    config = RobustConfig()\n",
        "\n",
        "    config.BASE_DIR = '/content/smartearsaudio/sm'\n",
        "    config.HEALTHY_DIR = '/content/smartearsaudio/sm/Healthy'\n",
        "    config.SICK_DIR = '/content/smartearsaudio/sm/Sick'\n",
        "\n",
        "    try:\n",
        "        # Build dataset\n",
        "        dataset_builder = BiasPreventionDatasetBuilder(config)\n",
        "        all_files, all_labels, class_names = dataset_builder.collect_and_balance_data()\n",
        "\n",
        "        if len(all_files) == 0:\n",
        "            raise ValueError(\"No valid audio files found!\")\n",
        "\n",
        "        # Split data\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            all_files, all_labels,\n",
        "            test_size=config.TEST_SPLIT,\n",
        "            random_state=42,\n",
        "            stratify=all_labels\n",
        "        )\n",
        "\n",
        "        val_size = config.VAL_SPLIT / (1 - config.TEST_SPLIT)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp,\n",
        "            test_size=val_size,\n",
        "            random_state=42,\n",
        "            stratify=y_temp\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Data split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
        "\n",
        "        # Calculate class weights\n",
        "        class_weights = compute_class_weight(\n",
        "            'balanced',\n",
        "            classes=np.unique(y_train),\n",
        "            y=y_train\n",
        "        )\n",
        "        class_weight_dict = dict(enumerate(class_weights))\n",
        "        logger.info(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "        # Create trainer\n",
        "        trainer = SimpleRobustTrainer(config)\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = trainer.create_dataset(X_train, y_train, is_training=True)\n",
        "        val_dataset = trainer.create_dataset(X_val, y_val, is_training=False)\n",
        "        test_dataset = trainer.create_dataset(X_test, y_test, is_training=False)\n",
        "\n",
        "        # Train model\n",
        "        model, history = trainer.train(\n",
        "            train_dataset, val_dataset, class_weight_dict, len(class_names)\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        results = trainer.evaluate_model(model, test_dataset, class_names)\n",
        "\n",
        "        # Save model and results\n",
        "        model.save(os.path.join(config.OUTPUT_DIR, 'final_model.h5'))\n",
        "\n",
        "        with open(os.path.join(config.OUTPUT_DIR, 'training_results.json'), 'w') as f:\n",
        "            json.dump({\n",
        "                'class_names': class_names,\n",
        "                'accuracy': float(results.get('accuracy', 0)),\n",
        "                'class_weights': {str(k): float(v) for k, v in class_weight_dict.items()},\n",
        "                'config': config.__dict__\n",
        "            }, f, indent=2)\n",
        "\n",
        "        logger.info(f\"Training completed! Results saved to {config.OUTPUT_DIR}\")\n",
        "\n",
        "        return model, results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Training pipeline failed: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, results = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ERR_HXv5QsR"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Proper Wav2Vec2 Audio Classification Trainer\n",
        "Real Wav2Vec2 implementation with bias prevention and robust training\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import gc\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss to address class imbalance and bias\"\"\"\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "class RobustChickenDataset(Dataset):\n",
        "    \"\"\"Robust dataset with comprehensive audio validation and augmentation\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, processor, max_length=48000, augment=True, balance_classes=True):\n",
        "        self.root_dir = root_dir\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.augment = augment\n",
        "        self.files = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load and validate all audio files\n",
        "        self._load_and_validate_files()\n",
        "\n",
        "        # Balance classes if requested\n",
        "        if balance_classes:\n",
        "            self._balance_classes()\n",
        "\n",
        "        logger.info(f\"Dataset loaded: {len(self.files)} files\")\n",
        "        self._print_class_distribution()\n",
        "\n",
        "    def _load_and_validate_files(self):\n",
        "        \"\"\"Load and validate all audio files\"\"\"\n",
        "        for label, folder in enumerate(['Healthy', 'Sick']):\n",
        "            folder_path = os.path.join(self.root_dir, folder)\n",
        "            if not os.path.exists(folder_path):\n",
        "                logger.warning(f\"Folder not found: {folder_path}\")\n",
        "                continue\n",
        "\n",
        "            for file in os.listdir(folder_path):\n",
        "                if file.endswith(('.wav', '.mp3', '.flac', '.m4a', '.aac', '.ogg')):\n",
        "                    file_path = os.path.join(folder_path, file)\n",
        "\n",
        "                    # Validate audio file\n",
        "                    if self._validate_audio_file(file_path):\n",
        "                        self.files.append(file_path)\n",
        "                        self.labels.append(label)\n",
        "                    else:\n",
        "                        logger.warning(f\"Invalid audio file: {file_path}\")\n",
        "\n",
        "    def _validate_audio_file(self, file_path):\n",
        "        \"\"\"Validate audio file can be loaded and processed\"\"\"\n",
        "        try:\n",
        "            waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "            # Check if file is not empty\n",
        "            if waveform.numel() == 0:\n",
        "                return False\n",
        "\n",
        "            # Check if file is not too short\n",
        "            if waveform.shape[-1] < 1600:  # At least 0.1 seconds\n",
        "                return False\n",
        "\n",
        "            # Check for NaN or infinite values\n",
        "            if torch.isnan(waveform).any() or torch.isinf(waveform).any():\n",
        "                return False\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error validating {file_path}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _balance_classes(self):\n",
        "        \"\"\"Balance classes using oversampling of minority class\"\"\"\n",
        "        label_counts = Counter(self.labels)\n",
        "        max_count = max(label_counts.values())\n",
        "\n",
        "        balanced_files = []\n",
        "        balanced_labels = []\n",
        "\n",
        "        for label in [0, 1]:  # Healthy, Sick\n",
        "            label_files = [f for f, l in zip(self.files, self.labels) if l == label]\n",
        "            current_count = len(label_files)\n",
        "\n",
        "            # Add original files\n",
        "            balanced_files.extend(label_files)\n",
        "            balanced_labels.extend([label] * current_count)\n",
        "\n",
        "            # Oversample minority class\n",
        "            if current_count < max_count:\n",
        "                needed = max_count - current_count\n",
        "                oversampled = np.random.choice(label_files, needed, replace=True)\n",
        "                balanced_files.extend(oversampled)\n",
        "                balanced_labels.extend([label] * needed)\n",
        "\n",
        "        self.files = balanced_files\n",
        "        self.labels = balanced_labels\n",
        "\n",
        "        logger.info(\"Classes balanced through oversampling\")\n",
        "\n",
        "    def _print_class_distribution(self):\n",
        "        \"\"\"Print class distribution\"\"\"\n",
        "        label_counts = Counter(self.labels)\n",
        "        logger.info(f\"Class distribution:\")\n",
        "        logger.info(f\"  Healthy: {label_counts[0]}\")\n",
        "        logger.info(f\"  Sick: {label_counts[1]}\")\n",
        "\n",
        "    def _apply_augmentation(self, waveform):\n",
        "        \"\"\"Apply audio augmentation\"\"\"\n",
        "        if not self.augment:\n",
        "            return waveform\n",
        "\n",
        "        # Random noise addition (very light)\n",
        "        if np.random.random() < 0.3:\n",
        "            noise_factor = np.random.uniform(0.001, 0.005)\n",
        "            noise = torch.randn_like(waveform) * noise_factor\n",
        "            waveform = waveform + noise\n",
        "\n",
        "        # Random gain (very conservative)\n",
        "        if np.random.random() < 0.3:\n",
        "            gain = np.random.uniform(0.8, 1.2)\n",
        "            waveform = waveform * gain\n",
        "\n",
        "        # Ensure no clipping\n",
        "        waveform = torch.clamp(waveform, -1.0, 1.0)\n",
        "\n",
        "        return waveform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.files[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "            # Convert to mono\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "            # Resample to 16kHz\n",
        "            if sample_rate != 16000:\n",
        "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
        "                waveform = resampler(waveform)\n",
        "\n",
        "            # Normalize audio\n",
        "            waveform = waveform.squeeze(0)\n",
        "            if torch.std(waveform) > 0:\n",
        "                waveform = (waveform - torch.mean(waveform)) / (torch.std(waveform) + 1e-8)\n",
        "\n",
        "            # Apply augmentation\n",
        "            waveform = self._apply_augmentation(waveform)\n",
        "\n",
        "            # Trim or pad to max_length\n",
        "            if len(waveform) > self.max_length:\n",
        "                # Random crop for training, center crop for validation\n",
        "                if self.augment:\n",
        "                    start = np.random.randint(0, len(waveform) - self.max_length + 1)\n",
        "                else:\n",
        "                    start = (len(waveform) - self.max_length) // 2\n",
        "                waveform = waveform[start:start + self.max_length]\n",
        "            elif len(waveform) < self.max_length:\n",
        "                waveform = torch.nn.functional.pad(waveform, (0, self.max_length - len(waveform)))\n",
        "\n",
        "            # Final validation\n",
        "            if torch.isnan(waveform).any() or torch.isinf(waveform).any():\n",
        "                logger.warning(f\"NaN/Inf detected in {file_path}, using zeros\")\n",
        "                waveform = torch.zeros(self.max_length)\n",
        "\n",
        "            return waveform.float(), label\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading {file_path}: {e}\")\n",
        "            return torch.zeros(self.max_length).float(), label\n",
        "\n",
        "class RobustWav2Vec2Classifier(nn.Module):\n",
        "    \"\"\"Robust Wav2Vec2 classifier with bias prevention\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"facebook/wav2vec2-base-960h\", num_classes=2, freeze_encoder=True, dropout_rate=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained Wav2Vec2 model\n",
        "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(model_name)\n",
        "\n",
        "        # Freeze encoder if requested (recommended for stability)\n",
        "        if freeze_encoder:\n",
        "            for param in self.wav2vec2.feature_extractor.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.wav2vec2.feature_projection.parameters():\n",
        "                param.requires_grad = False\n",
        "            logger.info(\"Wav2Vec2 encoder frozen for stability\")\n",
        "\n",
        "        # Robust classification head with batch normalization\n",
        "        hidden_size = self.wav2vec2.config.hidden_size\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_size, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate * 0.5),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate * 0.25),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights properly\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize classifier weights\"\"\"\n",
        "        for module in self.classifier:\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        # Get Wav2Vec2 features\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = self.wav2vec2(input_values)\n",
        "\n",
        "            # Global average pooling over time dimension\n",
        "            pooled = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "\n",
        "            # Classification\n",
        "            logits = self.classifier(pooled)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class RobustTrainer:\n",
        "    \"\"\"Robust trainer with comprehensive bias prevention\"\"\"\n",
        "\n",
        "    def __init__(self, model, device, use_focal_loss=True, use_class_weights=True):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.use_focal_loss = use_focal_loss\n",
        "        self.use_class_weights = use_class_weights\n",
        "\n",
        "        # Initialize loss function\n",
        "        if use_focal_loss:\n",
        "            self.criterion = FocalLoss(alpha=1, gamma=2)\n",
        "            logger.info(\"Using Focal Loss for class imbalance\")\n",
        "        else:\n",
        "            self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Initialize optimizer with conservative settings\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=1e-5,  # Very conservative learning rate\n",
        "            weight_decay=0.01,\n",
        "            eps=1e-8\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='max', factor=0.5, patience=3, verbose=True\n",
        "        )\n",
        "\n",
        "        # Mixed precision scaler\n",
        "        self.scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "        # Training history\n",
        "        self.history = {\n",
        "            'train_loss': [], 'train_acc': [],\n",
        "            'val_loss': [], 'val_acc': []\n",
        "        }\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "        for batch_idx, (waveforms, labels) in enumerate(progress_bar):\n",
        "            waveforms = waveforms.to(self.device, non_blocking=True)\n",
        "            labels = labels.to(self.device, non_blocking=True)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = self.model(waveforms)\n",
        "                loss = self.criterion(logits, labels)\n",
        "\n",
        "            # Check for NaN loss\n",
        "            if torch.isnan(loss):\n",
        "                logger.warning(\"NaN loss detected, skipping batch\")\n",
        "                continue\n",
        "\n",
        "            # Backward pass with gradient scaling\n",
        "            self.scaler.scale(loss).backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            self.scaler.unscale_(self.optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "\n",
        "            # Statistics\n",
        "            total_loss += loss.item()\n",
        "            with torch.no_grad():\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'LR': f'{self.optimizer.param_groups[0][\"lr\"]:.2e}',\n",
        "                'GPU': f'{torch.cuda.memory_allocated()/1e9:.1f}GB'\n",
        "            })\n",
        "\n",
        "            # Clear cache periodically\n",
        "            if batch_idx % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        return avg_loss, accuracy, all_labels, all_preds\n",
        "\n",
        "    def validate_epoch(self, val_loader):\n",
        "        \"\"\"Validate for one epoch\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm(val_loader, desc=\"Validation\")\n",
        "\n",
        "            for waveforms, labels in progress_bar:\n",
        "                waveforms = waveforms.to(self.device, non_blocking=True)\n",
        "                labels = labels.to(self.device, non_blocking=True)\n",
        "\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    logits = self.model(waveforms)\n",
        "                    loss = self.criterion(logits, labels)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        return avg_loss, accuracy, all_labels, all_preds\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs=20, save_path=\"best_wav2vec2_model.pth\"):\n",
        "        \"\"\"Full training loop\"\"\"\n",
        "        best_val_acc = 0.0\n",
        "        patience_counter = 0\n",
        "        max_patience = 7\n",
        "\n",
        "        logger.info(f\"Starting training for {num_epochs} epochs\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            logger.info(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "            # Train\n",
        "            train_loss, train_acc, train_labels, train_preds = self.train_epoch(train_loader)\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_acc, val_labels, val_preds = self.validate_epoch(val_loader)\n",
        "\n",
        "            # Update learning rate\n",
        "            self.scheduler.step(val_acc)\n",
        "\n",
        "            # Save history\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "            self.history['train_acc'].append(train_acc)\n",
        "            self.history['val_loss'].append(val_loss)\n",
        "            self.history['val_acc'].append(val_acc)\n",
        "\n",
        "            # Print epoch results\n",
        "            logger.info(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "            logger.info(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "            # Print detailed classification report\n",
        "            logger.info(\"\\nValidation Classification Report:\")\n",
        "            print(classification_report(val_labels, val_preds, target_names=['Healthy', 'Sick']))\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'val_acc': val_acc,\n",
        "                    'history': self.history\n",
        "                }, save_path)\n",
        "\n",
        "                logger.info(f\"New best model saved! Val Acc: {val_acc:.4f}\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            # Early stopping\n",
        "            if patience_counter >= max_patience:\n",
        "                logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "                break\n",
        "\n",
        "            # Clear GPU cache\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        logger.info(f\"Training completed! Best validation accuracy: {best_val_acc:.4f}\")\n",
        "        return self.history\n",
        "\n",
        "def setup_gpu():\n",
        "    \"\"\"Setup GPU with optimal settings\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        logger.info(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        logger.info(f\"CUDA Version: {torch.version.cuda}\")\n",
        "        logger.info(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "        # Optimize for training\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.enabled = True\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        logger.warning(\"CUDA not available, using CPU\")\n",
        "\n",
        "    return device\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "    # Setup\n",
        "    device = setup_gpu()\n",
        "\n",
        "    # Configuration\n",
        "    DATA_DIR = \"data/audio\"  # Adjust path as needed\n",
        "    MODEL_SAVE_PATH = \"best_wav2vec2_chicken_classifier.pth\"\n",
        "    BATCH_SIZE = 8 if device.type == 'cuda' else 4\n",
        "    NUM_EPOCHS = 25\n",
        "    MAX_AUDIO_LENGTH = 48000  # 3 seconds at 16kHz\n",
        "\n",
        "    # Check data directory\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        logger.error(f\"Data directory not found: {DATA_DIR}\")\n",
        "        logger.info(\"Please ensure your data is organized as:\")\n",
        "        logger.info(\"data/audio/Healthy/ - containing healthy chicken audio files\")\n",
        "        logger.info(\"data/audio/Sick/ - containing sick chicken audio files\")\n",
        "        return\n",
        "\n",
        "    # Initialize processor\n",
        "    logger.info(\"Loading Wav2Vec2 processor...\")\n",
        "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "    # Create datasets\n",
        "    logger.info(\"Creating datasets...\")\n",
        "    full_dataset = RobustChickenDataset(\n",
        "        DATA_DIR, processor, max_length=MAX_AUDIO_LENGTH,\n",
        "        augment=True, balance_classes=True\n",
        "    )\n",
        "\n",
        "    if len(full_dataset) == 0:\n",
        "        logger.error(\"No valid audio files found!\")\n",
        "        return\n",
        "\n",
        "    # Split dataset\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(0.7 * total_size)\n",
        "    val_size = int(0.15 * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        full_dataset, [train_size, val_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=2, pin_memory=True, persistent_workers=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=2, pin_memory=True, persistent_workers=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=2, pin_memory=True, persistent_workers=True\n",
        "    )\n",
        "\n",
        "    logger.info(f\"Dataset split - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Initialize model\n",
        "    logger.info(\"Initializing Wav2Vec2 model...\")\n",
        "    model = RobustWav2Vec2Classifier(\n",
        "        model_name=\"facebook/wav2vec2-base-960h\",\n",
        "        num_classes=2,\n",
        "        freeze_encoder=True,\n",
        "        dropout_rate=0.3\n",
        "    )\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    logger.info(f\"Total parameters: {total_params:,}\")\n",
        "    logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = RobustTrainer(model, device, use_focal_loss=True, use_class_weights=True)\n",
        "\n",
        "    # Train model\n",
        "    logger.info(\"Starting robust training...\")\n",
        "    history = trainer.train(train_loader, val_loader, NUM_EPOCHS, MODEL_SAVE_PATH)\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Val Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Acc')\n",
        "    plt.plot(history['val_acc'], label='Val Acc')\n",
        "    plt.title('Training Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    logger.info(\"Training completed successfully!\")\n",
        "    logger.info(f\"Model saved to: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyN5V4CXoMb/5Yd846pFMfHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0407b5c65ae542d3bccd37b35c28d1cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0740f24a5d5244dc9c58344a3880bda5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "084b64c6ddb2469e8db9a060215bf9b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09fa54cb218b47008475fa3709034917": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fb7ee5172904744aa7d81e435763b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "131d9e5f801c427e999783e0a88c7043": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6eaf2d9d7db445b94ad1e12c5d1f436",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0740f24a5d5244dc9c58344a3880bda5",
            "value": "‚Äá0/0‚Äá[00:00&lt;?,‚Äá?it/s]"
          }
        },
        "131fda7781954326ba65b44dccc841dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dad78b4371a0453eafffc5b11696ac9a",
              "IPY_MODEL_e656d9acdb99453fa7050f67d8c9dee8",
              "IPY_MODEL_383f3b3183d648c5ba71a16fc41b6bfe"
            ],
            "layout": "IPY_MODEL_e4cca20e778e411e90221ec578869ac8"
          }
        },
        "163d26ba92e24d29970a6098029249e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "165cc6f915fa41f69bffc066dd04a14c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6010f9693344d3db0deb390bed42d3e",
              "IPY_MODEL_80ec78492b924209b2a005c88055e527",
              "IPY_MODEL_2246c9d7bad24a3eaaea6a68f6604fdf"
            ],
            "layout": "IPY_MODEL_1ffdd112d13c42c995dbc709c5f6cba1"
          }
        },
        "167ee80494dd4e48b8145573c46ce9aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_601301c94df243a093c6c8b719538cbb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_493e86025abf407d83d293f652c94f5c",
            "value": "Validating‚Äáhealthy:‚Äá100%"
          }
        },
        "1b9646f6da58422387286060c906abee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d500e4d516f4dbea0338cc648f92c74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1da1e1b377bc47378e28396ec4b7876a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37cbfd4352c849828e34a100dc5128a2",
              "IPY_MODEL_af20e0a6914f4f6ea7e4d90c962d8176",
              "IPY_MODEL_7a32b19d67904fc7899d1bc2a7d0e0e4"
            ],
            "layout": "IPY_MODEL_1b9646f6da58422387286060c906abee"
          }
        },
        "1f71962c63e345749141013882929335": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fb03067ed7f4886ac078a44fd4b4bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ffdd112d13c42c995dbc709c5f6cba1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2246c9d7bad24a3eaaea6a68f6604fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_163d26ba92e24d29970a6098029249e7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_09fa54cb218b47008475fa3709034917",
            "value": "‚Äá2121/2121‚Äá[00:04&lt;00:00,‚Äá473.28it/s]"
          }
        },
        "22ecead0b09f4bc889d9df01391b6a0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2474a5b6fe0c4144bbe803553fc7afbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bd62c19d3f048b2804429b4dabd3954",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_51fa22de1cfe4538aa429c803fa74081",
            "value": "‚Äá2121/2121‚Äá[00:14&lt;00:00,‚Äá192.42it/s]"
          }
        },
        "247d324553744d2b95eaa8cfa378b29e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26b5583110e04c39a08fb5abf9ba1fdc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2774fe3e3e9e49dfb1aab5102c2dc086": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcebc5e8aec94ee8bd2bbf76a6fdd20e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8a2860485c54fc385bfd9dfa1bf6c00",
            "value": 0
          }
        },
        "2b1aba06426349b3ae03faa8f5be60ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b5b3d807d514cd2b9727c7ce626f69b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fe4cbe71a44414f9eaa690cc0ffd295": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "329eab22e37840ab94348d23924355f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c5bcce35b2f4fb4828b5fc33ef96d10",
            "max": 2121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5637f1de4d94692a9bef07b68c69b59",
            "value": 2121
          }
        },
        "33b929e304e04a34bfc85596f4ddc17c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37cbfd4352c849828e34a100dc5128a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93999d60a6774ad0890adcb2ed55d261",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5478e133b5fc4baeb5bb93a8a9bfdf94",
            "value": "Validating‚Äáhealthy:‚Äá100%"
          }
        },
        "383f3b3183d648c5ba71a16fc41b6bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5358ab5d281444a78e5ff85915a2b92e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8a61bd9ef841483d919c27247f4c7de1",
            "value": "‚Äá0/0‚Äá[00:00&lt;?,‚Äá?it/s]"
          }
        },
        "3a527bac78914b959f2d21e0c050922b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c89c733d60d4957add368a925b0548f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d6a7be1778647a495072c487d4ab42d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b590c458adb4cd0a72d4c33189156bf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e4decd5fef2548b4a67028535be0e6b7",
            "value": "Validating‚Äásick:‚Äá100%"
          }
        },
        "3dc584ffa8de4058ba968da54123d684": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42e1e184ea7540a4a6c62a0d805192bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46ffeed30add4a30908d5409bd6e48df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4907abcdf10543a6a6b5c4c142b37d78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49225a324a58471a8887792e4f5ee4eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b1aba06426349b3ae03faa8f5be60ed",
            "max": 2121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d60ec66223e54880b6f75edfff5dd77f",
            "value": 2121
          }
        },
        "493e86025abf407d83d293f652c94f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b125576c7064440bbb11cec1b2e8d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "513323030429496bab88edfdb3c86b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51fa22de1cfe4538aa429c803fa74081": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52d65b2034764ffaa573c7461d6015cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22ecead0b09f4bc889d9df01391b6a0a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_67d6757a0b01458dbb5ac755fdf15d10",
            "value": "Validating‚Äáhealthy:‚Äá100%"
          }
        },
        "5358ab5d281444a78e5ff85915a2b92e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "543425a6a22b4da58d786c4c01201068": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5478e133b5fc4baeb5bb93a8a9bfdf94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "558e37e9c8794abe88cf21f5f72c3fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fb7ee5172904744aa7d81e435763b9d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1fb03067ed7f4886ac078a44fd4b4bf2",
            "value": "Validating‚Äásick:‚Äá100%"
          }
        },
        "5ae3752dd8a34787aba3336a6fc50200": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c1019d0928c4a839f53314a5afca5be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fe4cbe71a44414f9eaa690cc0ffd295",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e493d52515f1486aa17ea125ca4848db",
            "value": "Validating‚Äánoise:‚Äá"
          }
        },
        "5d66468817044d4f8c186622e6272601": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f9894a73b7346dbaedc7d30a2ee3ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26b5583110e04c39a08fb5abf9ba1fdc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fa5cc01f16de42e5a7c6bbf667b5a4b9",
            "value": "‚Äá2139/2139‚Äá[00:18&lt;00:00,‚Äá186.10it/s]"
          }
        },
        "601301c94df243a093c6c8b719538cbb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "616f9f7121f946ae89268df99134a5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_705ab46ed5104157a9ca50c6a75221d1",
              "IPY_MODEL_329eab22e37840ab94348d23924355f5",
              "IPY_MODEL_7f42e1c1ebe64b90a4e7b6ba8183dd4d"
            ],
            "layout": "IPY_MODEL_f4c6cd6131234420970facd9e1e62707"
          }
        },
        "66da8e66def64bdaac0f7ad67e70d710": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67d6757a0b01458dbb5ac755fdf15d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a0b8b62014b4132811a72df91eea269": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5608b160e934eeca10b7c969cca662f",
            "max": 2139,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd1e7be504b44aa1a06408e00e0b6d3b",
            "value": 2139
          }
        },
        "6c9770922ff94ad3a6fcf0801ae08d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "705ab46ed5104157a9ca50c6a75221d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_513323030429496bab88edfdb3c86b6a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7379a4a1c3a442f2b43b583ef9450714",
            "value": "Validating‚Äásick:‚Äá100%"
          }
        },
        "7379a4a1c3a442f2b43b583ef9450714": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "764811ea9e1b4119931bb7963c971af6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c3567c085bd48aead16f45ce1e0e1ba",
              "IPY_MODEL_8d33673a829640188fb9739ea172a95d",
              "IPY_MODEL_92619e279a154c7393b31d193a87cd97"
            ],
            "layout": "IPY_MODEL_e8b331cfd8fe477590d0db6271c16d70"
          }
        },
        "76d8bf87ba0b4e45bcd20467b18afaf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f71962c63e345749141013882929335",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_887a8e05fa454d17b8f2727375a5a638",
            "value": "Validating‚Äánoise:‚Äá"
          }
        },
        "79634d26baad4e61876843d98e7afcca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a0b53deff16422b818c70acb13a28a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e44c47b87a0d4bfb82f6adb6374145be",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_aea948ec609b40cbb8abaaeee5670709",
            "value": "‚Äá0/0‚Äá[00:00&lt;?,‚Äá?it/s]"
          }
        },
        "7a32b19d67904fc7899d1bc2a7d0e0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e679bdb9a0874acabb39b050df1b4022",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f26029b67a3b4291a22be85aadbfde8c",
            "value": "‚Äá2139/2139‚Äá[00:05&lt;00:00,‚Äá438.97it/s]"
          }
        },
        "7b908b2f1f0149f68272b647d104db36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d6a7be1778647a495072c487d4ab42d",
              "IPY_MODEL_fed4260e346942dc9a895c5b536d1ac5",
              "IPY_MODEL_2474a5b6fe0c4144bbe803553fc7afbb"
            ],
            "layout": "IPY_MODEL_3c89c733d60d4957add368a925b0548f"
          }
        },
        "7bd62c19d3f048b2804429b4dabd3954": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dc36fea453b42e8a041aac002c67a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_167ee80494dd4e48b8145573c46ce9aa",
              "IPY_MODEL_c89ade4490f540fa972b12a79fafcda4",
              "IPY_MODEL_c6f8d96e656e49f69e4ecaf74f896b5a"
            ],
            "layout": "IPY_MODEL_247d324553744d2b95eaa8cfa378b29e"
          }
        },
        "7f42e1c1ebe64b90a4e7b6ba8183dd4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33b929e304e04a34bfc85596f4ddc17c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e9f080cf72004611b9ac72b6089e08b8",
            "value": "‚Äá2121/2121‚Äá[00:09&lt;00:00,‚Äá360.33it/s]"
          }
        },
        "80ec78492b924209b2a005c88055e527": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a319ab143aa4fbb85ad7c07752e5e4d",
            "max": 2121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0407b5c65ae542d3bccd37b35c28d1cf",
            "value": 2121
          }
        },
        "832dc389d7ef4eaeb9e27b821d78d508": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "839cbbcafca04203ad9b9e0f02c761fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "874d93a74cc649f1864e1c96a78bdc5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "887a8e05fa454d17b8f2727375a5a638": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "891176dfbc5b4addb3f9906372e23130": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c1019d0928c4a839f53314a5afca5be",
              "IPY_MODEL_2774fe3e3e9e49dfb1aab5102c2dc086",
              "IPY_MODEL_7a0b53deff16422b818c70acb13a28a1"
            ],
            "layout": "IPY_MODEL_919dac3d94fa4f00a9db9f48dd42e214"
          }
        },
        "8a61bd9ef841483d919c27247f4c7de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c3567c085bd48aead16f45ce1e0e1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cebf7351eafe41bba4e3dc6f716e5611",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6c9770922ff94ad3a6fcf0801ae08d3b",
            "value": "Validating‚Äáhealthy:‚Äá100%"
          }
        },
        "8d33673a829640188fb9739ea172a95d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9127fcadf6445be8a009aeea6713163",
            "max": 2139,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46ffeed30add4a30908d5409bd6e48df",
            "value": 2139
          }
        },
        "919dac3d94fa4f00a9db9f48dd42e214": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92619e279a154c7393b31d193a87cd97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dc584ffa8de4058ba968da54123d684",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3a527bac78914b959f2d21e0c050922b",
            "value": "‚Äá2139/2139‚Äá[00:11&lt;00:00,‚Äá336.15it/s]"
          }
        },
        "93999d60a6774ad0890adcb2ed55d261": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9981da3e32cb4485a383ea120a50a186": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a319ab143aa4fbb85ad7c07752e5e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b590c458adb4cd0a72d4c33189156bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c5bcce35b2f4fb4828b5fc33ef96d10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f08f3c17b304109a85f2b10bcd0969f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52d65b2034764ffaa573c7461d6015cc",
              "IPY_MODEL_6a0b8b62014b4132811a72df91eea269",
              "IPY_MODEL_5f9894a73b7346dbaedc7d30a2ee3ea6"
            ],
            "layout": "IPY_MODEL_4907abcdf10543a6a6b5c4c142b37d78"
          }
        },
        "a23b418af72c4ed7af016c30460e7be4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5637f1de4d94692a9bef07b68c69b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6eaf2d9d7db445b94ad1e12c5d1f436": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8a2860485c54fc385bfd9dfa1bf6c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9127fcadf6445be8a009aeea6713163": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aea948ec609b40cbb8abaaeee5670709": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af20e0a6914f4f6ea7e4d90c962d8176": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1569e55917c4092905b2a8fb129a09d",
            "max": 2139,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f732a86f455a4490b02c5760f9acd6d2",
            "value": 2139
          }
        },
        "bcebc5e8aec94ee8bd2bbf76a6fdd20e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bd1e7be504b44aa1a06408e00e0b6d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c37b6eea29fb468daefd4c834ae3127a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9463558caa74e1bad4cb021b3c03a52",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_42e1e184ea7540a4a6c62a0d805192bd",
            "value": "‚Äá2121/2121‚Äá[00:12&lt;00:00,‚Äá200.34it/s]"
          }
        },
        "c6f8d96e656e49f69e4ecaf74f896b5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9981da3e32cb4485a383ea120a50a186",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_79634d26baad4e61876843d98e7afcca",
            "value": "‚Äá2139/2139‚Äá[00:49&lt;00:00,‚Äá79.71it/s]"
          }
        },
        "c89ade4490f540fa972b12a79fafcda4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_084b64c6ddb2469e8db9a060215bf9b1",
            "max": 2139,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_543425a6a22b4da58d786c4c01201068",
            "value": 2139
          }
        },
        "cebf7351eafe41bba4e3dc6f716e5611": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5608b160e934eeca10b7c969cca662f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d60ec66223e54880b6f75edfff5dd77f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dad78b4371a0453eafffc5b11696ac9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a23b418af72c4ed7af016c30460e7be4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_66da8e66def64bdaac0f7ad67e70d710",
            "value": "Validating‚Äánoise:‚Äá"
          }
        },
        "df938b337bd64ee49e9a826ea8919958": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_558e37e9c8794abe88cf21f5f72c3fa5",
              "IPY_MODEL_49225a324a58471a8887792e4f5ee4eb",
              "IPY_MODEL_c37b6eea29fb468daefd4c834ae3127a"
            ],
            "layout": "IPY_MODEL_1d500e4d516f4dbea0338cc648f92c74"
          }
        },
        "e44c47b87a0d4bfb82f6adb6374145be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e493d52515f1486aa17ea125ca4848db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4cca20e778e411e90221ec578869ac8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4decd5fef2548b4a67028535be0e6b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e656d9acdb99453fa7050f67d8c9dee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_839cbbcafca04203ad9b9e0f02c761fa",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d66468817044d4f8c186622e6272601",
            "value": 0
          }
        },
        "e679bdb9a0874acabb39b050df1b4022": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8b331cfd8fe477590d0db6271c16d70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9463558caa74e1bad4cb021b3c03a52": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9f080cf72004611b9ac72b6089e08b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eca04068842846aca0e8d5b3047e0418": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1569e55917c4092905b2a8fb129a09d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f17c05f824324187bb9ac110f40edfff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76d8bf87ba0b4e45bcd20467b18afaf0",
              "IPY_MODEL_fe08440f3bd54f53bc7439532209078e",
              "IPY_MODEL_131d9e5f801c427e999783e0a88c7043"
            ],
            "layout": "IPY_MODEL_2b5b3d807d514cd2b9727c7ce626f69b"
          }
        },
        "f26029b67a3b4291a22be85aadbfde8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4c6cd6131234420970facd9e1e62707": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6010f9693344d3db0deb390bed42d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ae3752dd8a34787aba3336a6fc50200",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_eca04068842846aca0e8d5b3047e0418",
            "value": "Validating‚Äásick:‚Äá100%"
          }
        },
        "f6a932b0cf404023be9a4a858e86e16b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f732a86f455a4490b02c5760f9acd6d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa5cc01f16de42e5a7c6bbf667b5a4b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe08440f3bd54f53bc7439532209078e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6a932b0cf404023be9a4a858e86e16b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_874d93a74cc649f1864e1c96a78bdc5e",
            "value": 0
          }
        },
        "fed4260e346942dc9a895c5b536d1ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_832dc389d7ef4eaeb9e27b821d78d508",
            "max": 2121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b125576c7064440bbb11cec1b2e8d7b",
            "value": 2121
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}